[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics and Research Design",
    "section": "",
    "text": "Overview\nBook Name: Statistics and Research Design.\nSummary: Materials for the Statistics and Research Design course on the MSc Research Methods in Psychological Science programme, University of Glasgow School of Psychology & Neuroscience.\nAuthors: James Bartlett, Guillaume Rousselet, and Lisa DeBruine.\nAim: This course covers advanced statistics concepts you might need in psychological research, but you do not normally learn on standard curricula. The course is split into three segments for each lecturer to outline a core part of your advanced training. The first segment covers the general linear model, building up from simple linear regression to generalised linear regression (Bartlett). The second segment covers statistical fallacies, misconceptions, and bootstrapping (Rousselet). The third segment covers Bayesian approaches to hypothesis testing and estimation (Bartlett).\nContact: This book is a living document and will be regularly checked and updated for improvements. Should you have any issues using the book or queries, please contact James Bartlett.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "1  How to Use this Book",
    "section": "",
    "text": "1.1 Setup",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "instructions.html#sec-setup",
    "href": "instructions.html#sec-setup",
    "title": "1  How to Use this Book",
    "section": "",
    "text": "1.1.1 Install booktem\n# install.packages(\"devtools\")\ndevtools::install_github(\"debruine/booktem\")\n\n1.1.2 Quarto Options\nThe file _quarto.yml contains various options that you can set to change the format and look of your book.\n\n1.1.2.1 Language Options\nThere is some default text for things like the “authors” list and “table of contents” that might need translations. Set the lang key to the 2-letter language code for your language.\nYou can make a custom translation by translating the values in the include/_language.yml file.\nlang: en\n# language: include/_language.yml\n\n1.1.2.2 Project Options\nThe project key defines the inputs and outputs for the book (quarto reference).\n\n\n\n\n\n\nproject key\n\n\n\n\n\nproject:\n  type: book\n  output-dir: docs\n  resources: resources \n\n\n\nThe output-dir key defines the directory where the rendered web files will be saved. This is set to docs in order to be compatible with GitHub Pages, but you can change this if you are working with a different repository that expects the web files to be in a different directory.\nThe resources key specifies a directory that is copied verbatim to the output directory. This is where you should put, for example, data files that you want to make accessible online (sometimes they don’t automatically copy over when linked).\n\n1.1.2.3 Book Options\nThe book key defines options that affect the look and function of the book (quarto reference).\n\n\n\n\n\n\nbook key\n\n\n\n\n\nbook:\n  title: Book\n  subtitle: ~\n  author: ~\n  doi: ~\n  license: CC-BY 4.0\n  description: ~\n  cover-image: images/logos/logo.png\n  image: images/logos/logo.png\n  favicon: images/logos/logo.png\n  cookie-consent: false\n  google-analytics: ~\n  page-navigation: true\n  search: true\n  # comments:\n  #   hypothesis:\n  #     theme: clean\n  #     openSidebar: false\n  downloads: ~\n  sharing: ~\n  sidebar:\n    title: ~\n    logo: ~\n    search: true\n    contents: ~\n    style: floating\n    background: ~\n    foreground: ~\n    border: true\n    alignment: left\n    collapse-level: 3\n    pinned: true\n    header: \"\"\n    footer: \"\"\n  margin-header: ~\n  page-footer:\n    left: ~\n    right: ~\n  chapters:\n  - index.qmd\n  - instructions.qmd\n  appendices:\n  - references.qmd\n\n\n\n\n1.1.2.4 html Options\nThe format key defines options for specific formats, such as html or pdf. We’ll only be using html here (quarto reference).\n\n\n\n\n\n\nformat:html key\n\n\n\n\n\nformat:\n  html:\n    theme:\n      light:\n      - flatly\n      - include/light.scss\n      dark:\n      - darkly\n      - include/dark.scss\n    css:\n    - https://use.fontawesome.com/releases/v5.13.0/css/all.css\n    - include/booktem.css\n    - include/glossary.css\n    - include/style.css\n    df-print: kable\n    code-link: true\n    code-fold: false\n    code-line-numbers: true\n    code-overflow: wrap\n    code-copy: hover\n    highlight-style: a11y\n    mainfont: ~\n    monofont: ~\n    include-after-body: [include/script.js]\n\n\n\n\n1.1.3 Crossrefs\nSection links must start with sec- and look like this: Section 1.1.5.\n## Section Title {#sec-section-title}\n\nInternal links look like this: @sec-section-title\nFigure links must start with fig- and look like this: Figure 1.1.\n\n\n\n\n\n\n\nFigure 1.1: A histogram of a Poisson distribution with lambda = 3\n\n\n\n\nTable links must start with tbl- and look like this: Table 1.1.\n\n\n\nTable 1.1: The authors of this book\n\n\n\n\n\nfirst_name\nlast_name\n\n\n\nLisa\nDeBruine\n\n\nDaniël\nLakens\n\n\n\n\n\n\n\n\n\nSee the quarto documentation for more information.\n\n1.1.4 References\nZotero export - keep updated\n\n1.1.5 Snippets\nSnippets in RStudio provide shortcuts to syntax. For example, in an RMarkdown document, type “r” and shift-tab to expand a code chunk.\nYou can add your own snippets. Under the Tools menu, choose Edit Code Snippets... and paste the following text into the end of the appropriate sections.\n\n1.1.5.1 Markdown\nsnippet gls\n    r glossary(\"${1:term}\")\n    \nsnippet gls2\n    r glossary(\"${1:term}\", \"${2:display}\")\n    \nsnippet h1\n    # ${1:title} {#sec-${2:ref}}\n    \nsnippet h2\n    ## ${1:title} {#sec-${2:ref}}\n    \nsnippet h3\n    ### ${1:title} {#sec-${2:ref}}\n    \nsnippet h4\n    #### ${1:title} {#sec-${2:ref}}\n    \nsnippet h5\n    ##### ${1:title} {#sec-${2:ref}}\n\n1.1.6 Customize\n\n1.1.6.1 Page Footer\nThe default footer includes license YEAR, author, and github and twitter icons, but you can customize this in the _quarto.yml file under page-footer:. See the quarto documentation for more options. See the available icons at https://icons.getbootstrap.com/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "instructions.html#sec-layout",
    "href": "instructions.html#sec-layout",
    "title": "1  How to Use this Book",
    "section": "\n1.2 Layout",
    "text": "1.2 Layout\n\n1.2.1 Conventions\nThis book will use the following conventions:\n\nCode: list(number = 1, letter = \"A\")\n\nFile paths: data/sales.csv\n\nMenu/interface options: Tools &gt; Global Options… &gt; Pane Layout\n\nR Packages: tidyverse\n\nGlossary items: alphaThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\n\nCitations: Wickham et al. (2022)\n\nInternal links: Section 1.2.1\n\nExternal links: Mastering Shiny\n\nMac-specific: Cmd-Shift-F10\n\nWindows-specific: Ctl-Shift-F10\n\n\nA list of mac and windows keyboard shortcuts.\n\n1.2.2 Figures\nIt is best practice to set a custom ggplot theme, then each subsequent plot will use that theme. You can put this code in R/my_setup.R after loading ggplot2.\nStart with a built-in theme and then add any tweaks with the theme() function.\n\nlibrary(ggplot2)\n\nmy_theme &lt;- theme_minimal(base_size = 16) + \n            theme(panel.background = element_rect(fill = \"red\", \n                                                  color = \"black\", \n                                                  size = 5),\n                  panel.grid = element_blank())\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\ntheme_set(my_theme)\n\n\nggplot(midwest, aes(popdensity, percollege)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Population Density\", y = \"Percent College Educated\")\n\n\n\n\n\n\nFigure 1.2: Demographic information of midwest counties from 2000 US census\n\n\n\n\n\n1.2.3 Tables\n\nhead(beaver1)\n\n\n\nBeavers\n\nday\ntime\ntemp\nactiv\n\n\n\n346\n840\n36.33\n0\n\n\n346\n850\n36.34\n0\n\n\n346\n900\n36.35\n0\n\n\n346\n910\n36.42\n0\n\n\n346\n920\n36.55\n0\n\n\n346\n930\n36.69\n0\n\n\n\n\n\n\n\n1.2.4 Callout boxes\nSee the quarto reference for more options.]{.aside}\n\n\n\n\n\n\nNote\n\n\n\n.callout-note: Informational asides.\n\n\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\ncolapse = “true”: Expanded!\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n.callout-tip: Tips\n\n\n\n\n\n\n\n\nWarning\n\n\n\n.callout-warning: Notes to warn you about something.\n\n\n\n\n\n\n\n\nCaution\n\n\n\n.callout-caution: Notes about things that could cause serious errors.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n.callout-important: Notes about things that are important.\n\n\n\n1.2.5 Code and Output\n\n# code chunks\npaste(\"Code\", \"Output\", 1, sep = \" \")\n\n[1] \"Code Output 1\"\n\n\n\n\n\nFilename or header\n\n# code chunks with filename\na &lt;- 1\n\n\n\n\n```{r, fig.width = 2, fig.height = 2}\n# code chunks with visible headers\nhist(rnorm(100000))\n```\n\n\n## Markdown Example\n\n* Inline code: `r nrow(iris)`\n* *Italics*\n* **Bold**\n* [Linked text](https://psyteachr.github.io)\n\n1.2.6 Fonts",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "instructions.html#sec-extras",
    "href": "instructions.html#sec-extras",
    "title": "1  How to Use this Book",
    "section": "\n1.3 Extras",
    "text": "1.3 Extras\n\n1.3.1 Glossary\nBooks are set up with lightweight glossary functions from the glossary package.\n\n# code in R/my_setup.R to initialise the glossary on each page\nlibrary(glossary)\nglossary_path(\"include/glossary.yml\")\nglossary_popup(\"click\") # \"click\", \"hover\" or \"none\"\n\nEdit the file glossary.yml with your glossary terms like this:\nalpha: |\n  The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\np-value: |\n  The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\nLook up a term from the glossary file with glossary(\"alpha\"): alphaThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\nDisplay a different value for the term with glossary(\"alpha\", \"$\\\\alpha$\"): \\(\\alpha\\)The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\nUse an inline definition instead of the glossary file with glossary(\"beta\", def = \"The second letter of the Greek alphabet\"): betaThe second letter of the Greek alphabet\nJust show the definition with glossary(\"p-value\", show = \"def\"): The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\nShow the table of terms defined on this page with glossary_table():\n\n\n\n\nterm\ndefinition\n\n\n\nalpha\nThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\n\n\nbeta\nThe second letter of the Greek alphabet\n\n\np-value\nThe probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\n\n\n\n\n\n\n1.3.2 FontAwesome\nThe fontAwesome quarto extension allows you to use the free icons with syntax like:\n{{&lt; fa dragon &gt;}}\n{{&lt; fa brands github 5x \"(github logo)\" &gt;}}\nTo install it, just run this code in the Terminal pane of RStudio (not the Console pane).\nquarto install extension quarto-ext/fontawesome\n\n\n\n\nWickham, H., Bryan, J., & Barrett, M. (2022). Usethis: Automate package and project setup. https://CRAN.R-project.org/package=usethis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "webexercises.html",
    "href": "webexercises.html",
    "title": "Appendix A — Webexercises",
    "section": "",
    "text": "A.1 Example Questions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "webexercises.html#example-questions",
    "href": "webexercises.html#example-questions",
    "title": "Appendix A — Webexercises",
    "section": "",
    "text": "A.1.1 Fill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 25 is: \n\n\nThe blanks are case-sensitive; if you don’t care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\nA.1.2 Multiple Choice (mcq())\n\n“Never gonna give you up, never gonna: \nlet you go\nturn you down\nrun away\nlet you down”\n“I \nbless the rains\nguess it rains\nsense the rain down in Africa” -Toto\n\nA.1.3 True or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). \nTRUE\nFALSE\n\n\nA.1.4 Longer MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It’s probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\nthe probability that the null hypothesis is truethe probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is truethe probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\nif you repeated the process many times, 95% of intervals calculated in this way contain the true meanthere is a 95% probability that the true mean lies within this range95% of the data fall within this range",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "webexercises.html#checked-sections",
    "href": "webexercises.html#checked-sections",
    "title": "Appendix A — Webexercises",
    "section": "\nA.2 Checked sections",
    "text": "A.2 Checked sections\nCreate sections with the class webex-check to add a button that hides feedback until it is pressed. Add the class webex-box to draw a box around the section (or use your own styles).\n\nI am going to learn a lot: \nTRUE\nFALSE\n\nWhat is a p-value?\n\nthe probability that the null hypothesis is truethe probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is truethe probability of making an error in your conclusion",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "webexercises.html#hidden-solutions-and-hints",
    "href": "webexercises.html#hidden-solutions-and-hints",
    "title": "Appendix A — Webexercises",
    "section": "\nA.3 Hidden solutions and hints",
    "text": "A.3 Hidden solutions and hints\nYou can fence off a solution area that will be hidden behind a button using hide() before the solution and unhide() after, each as inline R code. Pass the text you want to appear on the button to the hide() function.\nIf the solution is a code chunk, instead of using hide() and unhide(), simply set the webex.hide chunk option to TRUE, or set it to the string you wish to display on the button.\nRecreate the scatterplot below, using the built-in cars dataset.\n\n\n\n\n\n\n\n\n\n\nI need a hint\n\nSee the documentation for plot() (?plot)\n\n\n\n\n\nClick here to see the solution\n\nplot(cars$speed, cars$dist)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bem, D. J. (2011). Feeling the future: Experimental\nevidence for anomalous retroactive influences on cognition and affect.\nJournal of Personality and Social Psychology, 100(3),\n407–425. https://doi.org/10.1037/a0021524\n\n\nDawtry, R. J., Sutton, R. M., & Sibley, C. G. (2015). Why\nWealthier People Think\nPeople Are Wealthier, and\nWhy It Matters: From\nSocial Sampling to Attitudes to\nRedistribution. Psychological Science,\n26(9), 1389–1400. https://doi.org/10.1177/0956797615586560\n\n\nEvans, T. R. (2024). Unethical Behaviour in the\nWorkplace: A Direct and\nConceptual Replication of Jones\n& Kavanagh (1996). https://doi.org/10.31234/osf.io/a2rj9\n\n\nJakobsen, J. C., Gluud, C., Wetterslev, J., & Winkel, P. (2017).\nWhen and how should multiple imputation be used for handling missing\ndata in randomised clinical trials – a practical guide with flowcharts.\nBMC Medical Research Methodology, 17(1), 162. https://doi.org/10.1186/s12874-017-0442-1\n\n\nJames, E. L., Bonsall, M. B., Hoppitt, L., Tunbridge, E. M., Geddes, J.\nR., Milton, A. L., & Holmes, E. A. (2015). Computer\nGame Play Reduces\nIntrusive Memories of\nExperimental Trauma via\nReconsolidation-Update\nMechanisms: Psychological Science, 26(8),\n1201–1215. https://doi.org/10.1177/0956797615583071\n\n\nJones, G. E., & Kavanagh, M. J. (1996). An experimental examination\nof the effects of individual and situational factors on unethical\nbehavioral intentions in the workplace. Journal of Business\nEthics, 15, 511–523.\n\n\nKnief, U., & Forstmeier, W. (2021). Violating the normality\nassumption may be the lesser of two evils. Behavior Research\nMethods, 53(6), 2576–2590. https://doi.org/10.3758/s13428-021-01587-5\n\n\nLeys, C., Delacre, M., Mora, Y. L., Lakens, D., & Ley, C. (2019).\nHow to Classify, Detect, and\nManage Univariate and\nMultivariate Outliers, With\nEmphasis on Pre-Registration.\nInternational Review of Social Psychology, 32(1), 5.\nhttps://doi.org/10.5334/irsp.289\n\n\nLopez, A., Choi, A. K., Dellawar, N. C., Cullen, B. C., Avila Contreras,\nS., Rosenfeld, D. L., & Tomiyama, A. J. (2023). Visual cues and food\nintake: A preregistered replication of Wansink\net al (2005). Journal of Experimental Psychology: General. https://doi.org/10.1037/xge0001503.supp\n\n\nTroy, A. S., Ford, B. Q., McRae, K., Zarolia, P., & Mauss, I.\n(2017). Change the things you can: Emotion regulation is\nmore beneficial for people from lower than from higher socioeconomic\nstatus. Emotion, 17(1), 141–154. https://doi.org/10.1037/emo0000210",
    "crumbs": [
      "Appendices",
      "References"
    ]
  },
  {
    "objectID": "01-lm-continuous.html",
    "href": "01-lm-continuous.html",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "",
    "text": "1.1 Chapter preparation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#chapter-preparation",
    "href": "01-lm-continuous.html#chapter-preparation",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "",
    "text": "1.1.1 Introduction to the data set\nFor this chapter, we are using open data from Dawtry et al. (2015). The abstract of their article is:\n\nThe present studies provide evidence that social-sampling processes lead wealthier people to oppose redistribution policies. In samples of American Internet users, wealthier participants reported higher levels of wealth in their social circles (Studies 1a and 1b). This was associated, in turn, with estimates of higher mean wealth in the wider U.S. population, greater perceived fairness of the economic status quo, and opposition to redistribution policies. Furthermore, results from a large-scale, nationally representative New Zealand survey revealed that low levels of neighborhood-level socioeconomic deprivation?an objective index of wealth within participants’ social circles mediated the relation between income and satisfaction with the economic status quo (Study 2). These findings held controlling for relevant variables, including political orientation and perceived self-interest. Social-structural inequalities appear to combine with social-sampling processes to shape the different political attitudes of wealthier and poorer people.\n\nIn summary, the authors investigated why people with more money tend to oppose wealth redistribution policies like higher taxes for higher incomes to decrease inequality in society. We are using data from Study 1A where 305 people completed measures on household income, predicted population income, their predicted social circle income, in addition to measures on support for wealth redistribution and fairness and satisfaction with the current system.\nThey predicted people with higher incomes have social circles with higher incomes, so they are more satisfied with the current system of wealth redistribution and less interested in changing it. In essence, poorer people and richer people have different experiences of how rich and equal their country is. In this chapter, we will explore the relationship between a range of these variables.\n\n1.1.2 Organising your files and project for the chapter\nBefore we can get started, you need to organise your files and project for the chapter, so your working directory is in order. This course builds on Data Skills for Reproducible Research to focus on inferential statistics, so if any concepts sound unfamiliar, make sure you revisit the course book.\nTo keep your work organised for this course, we recommend creating a folder on your computer for the course, then a separate sub-folder for assessments and each chapter of the book. Within each sub-folder, create folders for data and figures, so you have somewhere memorable to save the files and you know where everything is.\n\nIn your folder for statistics and research design Stats_Research_Design, create a new folder called 01_regression_continuous. Within 01_regression_continuous, create two new folders called data and figures.\nCreate an R Project for 01_regression_continuous as an existing directory for your chapter folder. This should now be your working directory.\nCreate a new Quarto document and give it a sensible title describing the chapter, such as 01 Correlations and Regression and save the file in your 01_regression_continuous folder.\nWe are working with a new data set, so please save the following data file: Dawtry_2015.csv. Right click the link and select “save link as”, or clicking the link will save the files to your Downloads. Make sure that you save the file as “.csv”. Save or copy the file to your data/ folder within 01_regression_continuous.\n\nYou are now ready to start working on the chapter!\n\n1.1.3 Activity 1 - Read and wrangle the data\nAs the first activity, try and test yourself by completing the following task list to practice your data wrangling skills. Create a final object called dawtry_clean to be consistent with the tasks below. If you just want to focus on correlations and regression, then you can just type the code in the solution.\n\n\n\n\n\n\nTry this\n\n\n\nTo wrangle the data, complete the following tasks:\n\n\nLoad the following packages. If you do not have them already, remember you will need to install them first if you are working on your own computer:\n\ntidyverse\neffectsize\ncorrelation\nperformance\n\n\nRead the data file data/Dawtry_2015.csv to the object name dawtry_data.\nReverse code two items: redist2 and redist4 to create two new variables redist2_R and redist4_R. See the codebook below, but they are on a 1-6 scale.\nSummarise the data to calculate the mean fairness_satisfaction score, by taking the mean of two items: fairness and satisfaction.\nSummarise the data to calculate the mean redistribution score, by taking the mean of four items: redist1, redist2_R, redist3, and redist4_R.\nCreate a new object called dawtry_clean by joining dawtry_data with your two new variables fairness_satisfaction and redistribution.\nDecrease the number of columns in dawtry_clean by selecting PS, all the columns between Household_Income and redistribution, but removing the two reverse coded items redist2_R and redist4_R.\n\nYour data should look like this to be ready to analyse:\n\n\nRows: 305\nColumns: 11\n$ PS                                  &lt;dbl&gt; 233, 157, 275, 111, 52, 11, 76, 90…\n$ Household_Income                    &lt;dbl&gt; NA, 20.00, 100.00, 150.00, 500.00,…\n$ Political_Preference                &lt;dbl&gt; 5, 5, 5, 8, 5, 3, 4, 3, 2, 3, NA, …\n$ age                                 &lt;dbl&gt; 40, 59, 41, 59, 35, 34, 36, 39, 40…\n$ gender                              &lt;dbl&gt; 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, NA, …\n$ Population_Inequality_Gini_Index    &lt;dbl&gt; 38.78294, 37.21451, 20.75000, 35.3…\n$ Population_Mean_Income              &lt;dbl&gt; 29715, 123630, 60000, 59355, 15360…\n$ Social_Circle_Inequality_Gini_Index &lt;dbl&gt; 28.056738, 24.323388, 14.442577, 2…\n$ Social_Circle_Mean_Income           &lt;dbl&gt; 21150, 65355, 107100, 86640, 56850…\n$ fairness_satisfaction               &lt;dbl&gt; 1.0, 3.5, 5.0, 7.0, 4.5, 2.5, 3.0,…\n$ redistribution                      &lt;dbl&gt; 5.50, 3.25, 3.75, 2.75, 3.00, 3.75…\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nYou should have the following in a code chunk:\n\n# Load the packages below\nlibrary(tidyverse)\nlibrary(effectsize)\nlibrary(correlation)\nlibrary(performance)\n\n# Load the data file\n# This should be the Dawtry_2015.csv file \ndawtry_data &lt;- read_csv(\"data/Dawtry_2015.csv\")\n\n# Reverse code redist2 and redist4\ndawtry_data &lt;- dawtry_data %&gt;%\n  mutate(redist2_R = 7 - redist2,\n         redist4_R = 7 - redist4)\n\n# calculate mean fairness and satisfaction score  \nfairness_satisfaction &lt;- dawtry_data %&gt;% \n  pivot_longer(cols = fairness:satisfaction, \n               names_to = \"Items\", \n               values_to = \"Response\") %&gt;% \n  group_by(PS) %&gt;% \n  summarise(fairness_satisfaction = mean(Response)) %&gt;% \n  ungroup()\n\n# calculate mean wealth redistribution score  \nredistribution &lt;- dawtry_data %&gt;% \n  pivot_longer(cols = c(redist1, redist2_R, redist3, redist4_R), \n               names_to = \"Items\", \n               values_to = \"Response\") %&gt;% \n  group_by(PS) %&gt;% \n  summarise(redistribution = mean(Response)) %&gt;% \n  ungroup()\n\n# join data and select columns for focus\ndawtry_clean &lt;- dawtry_data %&gt;% \n  inner_join(fairness_satisfaction, by = \"PS\") %&gt;% \n  inner_join(redistribution, by = \"PS\") %&gt;% \n  select(PS, Household_Income:redistribution, -redist2_R, -redist4_R)\n\n\n\n\n\n1.1.4 Activity 2 - Explore the data\n\n\n\n\n\n\nTry this\n\n\n\nAfter the wrangling steps, try and explore dawtry_clean to see what variables you are working with. For example, opening the data object as a tab to scroll around, explore with glimpse(), or try plotting some of the individual variables using a histogram.\n\n\nIn dawtry_clean, we have the following variables:\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\nPS\ndouble\nParticipant ID number.\n\n\nHousehold_Income\ndouble\nHousehold income in US Dollars ($).\n\n\nPolitical_Preference\ndouble\nPolitical attitudes: 1 = very liberal/very left-wing/strong Democrat to 7 = very conservative/very right-wing/strong Republican.\n\n\nage\ndouble\nAge in years.\n\n\ngender\ndouble\n1 = “Male”, 2 = “Female.\n\n\nPopulation_Inequality_Gini_Index\ndouble\nMeasure of income inequality from 0 (perfect equality) to 100 (perfect inequality), here where participants estimated population in equality.\n\n\nPopulation_Mean_Income\ndouble\nParticipant estimate of the mean household income in the population ($).\n\n\nSocial_Circle_Inequality_Gini_Index\ndouble\nMeasure of income inequality from 0 (perfect equality) to 100 (perfect inequality), here where participants estimated inequality in their social circle.\n\n\nSocial_Circle_Mean_Income\ndouble\nParticipant estimate of the mean household income in their social circle ($).\n\n\nfairness_satisfaction\ndouble\nPerceived fairness and satisfaction about the current system of wealth redistribution: Mean of two items (1 extremely fair – 9 extremely unfair)\n\n\nredistribution\ndouble\nSupport for wealth distribution: Mean of four items (1 strongly disagree – 6 strongly agree).\n\n\n\nWe will use this data set to demonstrate correlations and regression when you have one continuous predictor.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#correlation",
    "href": "01-lm-continuous.html#correlation",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.2 Correlation",
    "text": "1.2 Correlation\nBefore we cover regression as a more flexible framework for inferential statistics, we think it is useful to start with correlation to get a feel for how we can capture the relationship between two variables. As a reminder, correlations are standardised to range from -1 (a perfect negative correlation) to 1 (a perfect positive correlation). A value of 0 would mean there is no correlation between your variables.\n\n1.2.1 Activity 3 - Visualise the relationship\nTo explore the relationship between two variables, it is useful to create a scatterplot early for yourself, then provide a more professional looking version to help communicate your results. For most of the demonstrations in this chapter, we will try and answer the research question: “Is there a relationship between support for wealth redistribution and fairness and satisfaction with the current system?”\n\n\n\n\n\n\nTry this\n\n\n\nUsing your data visualisation skills from Repro Res, recreate the scatterplot below using the variables fairness_satisfaction and redistribution from dawtry_clean.\n\n\n\n\n\n\n\n\nLooking at the graph, we can describe the relationship as \npositive\nlittle to no correlation\nnegative.\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe scatterplot shows a negative correlation between the two variables. You need to be careful interpreting fairness and satisfaction as it is coded a little counterintuitive. Higher values mean great dissatisfaction.\nAs support for wealth redistribution increases to be more positive, perceived fairness and satisfaction tends to decrease. This makes sense as people who are more dissatisfied with the current system think there should be more wealth redistribution strategies.\nYou should have the following in a code chunk:\n\ndawtry_clean %&gt;% \n  ggplot(aes(x = fairness_satisfaction, y = redistribution)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  scale_x_continuous(name = \"Perceived Fairness and Satisfaction\", \n                     breaks = c(1:9)) + \n  scale_y_continuous(name = \"Support for Wealth Redistribution\", \n                     breaks = c(1:6))\n\n\n\n\n\n1.2.2 Activity 4 - Calculate the correlation coefficient\nVisualising the relationship between two variables is great for our understanding, but it does not tell us anything about the inferential statistics for what we can learn from our sample in hypothesis testing and measures of effect size.\nA correlation is a specific application of the general linear model. We want to capture the covariation between two variables. If you are interested, see the Handy Workbook (McAleer, 2023) for the calculations behind different types of correlation and how it represents the covariance of two variables compared to their total variability. They are not the only methods, but the two most common versions of a correlation are:\n\nPearson’s product-moment correlation (often shortened to the PearsonA standardised measure of the linear relationship between two variables that makes stringent assumptions about the population. correlation) and symbolised by r.\nSpearman’s rank correlation coefficient (often shortened to the SpearmanA standardised measure of the relationship between two variables that assumes a monotonic - but not necessarily a linear - relationship and makes less stringent assumptions about the population. correlation) and symbolised by \\(r_s\\) or sometimes the Greek letter rho \\(\\rho\\).\n\nThere is a function built into R (cor.test()) to calculate the correlation between two variables, but we tend to use the correlation() function from the correlation package as it has more consistent reporting features. The correlation() function requires:\n\nThe name of the data set you are using.\nThe name of the first variable you want to select for the correlation.\nThe name of the second variable you want to select for the correlation.\nThe type of correlation you want to run: e.g. pearson, spearman.\n\nFor our dawtry_clean data, we would run the following code for a two-tailed Pearson correlation:\n\ncorrelation(data = dawtry_clean, \n            select = \"fairness_satisfaction\", \n            select2 = \"redistribution\",  \n            method = \"pearson\", \n            alternative = \"two.sided\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\nfairness_satisfaction\nredistribution\n-0.70034\n0.95\n-0.7533907\n-0.6382316\n-17.07843\n303\n&lt; .001\nPearson correlation\n305\n\n\n\n\n\nYour output will look a little different due to how our book renders tables, but you should get the same information. For the three key concepts of inferential statistics, we get\n\nHypothesis testing: p &lt; .001, suggesting we can reject the null hypothesis assuming \\(\\alpha\\) = .05.\nEffect size: r = -.70, suggesting a strong negative correlation.\nConfidence interval: [-0.75, -0.64], showing the precision around the effect size estimate.\n\nTo summarise: a Pearson correlation showed there was a strong, negative, statistically significant relationship between attitudes on wealth redistribution and perceived fairness and satisfaction, r (303) = -0.70, p &lt; .001, 95% CI = [-0.75, -0.64].\nIf we had reason to use a Spearman correlation instead, all we need to do is change the method argument.\n\ncorrelation(data = dawtry_clean, \n            select = \"fairness_satisfaction\", \n            select2 = \"redistribution\",  \n            method = \"spearman\", \n            alternative = \"two.sided\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nrho\nCI\nCI_low\nCI_high\nS\np\nMethod\nn_Obs\n\n\nfairness_satisfaction\nredistribution\n-0.6806667\n0.95\n-0.738182\n-0.6133274\n7947402\n&lt; .001\nSpearman correlation\n305\n\n\n\n\n\nSimilarly, we could report the results as: a Spearman correlation showed there was a strong, negative, statistically significant relationship between attitudes on wealth redistribution and perceived fairness and satisfaction, \\(r_s\\) (303) = -0.68, p &lt; .001, 95% CI = [-0.74, -0.61].\n\n\n\n\n\n\nTry this\n\n\n\nGreat work following along so far, but now it is time to test your understanding on a new set of variables. Use the variables age and redistribution from dawtry_clean. We can ask the question: “What is the relationship between age and attitudes on wealth redistribution?”\n\nCreate a scatterplot to visualise the relationship between age and redistribution from dawtry_clean.\n\nApply the Pearson correlation to get your inferential statistics and answer the following questions:\n\nHypothesis testing: Assuming \\(\\alpha\\) = .05, the relationship between age and wealth redistribution is \nstatistically significant\nnot statistically significant.\nEffect size: Rounded to 2 decimals, the value for Pearson’s correlation coefficient is \n-0.14\n-0.03\n0.08\n-0.49.\nConfidence interval: Rounded to 2 decimals, the lower bound is \n-0.14\n-0.03\n0.08\n-0.49 and the upper bound is \n-0.14\n-0.03\n0.08\n-0.49.\n\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe scatterplot shows very little correlation between the two variables. The regression line is almost flat and there does not appear to be a clear pattern to the data points.\n\ndawtry_clean %&gt;% \n  ggplot(aes(x = age, y = redistribution)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  scale_y_continuous(name = \"Attitudes on Wealth Distribution\", \n                     breaks = c(1:6))\n\n\n\n\n\n\n\nFor our inferential statistics, the relationship is not statistically significant and the Pearson correlation coefficient is very weak, r (302) = -0.03, p = .625, 95% CI = [-0.14, 0.08]. Note there is a missing value for age, so we have one few participant / degrees of freedom.\n\ncorrelation(data = dawtry_clean, \n            select = \"age\", \n            select2 = \"redistribution\",  \n            method = \"pearson\", \n            alternative = \"two.sided\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\nage\nredistribution\n-0.0281749\n0.95\n-0.1402227\n0.0845855\n-0.4898215\n302\n0.6246159\nPearson correlation\n304",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#linear-regression-with-one-continuous-predictor",
    "href": "01-lm-continuous.html#linear-regression-with-one-continuous-predictor",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.3 Linear regression with one continuous predictor",
    "text": "1.3 Linear regression with one continuous predictor\nNow you know how to calculate a correlation in R, we can turn to simple linear regression as a more flexible tool for modelling the relationship between variables. In this chapter, we focus on the relationship between a continuous outcomeThe outcome (also known as the dependent variable) is the variable you are interested in seeing a potential change in. and one continuous predictorThe predictor (also known as an independent variable) is the variable you measure or manipulate to see how it is associated with changes in the outcome variable., before extending the framework to one categorical predictor in Chapter 2. There is also a chapter in the Handy Workbook (McAleer, 2023) dedicated to manually calculating simple linear regression.\n\n1.3.1 Activity 5- Calculating descriptive statistics\nFor all the information we want to include in a report, calculating descriptive statistics is helpful for the reader to show the context behind the results you present. Here, we can report the mean and standard deviation of our two variables.\n\ndawtry_clean %&gt;% \n  # pivot longer to avoid repeating yourself\n  pivot_longer(cols = fairness_satisfaction:redistribution,\n               names_to = \"Variable\", \n               values_to = \"Value\") %&gt;% \n  # group by Variable to get one value per variable\n  group_by(Variable) %&gt;% \n  # mean and SD, rounded to 2 decimals\n  summarise(mean_variable = round(mean(Value), 2),\n            sd_variable = round(sd(Value), 2))\n\n\n\n\nVariable\nmean_variable\nsd_variable\n\n\n\nfairness_satisfaction\n3.54\n2.02\n\n\nredistribution\n3.91\n1.15\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want some reinforcement of how these skills apply to published research, look at Table 1 in Dawtry et al. (2015). The means and standard deviations here (and the correlation in Activity 4) exactly reproduce the values they report.\n\n\nIf other types of descriptive statistic would be more suitable to your data, then you can just replace the functions you use within summarise().\n\n1.3.2 Activity 6 - Using the lm() function\nFor our research question of “is there a relationship between support for wealth redistribution and fairness and satisfaction”, we can address it with simple linear regression.\nInstead of a standardised correlation coefficient, we can frame it as whether knowing fairness and satisfaction can predict values of support for wealth redistribution. The design is still correlational, so it does not tell us anything about a causal relationship in isolation. We use the word predict in the statistical sense, where we can ask whether knowing values of one variable help us predict values of another variable with a degree of error.\nThe first step is to create an object (lm_redistribution) for the linear model.\n\nlm_redistribution &lt;- lm(redistribution ~ fairness_satisfaction,\n                        data = dawtry_clean)\n\nThe function lm() is built into R and is incredibly flexible for creating linear regression models.\n\nThe first argument is to specify a formula which defines our model. The first component (redistribution) is our outcome variable for what we are interested in modelling.\nThe tilde (~) separates the equation, where everything on the right is your predictor variable(s). In simple linear regression, we just have one predictor, which is fairness_satisfaction in our model here. This is saying we want to predict redistribution as our outcome from fairness_satisfaction as our predictor.\nWe then specify the data frame we want to use.\n\n\n\n\n\n\n\nNote\n\n\n\nIn some resources, you might see people enter the same model as redistribution ~ 1 + fairness_satisfaction. The 1 + component explicitly tells R to fit an intercept, plus a slope from fairness_satisfaction. R includes an intercept by default, so you do not need to add it, but some people like to include it for clarity.\n\n\nWhen you create this object, it stores a bunch of information, but does not really tell us all the statistics we expect. If you simply print the object in the console, it will tell you the intercept and coefficient(s), but none of the model fitting nor hypothesis testing values. If you look at the object in the R environment, you will see it is a list containing several elements. It stores things like the model, the residuals, and other information you can use.\n\nlm_redistribution\n\n\nCall:\nlm(formula = redistribution ~ fairness_satisfaction, data = dawtry_clean)\n\nCoefficients:\n          (Intercept)  fairness_satisfaction  \n               5.3169                -0.3975  \n\n\nTo get that extra information, we need to call the summary() function around the linear model object to explore it’s properties like estimates and model fit.\n\nsummary(lm_redistribution)\n\n\nCall:\nlm(formula = redistribution ~ fairness_satisfaction, data = dawtry_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9193 -0.5279  0.0233  0.4782  3.3634 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            5.31686    0.09488   56.04   &lt;2e-16 ***\nfairness_satisfaction -0.39754    0.02328  -17.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8218 on 303 degrees of freedom\nMultiple R-squared:  0.4905,    Adjusted R-squared:  0.4888 \nF-statistic: 291.7 on 1 and 303 DF,  p-value: &lt; 2.2e-16\n\n\nTo walk through the output, Call: summarises the model you specified. Residuals: provides a summary of the model residuals which we will come back to later. Coefficients: provides our model output, this time with inferential statistics. The two key lines are:\n\n(Intercept) - This is the value of the outcome when our predictor is set to 0. For a fairness and satisfaction value of 0, we would expect a value of 5.32 for redistribution. You get a p-value for this, but in isolation it is not too useful. It just compares the intercept estimate to 0 which typically you are not interested in.\nfairness_satisfaction - This is the regression slope or coefficient. This is the change in the outcome for every 1 unit change in the predictor. So, for every 1 unit increase in fairness and satisfaction, we expect support for wealth redistribution to decrease (as we have a negative value) by 0.40 units. This is consistent with the correlation as we have a negative relationship between the two variables. Looking at the p-value, this is statistically significant (p &lt; .001), suggesting we can reject the null hypothesis and conclude there is an effect here.\n\n\n\n\n\n\n\nDoes it matter if the slope is positive or negative?\n\n\n\nWhen you have a continuous predictor, the sign is important to keep in mind. A positive slope would mean an increase in the predictor is associated with increased values of your outcome. A negative slope would mean an increase in the predictor is associated with decreased values of your outcome. This is crucial for interpreting the coefficient.\n\n\nAt the bottom of the model output, you then get the fit statistics. Multiple \\(R^2\\) tells you how much variance in your outcome your predictor(s) explain. Adjusted \\(R^2\\) tends to be more conservative as it adjusts for the number of predictors in the model (something we will not cover until Chapter 3 on multiple regression), but they will be very similar when you have one predictor. Adjusted \\(R^2\\) is .49, suggesting fairness and satisfaction explains 49% of the variance in support for wealth redistribution.\nFinally, we have the model fit statistics to tell us whether the model explains a significant amount of variance in the outcome. With one predictor, the p-value next to the coefficient and next to the model fit will be identical, as one predictor is the whole model. The F-statistic is 291.7, the model degrees of freedom is 1, the residual degrees of freedom is 303, and the p-value is p &lt; .001.\n\n\n\n\n\n\nWhat does 2e-16 mean?\n\n\n\nFor the p-value here, the output looks a little weird. R reports very small or very large numbers using scientific notation to save space. We normally report p-values to three decimals, so we report anything smaller as p &lt; .001 to say it is smaller than this.\nIf you want to see the real number, you can use the following function which shows just how small the p-value is:\n\nformat(2e-16, scientific = FALSE)\n\n[1] \"0.0000000000000002\"\n\n\n\n\n\n\n\n\n\n\nHow are correlation and regression the same?\n\n\n\n\n\nIf you are interested in the relationship between the two concepts, we said correlation was a specific application of the general linear model. It describes the - standardised - covariation between two variables compared to their total variability. For values of -1 and 1, knowing the value of one variable perfectly correlates to the value of your other variable. As you approach 0, the relationship between the variables is less perfect, meaning there is more variability left over compared to the covariance.\nIn regression, we frame it as how much variance you explain compared to the total amount of variance. The more variance your predictor explains, the less unexplained variance there is left over. We no longer calculate r, we calculate \\(R^2\\) as the proportion of variance in your outcome explained by your model. A value of 0 would be you explain no variance and a value of 1 means you explain all the variance.\nYou can see the connection between the two by comparing the value of Pearson’s r from Activity 4 (-.70) to the value of \\(R^2\\) = .4905. If you take the square root to get r (sqrt(.4905)), you get .70, which is exactly the same absolute value since \\(R^2\\) can only be positive.\nSo, when you have a single continuous predictor, it is the exact same process as correlation, just expressed slightly different.\n\n\n\n\n1.3.3 Activity 7 - Calculating confidence intervals\nIn the standard lm() and summary() output, we get most of the key values we need for our inferential statistics, but the one thing missing is confidence intervals around our estimates. Fortunately, R has a built-in function called confint() for calculating confidence intervals using your linear model object.\n\nconfint(lm_redistribution)\n\n                           2.5 %     97.5 %\n(Intercept)            5.1301581  5.5035664\nfairness_satisfaction -0.4433442 -0.3517332\n\n\nNormally, you focus on the confidence interval around your slope estimate as the intercept is not usually super useful for interpreting your findings when you have a continuous predictor. Now, we can summarise the three key concepts of inferential statistics as:\n\nHypothesis testing: p &lt; .001, suggesting we can reject the null hypothesis assuming \\(\\alpha\\) = .05. Fairness and satisfaction is a significant predictor of support for wealth redistribution.\nEffect size: \\(b_1\\) = -0.40, suggesting fairness and satisfaction is a negative predictor.\nConfidence interval: [-0.44, -0.35], showing the precision around the slope estimate.\n\n\n\n\n\n\n\nTry this\n\n\n\nGreat work following along so far, but now it is time to test your understanding on a new set of variables. This time, use redistribution as your outcome, age as your predictor, and use dawtry_clean as your data. We can ask the same question as before: “What is the relationship between age and attitudes on wealth redistribution?”.\nApply simple linear regression to get your inferential statistics and answer the following questions:\n\nHypothesis testing: Assuming \\(\\alpha\\) = .05, age is a \nstatistically significant\nnon-significant predictor of support for wealth redistribution.\nEffect size: Rounded to 2 decimals, the age coefficient is \n4.01\n-0.003\n0.005\n0.22.\nConfidence interval: Rounded to 2 decimals, the lower bound of the age coefficient is \n3.59\n-0.01\n4.44\n0.01 and the upper bound is \n3.59\n-0.01\n4.44\n0.01.\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe conclusions are the same as when we calculated the correlation where age is not a statistically significant predictor of support for wealth redistribution. As a regression model, we get the same conclusions expressed in a slightly different way. Age is negative, but the size of the slope is very small (-0.003) and non-significant (p = .625). We explain pretty much no variance in support for wealth redistribution (\\(R^2\\) = .0008), so age is not very informative as a predictor.\n\n# Create lm object for age as a predictor\nlm_age &lt;- lm(redistribution ~ age,\n             data = dawtry_clean)\n\n# summary of the model object\nsummary(lm_age)\n\n\nCall:\nlm(formula = redistribution ~ age, data = dawtry_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.93382 -0.69527  0.08099  0.84379  2.13621 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.011931   0.216042   18.57   &lt;2e-16 ***\nage         -0.002693   0.005499   -0.49    0.625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.153 on 302 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.0007938, Adjusted R-squared:  -0.002515 \nF-statistic: 0.2399 on 1 and 302 DF,  p-value: 0.6246\n\n# confidence intervals around estimates\nconfint(lm_age)\n\n                  2.5 %     97.5 %\n(Intercept)  3.58679351 4.43706821\nage         -0.01351424 0.00812738\n\n\n\n\n\n\n1.3.4 Activity 8 - Centering and standardising predictors\nSo far, we have covered specifying your outcome and predictor variables as their raw values in the data. However, there are two variations that are useful to understand: centering and standardising predictors. These do not change the model fitting or p-values, but change how you interpret the intercept and/or slope.\n\n1.3.4.1 Centering predictors\nCenteringUsually, centering a predictor means subtracting the mean from each value, so the mean is 0. You can then interpret the intercept as the value of your outcome for the mean value of your predictor(s). predictors is where you change the values of your predictor, but not their scale. Typically, this means substracting the mean of your predictor from each observation. This changes how you interpret the intercept of your regression model.\nRemember the interpretation of the intercept is the predicted value of your outcome when your predictor is set to 0. If 0 is not present in your data or a value of 0 would be uninformative, the intercept can be difficult to interpret. When you center your predictor, 0 becomes the mean value of your predictor. So, the intercept is now the predicted value of your outcome for the mean value of your predictor, but the slope itself does not change. You can see the impact of this by plotting the data side by side in Figure 1.1.\n\ndawtry_clean &lt;- dawtry_clean %&gt;% \n  # subtract fairness values from mean of fairness\n  mutate(fairness_center = fairness_satisfaction - mean(fairness_satisfaction))\n\n\n\n\n\n\n\n\nFigure 1.1: Top: The relationship between wealth redistribution and perceived fairness and satisfaction using raw values. Bottom: The relationship after centering perceived fairness and satisfaction values.\n\n\n\n\nThe relationship between the two variables is exactly the same, but the values of fairness and satisfaction shifted so the mean is 0. If you create a new linear model object, you can see the difference this makes to the output.\n\nlm_center &lt;- lm(redistribution ~ fairness_center, \n                data = dawtry_clean)\n\nsummary(lm_center)\n\n\nCall:\nlm(formula = redistribution ~ fairness_center, data = dawtry_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9193 -0.5279  0.0233  0.4782  3.3634 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.90984    0.04706   83.09   &lt;2e-16 ***\nfairness_center -0.39754    0.02328  -17.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8218 on 303 degrees of freedom\nMultiple R-squared:  0.4905,    Adjusted R-squared:  0.4888 \nF-statistic: 291.7 on 1 and 303 DF,  p-value: &lt; 2.2e-16\n\n\nEvery single one of the values remains the same apart from the intercept. Now, we can interpret it as the predicted value of redistribution when fairness and satisfaction is set to 0, i.e., the mean value. So, for the mean value of fairness and satisfaction, we would predict a value of 3.91 for redistribution.\n\n1.3.4.2 Standardising predictors\nStandardisingStandardising involves substracting the variable mean from each value and dividing it by the variable standard deviation. It then has the property of a mean of 0 and standard deviation of 1, so you interpret the units as standard deviations. predictors is where you first convert your values to z-scores. This means you interpret the values as standard deviations rather than your original units. This is more useful in multiple regression (Chapter 3) to compare the magnitude of predictors, but it is useful to get used to now when you only have one predictor to focus on.\nThe first step is to standardise all your variables, not just the predictor this time. This involves subtracting the mean of your variable from each value, and dividing by the standard deviation of the variable. They now have a mean of 0 and a standard deviation of 1.\n\n# Be careful with the bracket placement to subtract the mean first\ndawtry_clean &lt;- dawtry_clean %&gt;% \n  mutate(redistribution_std = (redistribution - mean(redistribution)) / sd(redistribution),\n         fairness_std = (fairness_satisfaction - mean(fairness_satisfaction)) / sd(fairness_satisfaction))\n\nOnce we enter them into the model, we no longer have values in the original units of measurement, we now have them expressed as standard deviations.\n\nlm_standardised &lt;- lm(redistribution_std ~ fairness_std, \n                      data = dawtry_clean)\n\nsummary(lm_standardised)\n\n\nCall:\nlm(formula = redistribution_std ~ fairness_std, data = dawtry_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.53979 -0.45930  0.02026  0.41604  2.92617 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.387e-16  4.094e-02    0.00        1    \nfairness_std -7.003e-01  4.101e-02  -17.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.715 on 303 degrees of freedom\nMultiple R-squared:  0.4905,    Adjusted R-squared:  0.4888 \nF-statistic: 291.7 on 1 and 303 DF,  p-value: &lt; 2.2e-16\n\n\nLike centering, the model fit and p-values do not change again, apart from the intercept. The relationship between the variables is exactly the same, but we changed their units. The intercept is tiny and close enough to zero that the p-value is 1.\nMore importantly, the slope is now expressed in standard deviations. Annoyingly, R prints the values in scientific notation, so this can be awkward to read (remember the format() function). Now, for every 1 standard deviation increase in our predictor, we predict the outcome to decrease by 0.70 standard deviations.\n\n\n\n\n\n\nTip\n\n\n\nIt is important to demonstrate the underlying concepts first but if you want a shortcut without needing to standardise all your variables, the effectsize package has a handy function called standardize_parameters() which you can apply to your initial lm() object.\n\nstandardize_parameters(lm_redistribution)\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n(Intercept)\n0.00000\n0.95\n-0.0805627\n0.0805627\n\n\nfairness_satisfaction\n-0.70034\n0.95\n-0.7810351\n-0.6196449",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#checking-assumptions",
    "href": "01-lm-continuous.html#checking-assumptions",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.4 Checking assumptions",
    "text": "1.4 Checking assumptions\nFor the inferential statistics to work as intended, the model makes certain assumptions about the data you are putting into it and the accuracy of the inferences depends on how sensible these assumption are. Remember these functions will always work even if the numbers you enter are nonsense, so it’s important for you as the researcher to recognise when it’s appropriate to use these techniques and when it is not.\n\n1.4.1 Activity 9 - Diagnostic plots for linear regression\nAs a reminder, the assumptions for simple linear regression are:\n\nThe outcome is interval/ratio level data.\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\nThe predictors have non-zero variance.\nThe relationship between the outcome and predictor is linear.\nThe residuals should be normally distributed.\nThere should be homoscedasticity.\n\nAssumptions 1-4 are pretty straight forward as they relate to your understanding of the design or a simple check on the data for non-zero variance (the responses are not all the exact same value).\nAssumptions 5-7 require diagnostic checks on the residualsThe difference between the observed value in the data and the predicted value from the model given its assumptions. from the model. The residuals are the difference between your observed values in the data and the values your model predicts given it’s assumptions. If you remember back, we highlighted the model output mentioned Residuals: and they are saved within the model object.\n\n# head shows the first 6 values \nhead(lm_redistribution$residuals)\n\n         1          2          3          4          5          6 \n 0.5806764 -0.6754769  0.4208311  0.2159084 -0.5279383 -0.5730156 \n\n\n\n\n\n\n\n\nWhat does the $ symbol mean?\n\n\n\nWe have not used this symbol in the book yet, but it is a base R operator for extracting information. You use it to access specific components within a data frame or object.\nTry the following in the console to see what it does:\n\ndawtry_clean$age\nlm_redistribution$coefficients\n\nIf you type an object name into the console and add the $, you will see all the components appear to auto complete.\n\n\nIn your reading, you might see individual statistical tests to check these assumptions, but they have more limitations than benefits. The best way to check the assumptions is through diagnostic plots which express the model residuals in different ways. We want to walk through this longer way of checking assumptions to develop a solid understanding before showing you a shortcut to see them all below.\nIn the code below, we use a format you will be less familiar with as all these functions come from base R. If you just run the code for the diagnostic plots plot(lm_redistribution), each one gets individually printed to your Plots window. Here, we create a 2x2 panel to show them all together.\n\n# Change the panel layout to 2 x 2\npar(mfrow=c(2,2))\n\n# plot the diagnostic plots \nplot(lm_redistribution)\n\n\n\n\n\n\n\nFor more information on each of these plots, see this great resource by Kim (2015) via the University of Virginia, but we will break the key ones down below.\n\n1.4.2 Checking linearity\nTo isolate each plot, we can use the which argument. Plot 1 is a residuals vs fitted plot and helps us check linearity by showing the residuals on the y-axis and the fitted (predicted) values on the x-axis.\n\nplot(lm_redistribution, \n     which = 1)\n\n\n\n\n\n\n\nHere, you are looking out for a roughly flat horizontal red line. Common patterns to look out for if there is a problem are when the line has an obvious curve to look like a hump or several bends to look like an S.\nThe only downside to using diagnostic plots is it takes experience to recognise when there is nothing wrong with a regression model compared to when it violates the assumptions. It is easy to see a little deviation and think there is some drastically wrong. Our advice is if you squint and it looks fine, it is probably fine. You are looking for clear and obvious deviations from what you expect and all the models we use in Chapters 1 to 3 are intentionally fine to develop foundational skills. In Chapter 4 and 5, we then introduce you to more problematic cases and what your options are, and introducing more flexible models.\n\n\n\n\n\n\nError mode\n\n\n\nIf you want to save these plots to add into a report, you might try using ggsave(). However, it will not work as these plots have not been created by ggplot2.\nTo save these plots, you can either right click and choose save as to save on your computer. Alternatively, if they open in the Plots window, you can click on Export and save them as an image or PDF to insert into your documents.\n\n\n\n1.4.3 Checking normality\nPlot 2 is a qq-plot (quantile-quantile plot) and helps us check the normality of the model residuals by showing the standardised residuals on the y-axis and the theoretical quantiles on the x-axis. A common misconception is your variables should all be normally distributed, but it is actually the model residuals which should be normal.\n\nplot(lm_redistribution, \n     which = 2)\n\n\n\n\n\n\n\nIn this plot, you are looking for the data points to roughly follow the dashed line. The idea is there should be a linear relationship between the residuals and the values we would expect under a normal distribution.\nLike the other diagnostic plots, it is tempting to think there are problems where there are none. The vast majority of the points here follow the line nicely, but tail off a little at the extremes. It flags the points with the largest deviations but there do not appear to be any obvious problems.\nWhen there are problems with normality, you are looking for obvious deviations, like the points curving around in a banana shape, or snaking around like an S.\n\n1.4.4 Checking homoscedasticity\nPlot 3 is a scale-location plot and helps us check homoscedasticity by showing the square root of the standardised residuals on the y-axis and the fitted values on the x-axis. Homoscedasticity is where the variance of the residuals is approximately equal across the range of your predictor(s).\n\nplot(lm_redistribution, \n     which = 3)\n\n\n\n\n\n\n\nIn this plot, you are looking out for a roughly random spread of points as you move from one end of the x-axis to the other. The red line should be roughly flat and horizontal.\nWhen there is heteroscedasticity, the characteristic patterns to look out for are a kind of arrow shape where there is a wide spread of points at one end and decreases to a denser range at the other end, or a bow tie where there is a wide spread of points at each end and dense in the middle.\n\n1.4.5 Checking influential cases\nFinally, there are two main plots to help us identify influential cases. You might have heard of the term outlier before and this is one way of classifying data points that are different enough to the rest of the data in a regression model. It is not strictly an assumption of linear regression, but it can affect the other assumptions. Identifying outliers is a complex decision and we will explore your options in the course materials and Chapter 14 on data screening.\nIf you only run plot(lm_redistribution) and cycle through the plots, you do not see this version. This plot shows values of Cook’s distance for each observation in your data along the x-axis. Cook’s distance measures the influence of deleting a given observation, where higher values mean deleting that observation results in a larger change to the model estimates. There are different thresholds in the literature, but estimates range from 1, 0.5, to 4/n. We explore the decision making around this and your options in Chapter 4.\n\nplot(lm_redistribution, \n     which = 4)\n\n\n\n\n\n\n\nFinally, we get a residuals vs leverage plot to show influential cases in a slightly different way. Instead of just the Cook’s distance value of each observation, it plots the standardised residuals against leverage values.\n\nplot(lm_redistribution, \n     which = 5)\n\n\n\n\n\n\n\nInfluential points and potential outliers would have high leverage values and the plot will show a threshold of Cook’s distance as red dashed lines. In this plot, they are not visible as there is no value with a big enough leverage value, but you would be looking for data points outside this threshold to identify influential values.\n\n1.4.6 Checking all the assumptions\nNow we have covered the individual diagnostic plots, there is a handy function called check_model() from the performance package. This function reports all the diagnostic checks from plot(), but tidies up the presentation and has some useful reminders of what you are looking for.\n\ncheck_model(lm_redistribution)\n\n\n\n\n\n\n\nThe key difference is you get a posterior predictive check (essentially comparing values you observe compared to what your model predicts). This will be an idea we return to in a much more important role when we cover Bayesian statistics later in the course, but it’s useful for these models too. The qq-plot for normality of residuals also looks a little different. Instead of a kind of angled line, the residuals are expressed as deviations instead, so the points should be close to a flat horizontal line. This version can make smaller deviations look worse, so keep in mind again you are looking for clear deviations in the overall pattern.\n\n\n\n\n\n\nNote\n\n\n\nThe performance version of the diagnostic plots are actually created using ggplot2, so the function ggsave() would work here if you need to save the plot to add into your report.\n\n\n\n\n\n\n\n\nTry this\n\n\n\nIn activity 7, you should have calculated the relationship between age and support for redistribution for your independent task. Using the model object lm_age, work through assumptions for simple linear regression and make a note of whether you think it meets the assumptions, or there might be any problems. Some of the assumptions you consider what you know about the design, while others you need the diagnostic plots.\n\nThe outcome is interval/ratio level data.\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\nThe predictors have non-zero variance.\nThe relationship between the outcome and predictor is linear.\nThe residuals should be normally distributed.\nThere should be homoscedasticity.\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nThe outcome is interval/ratio level data.\n\nThere is a debate here we cover in the course materials, but there is an argument you can treat the average of multiple Likert items as interval, but you need to be careful.\n\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\n\nOur predictor age is nicely ratio.\n\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\n\nYou need to know this from the design / data collection, but it appears to be the case in this study.\n\nThe predictors have non-zero variance.\n\nThere are a range of ages in the data.\n\nThe relationship between the outcome and predictor is linear.\n\nLooking at the first plot, the red line is pretty flat and horizontal, so we are happy with this.\n\nplot(lm_age, which = 1)\n\n\n\n\n\n\n\n\nThe residuals should be normally distributed.\n\nThe qq-plot is fine for the vast majority of the range. We just have a few deviations in the extreme ends of the x-axis, but this is a byproduct of using a scale score as our outcome as responses cannot go beyond 1-6.\n\nplot(lm_age, which = 2)\n\n\n\n\n\n\n\n\nThere should be homoscedasticity.\n\nThe red line is pretty flat and it looks like there is a fairly even range of values across the x-axis range.\n\nplot(lm_age, which = 3)\n\n\n\n\n\n\n\nAll in all, there do not appear to be any issues with the assumptions here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#reporting-your-results",
    "href": "01-lm-continuous.html#reporting-your-results",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.5 Reporting your results",
    "text": "1.5 Reporting your results\nNow we have some results to go with, there are a few recommendations on how to communicate that information. In psychology (and other disciplines), we tend to follow the American Psychological Association (APA) formatting guidelines as they provide a comprehensive standardised style to make sure information is being reported as easily digestible and consistent as possible. You can see this PDF online for a little cheat sheet for numbers and statistics, but we will outline some key principles to ensure you provide your reader with enough information.\n\nExplain to the reader what your linear regression model was. For example, what was your outcome and predictor variable?\nReport descriptive statistics to help contextualise your findings. For example, the mean/standard deviation for your outcome and continuous predictor.\nProvide an appropriate data visualisation to help communciate key patterns to the reader. For example, a scatterplot for the relationship between your outcome and predictor.\nReport all three key inferential statistics concepts for the coefficient: the slope, the confidence interval around your slope, and the p-value for hypothesis testing. When you have one predictor in simple linear regression, you typically focus on the slope as your key effect size that helps address your research question and hypothesis. APA style rounds numbers to 2 decimal places when numbers can be bigger than 1, and 3 decimals with no leading zero when it cannot be bigger than 1. When you report the unstandardised slope, you use the symbol \\(b_1\\) but for the standardised slope, you use Beta instead \\(\\beta_1\\).\nProvide an overview of the model fit statistics for whether your model explained a significant amount of variance in your outcome. Remember: the p-value for your model will be the same as for the slope in simple linear regression.\n\nFor our main example, we could summarise the findings as:\n“Our research question was: is there a relationship between support for wealth redistribution and fairness and satisfaction with the current system? To test this, we applied simple linear regression using fairness and satisfaction as a predictor and support for wealth redistribution as our outcome. Figure 1 shows a scatterplot of the relationship.\n\n\n\n\n\n\n\n\nOur model explained a statistically significant amount of variance in our outcome (adjusted \\(R^2\\) = .489, F(1, 303) = 291.70, p &lt; .001). Fairness and satisfaction was a negative predictor, where for every 1-unit increase we expect support for redistribution to decrease by 0.40 (\\(b_1 = -0.40\\), 95% CI = [-0.44, -0.35], p &lt; .001).”\nNote: we have not included an APA formatted Figure title here as it is not easy to format in our book, so refer to the course materials for guidance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#test-yourself",
    "href": "01-lm-continuous.html#test-yourself",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.6 Test Yourself",
    "text": "1.6 Test Yourself\nTo end the chapter, we have some knowledge check questions to test your understanding of the concepts we covered in the chapter.\nFor this chapter’s knowledge check section, we have another example of linear regression from Dawtry et al. (2015). Feel free to create this model yourself, but we will show you some output and ask you questions based on it.\nFor this model, we focus on the two variables estimated population inequality index (Population_Inequality_Gini_Index) and support for wealth redistribution (redistribution). Check back to the code book in Activity 2 if you need a reminder of what the variables mean.\nQuestion 1. In the scatterplot of the relationship below, this shows a negative relationship between the inequality index and support for redistribution: \nTRUE\nFALSE.\n\n\n\n\n\n\n\n\nQuestion 2 For the next few questions, we have the output from a linear regression model and we would like you to interpret it.\n\n\n\nCall:\nlm(formula = redistribution ~ Population_Inequality_Gini_Index, \n    data = dawtry_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9478 -0.6384  0.1389  0.8511  2.3854 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      3.097452   0.316914   9.774  &lt; 2e-16 ***\nPopulation_Inequality_Gini_Index 0.022879   0.008734   2.619  0.00925 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.139 on 303 degrees of freedom\nMultiple R-squared:  0.02214,   Adjusted R-squared:  0.01892 \nF-statistic: 6.861 on 1 and 303 DF,  p-value: 0.009251\n\n\nThe outcome variable in this model is \nAttitudes on Wealth Redistribution\nPopulation Inequality Index and the predictor variable is \nAttitudes on Wealth Redistribution\nPopulation Inequality Index.\nQuestion 3 Rounded to 2 decimals, when the predictor is 0, we predict a value of  for our outcome variable.\nQuestion 4 The predictor is \nsignificant\nnon-significant with a p-value of .\nQuestion 5 The predictor is \npositive\nnegative, where we expect for every 1-unit increase in the predictor a  -unit \nincrease\ndecrease in the outcome.\n\n\n\n\nDawtry, R. J., Sutton, R. M., & Sibley, C. G. (2015). Why Wealthier People Think People Are Wealthier, and Why It Matters: From Social Sampling to Attitudes to Redistribution. Psychological Science, 26(9), 1389–1400. https://doi.org/10.1177/0956797615586560",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#words-from-this-chapter",
    "href": "01-lm-continuous.html#words-from-this-chapter",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.7 Words from this Chapter",
    "text": "1.7 Words from this Chapter\nBelow you will find a list of words that were used in this chapter that might be new to you in case it helps to have somewhere to refer back to what they mean. The links in this table take you to the entry for the words in the PsyTeachR Glossary. Note that the Glossary is written by numerous members of the team and as such may use slightly different terminology from that shown in the chapter.\n\n\n\nterm\ndefinition\n\n\n\nCentered predictors\nUsually, centering a predictor means subtracting the mean from each value, so the mean is 0. You can then interpret the intercept as the value of your outcome for the mean value of your predictor(s).\n\n\nlist\n\n\n\noutcome\nThe outcome (also known as the dependent variable) is the variable you are interested in seeing a potential change in.\n\n\nPearson\nA standardised measure of the linear relationship between two variables that makes stringent assumptions about the population.\n\n\npredictor\nThe predictor (also known as an independent variable) is the variable you measure or manipulate to see how it is associated with changes in the outcome variable.\n\n\nresiduals\nThe difference between the observed value in the data and the predicted value from the model given its assumptions.\n\n\nSpearman\nA standardised measure of the relationship between two variables that assumes a monotonic - but not necessarily a linear - relationship and makes less stringent assumptions about the population.\n\n\nStandardised predictors\nStandardising involves substracting the variable mean from each value and dividing it by the variable standard deviation. It then has the property of a mean of 0 and standard deviation of 1, so you interpret the units as standard deviations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#end-of-chapter",
    "href": "01-lm-continuous.html#end-of-chapter",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.7 End of chapter",
    "text": "1.7 End of chapter\nGreat work, that was your first chapter working on inferential statistics!\nIn the next chapter, we reinforce most of the content by applying simple linear regression to a categorical predictor. This is when you want to test for differences between two groups on your outcome instead of testing the relationship between two continuous variables.\n\n\n\n\nDawtry, R. J., Sutton, R. M., & Sibley, C. G. (2015). Why Wealthier People Think People Are Wealthier, and Why It Matters: From Social Sampling to Attitudes to Redistribution. Psychological Science, 26(9), 1389–1400. https://doi.org/10.1177/0956797615586560",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html",
    "href": "02-lm-categorical.html",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "",
    "text": "2.1 Chapter preparation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#chapter-preparation",
    "href": "02-lm-categorical.html#chapter-preparation",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "",
    "text": "2.1.1 Introduction to the data set\nFor most of this chapter, we are using open data from Lopez et al. (2023). The abstract of their article is:\n\nImagine a bowl of soup that never emptied, no matter how many spoonfuls you ate—when and how would you know to stop eating? Satiation can play a role in regulating eating behavior, but research suggests visual cues may be just as important. In a seminal study by Wansink et al. (2005), researchers used self-refilling bowls to assess how visual cues of portion size would influence intake. The study found that participants who unknowingly ate from self-refilling bowls ate more soup than did participants eating from normal (not self-refilling) bowls. Despite consuming 73% more soup, however, participants in the self-refilling condition did not believe they had consumed more soup, nor did they perceive themselves as more satiated than did participants eating from normal bowls. Given recent concerns regarding the validity of research from the Wansink lab, we conducted a preregistered direct replication study of Wansink et al. (2005) with a more highly powered sample (N = 464 vs. 54 in the original study). We found that most results replicated, albeit with half the effect size (d = 0.45 instead of 0.84), with participants in the self-refilling bowl condition eating significantly more soup than those in the control condition. Like the original study, participants in the selfrefilling condition did not believe they had consumed any more soup than participants in the control condition. These results suggest that eating can be strongly controlled by visual cues, which can even override satiation.\n\nIn summary, they replicated an (in)famous experiment that won the Ig-Nobel prize. Participants engaged in a intricate setting (seriously, go and look at the diagrams in the article) where they ate soup from bowls on a table. In the control group, participants could eat as much soup as they wanted and could ask for a top-up from the researchers. In the experimental group, the soup bowls automatically topped up through a series of hidden tubes under the table. The idea behind the control group is they get an accurate visual cue by the soup bowl reducing, and the experimental group get an inaccurate visual cue by the soup bowl seemingly never reducing. So, the inaccurate visual cue would interfere with natural signs of getting full and lead to people eating more.\nIn the original article, participants in the experimental group ate more soup than participants in the control group, but the main author was involved in a series of research misconduct cases. Lopez et al. (2023) wanted to see if the result would replicate in an independent study, so they predicted they would find the same results. In this chapter, we will explore the difference between the control and experimental groups on several variables in their data set.\n\n2.1.2 Organising your files and project for the chapter\nBefore we can get started, you need to organise your files and project for the chapter, so your working directory is in order.\n\nIn your folder for statistics and research design Stats_Research_Design, create a new folder called 02_regression_categorical. Within 02_regression_categorical, create two new folders called data and figures.\nCreate an R Project for 02_regression_categorical as an existing directory for your chapter folder. This should now be your working directory.\nCreate a new Quarto document and give it a sensible title describing the chapter, such as 02 t-tests and Regression. Save the file in your 02_regression_categorical folder.\nWe are working with a new data set, so please save the following data file: Lopez_2023.csv. Right click the link and select “save link as”, or clicking the link will save the files to your Downloads. Make sure that you save the file as “.csv”. Save or copy the file to your data/ folder within 02_regression_categorical.\n\nYou are now ready to start working on the chapter!\n\n2.1.3 Activity 1 - Read and wrangle the data\nAs the first activity, try and test yourself by completing the following task list to practice your data wrangling skills. In this example, there is not loads to do, you just need to tidy up some variables. Create a final object called lopez_clean to be consistent with the tasks below. If you want to focus on t-tests and regression, then you can just type the code in the solution.\n\n\n\n\n\n\nTry this\n\n\n\nTo wrangle the data, complete the following tasks:\n\n\nLoad the following packages:\n\ntidyverse\neffectsize\nperformance\n\n\nRead the data file data/Lopez_2023.csv to the object name lopez_data.\n\nCreate a new object called lopez_clean based on lopez_data:\n\nModify the variable Condition to turn it into a factor.\nCreate a new variable called Condition_label by recoding Condition. “0” is the “Control” group and “1” is the “Experimental” group.\n\n\n\nYour data should look like this to be ready to analyse:\n\n\nRows: 464\nColumns: 10\n$ ParticipantID      &lt;dbl&gt; 1002, 1004, 1007, 1016, 1018, 1021, 1022, 1024, 102…\n$ Sex                &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, …\n$ Age                &lt;dbl&gt; 18, 19, 19, 21, 20, 20, 21, 21, 19, 20, 21, 20, 21,…\n$ Ethnicity          &lt;dbl&gt; 7, 3, 3, 4, 1, 3, 1, 6, 4, 7, 1, 3, 3, 4, 7, 2, 3, …\n$ OzEstimate         &lt;dbl&gt; 3.0, 2.0, 1.0, 3.0, 5.0, 1.0, 1.0, 3.0, 4.0, 1.0, 4…\n$ CalEstimate        &lt;dbl&gt; 65, 10, 20, 25, 50, 5, 20, 180, 470, 50, 130, 100, …\n$ M_postsoup         &lt;dbl&gt; 3.3, 3.1, 43.4, 5.5, 6.0, 0.8, 3.8, 4.5, 7.9, 8.1, …\n$ F_CaloriesConsumed &lt;dbl&gt; 73.19441, 68.75839, 962.61743, 121.99069, 133.08075…\n$ Condition          &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Condition_label    &lt;chr&gt; \"Control\", \"Control\", \"Control\", \"Control\", \"Contro…\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nYou should have the following in a code chunk:\n\n# load the relevant packages\nlibrary(effectsize)\nlibrary(performance)\nlibrary(tidyverse)\n\n# Read the Lopez_2023.csv file \nlopez_data &lt;- read_csv(\"data/Lopez_2023.csv\")\n\n# turn condition into a factor and recode\nlopez_clean &lt;- lopez_data %&gt;% \n  mutate(Condition = as.factor(Condition),\n         Condition_label = case_match(Condition,\n                                      \"0\" ~ \"Control\",\n                                      \"1\" ~ \"Experimental\"))\n\n\n\n\n\n2.1.4 Activity 2 - Explore the data\n\n\n\n\n\n\nTry this\n\n\n\nAfter the wrangling steps, try and explore lopez_clean to see what variables you are working with. For example, opening the data object as a tab to scroll around, explore with glimpse(), or try plotting some of the individual variables.\n\n\nIn lopez_clean, we have the following variables:\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\nParticipantID\ndouble\nParticipant ID number.\n\n\nSex\ndouble\nParticipant sex.\n\n\nAge\ndouble\nParticipant age in years.\n\n\nEthnicity\ndouble\nParticipant ethnicity.\n\n\nOzEstimate\ndouble\nEstimated soup consumption in ounces (Oz).\n\n\nCalEstimate\ndouble\nEstimated soup consumption in calories (kcals).\n\n\nM_postsoup\ndouble\nActual soup consumption in ounces (Oz).\n\n\nF_CaloriesConsumed\ndouble\nActual soup consumption in calories (kcals).\n\n\nCondition\ninteger\nCondition labelled numerically as 0 (Control) and 1 (Experimental).\n\n\nCondition_label\ncharacter\nCondition as a direct label: Control and Experimental.\n\n\n\nWe will use this data set to demonstrate t-tests and regression when you have one categorical predictor.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#comparing-differences-using-the-t-test",
    "href": "02-lm-categorical.html#comparing-differences-using-the-t-test",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.2 Comparing differences using the t-test",
    "text": "2.2 Comparing differences using the t-test\nLike correlations are a specific application of the general linear model for the relationship between two continuous variables, t-tests are a specific application for the difference between two groups. Before we demonstrate how you can express this kind of design as a regression model, we cover t-tests so you know how to calculate and interpret them when you come across them in your research.\n\n2.2.1 Activity 3 - Visualising the difference\nTo visualise the difference between two groups, it is useful to create something like a boxplot early for yourself, then provide a more professional looking violin-boxplot to help communicate your results. For most of the demonstrations in this chapter, we will try and answer the research question: “Is there a difference in actual calories consumed between the control and experimental groups?”\n\n\n\n\n\n\nTry this\n\n\n\nUsing your data visualisation skills from Repro Res, recreate the violin-boxplot below using the variables F_CaloriesConsumed and Condition_label from lopez_clean.\n\n\n\n\n\n\n\n\nLooking at the graph, the \nControl\nExperimental group consumed more calories on average.\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe violin-boxplot shows the experimental group who had the biased visual cues consumed more soup in calories than the control group who had the accurate visual cues.\nYou should have the following in a code chunk:\n\nlopez_clean %&gt;% \n  ggplot(aes(y = F_CaloriesConsumed, x = Condition_label, fill = Condition_label)) +\n  geom_violin(alpha = 0.5) + \n  geom_boxplot(width = 0.2, \n               fatten = NULL) + \n  stat_summary(fun = \"mean\", \n               geom = \"point\") +\n  stat_summary(fun.data = \"mean_cl_boot\", # confidence interval\n               geom = \"errorbar\", \n               width = 0.1) +\n  scale_fill_viridis_d(option = \"E\") + \n  scale_y_continuous(name = \"Actual Calories Consumed (kcals)\") +\n  scale_x_discrete(name = \"Study Condition\") + \n  guides(fill = FALSE) + \n  theme_classic()\n\n\n\n\n\n2.2.2 Activity 4 - Using the t.test() function\nA t-test is a specific application of the general linear model. In this test, we express the difference in an outcome between two groups as a kind of standardised mean difference. If you are interested, see the Handy Workbook (McAleer, 2023) for the calculations behind the Student and Welch t-test. Conceptually, a t-test is the difference between two groups divided by the standard error of the difference. There are two main versions of a t-test:\n\nStudent t-testCalculating a t-value based on the mean difference divided by the pooled standard deviation. In the Student t-test, we times the pooled standard deviation by a term containing the sample sizes of each group.\nWelch t-testCalculating a t-value based on the mean difference divided by a term containing the variance of each group. We also correct the degrees of freedom for the difference in variances.\n\nThere is a function built into R to calculate the t-test: t.test(). The function requires:\n\nA formula like lm() where you specify the outcome/dependent variable and the predictor/independent variable in the form outcome ~ predictor.\nThe data set you want to use.\n\nFor our lopez_clean data, we would run the following code for a two-tailed Welch t-test:\n\nt.test(formula = F_CaloriesConsumed ~ Condition_label, \n       data = lopez_clean)\n\n\n    Welch Two Sample t-test\n\ndata:  F_CaloriesConsumed by Condition_label\nt = -4.8578, df = 453.45, p-value = 1.638e-06\nalternative hypothesis: true difference in means between group Control and group Experimental is not equal to 0\n95 percent confidence interval:\n -88.55610 -37.54289\nsample estimates:\n     mean in group Control mean in group Experimental \n                  196.6818                   259.7313 \n\n\nFor the three key concepts of inferential statistics, we get\n\n\nHypothesis testing: p &lt; .001, suggesting we can reject the null hypothesis assuming \\(\\alpha\\) = .05.\n\n\n\n\n\n\n\nWhat does 1.638e-06 mean?\n\n\n\nRemember: R reports very small or very large numbers using scientific notation to save space. We normally report p-values to three decimals, so we report anything smaller as p &lt; .001 to say it is smaller than this.\nIf you want to see the real number, you can use the following function which shows just how small the p-value is:\n\nformat(1.638e-06, scientific = FALSE)\n\n[1] \"0.000001638\"\n\n\n\n\n\n\nEffect size: Somewhat annoyingly, we do not directly get the mean difference between groups as a raw/unstandardised mean difference. We must manually calculate it by subtracting the means of each group (196.6818 - 259.7313 = -63.05). So, those in the experimental group ate on average 63 more calories of soup than the control group.\n\n\n\n\n\n\n\nDoes it matter whether the difference is positive or negative?\n\n\n\nFor effect sizes describing the difference between two groups, it is the absolute difference which is important, providing it is consistent with your predictions (if applicable). If you entered the groups the other way around, the mean difference would become 259.7313 - 196.6818 = 63.05. The same applies when we calculate a standardised mean difference like Cohen’s d later.\n\n\n\n\nConfidence interval: [-88.56, -37.54], although we do not get the mean difference, we get the confidence interval around the mean difference.\n\nTo summarise: A Welch t-test showed participants in the experimental group ate significantly more calories of soup than participants in the control group, t (453.45) = -4.86, p &lt; .001. On average, those in the experimental group ate 63.05 (95% CI = [37.54, 88.56]) more calories than those in the control group.\nWhen you have statistics software like R to do the heavy lifting for you, there is not really a scenario where you would use the Student t-test anymore, but if you did, you can use the var.equal argument to say you assume there are equal variances in each group:\n\nt.test(formula = F_CaloriesConsumed ~ Condition_label, \n       data = lopez_clean, \n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  F_CaloriesConsumed by Condition_label\nt = -4.8625, df = 462, p-value = 1.591e-06\nalternative hypothesis: true difference in means between group Control and group Experimental is not equal to 0\n95 percent confidence interval:\n -88.52983 -37.56915\nsample estimates:\n     mean in group Control mean in group Experimental \n                  196.6818                   259.7313 \n\n\nYou can see the main difference between the two versions is the Welch t-test corrects the degrees of freedom, so they are a decimal. While the Student t-test does not correct the degrees of freedom, so they are predictably N - 2.\nTo summarise: A Student t-test showed participants in the experimental group ate significantly more calories of soup than participants in the control group, t (462) = -4.86, p &lt; .001. On average, those in the experimental group ate 63.05 (95% CI = [37.57, 88.53]) more calories than those in the control group.\nOne further useful argument is specifying a one-tailed test if you have a specific prediction. The only downside to using linear models later is there is not a simple argument to apply a one-tailed test.\n\nt.test(formula = F_CaloriesConsumed ~ Condition_label, \n       data = lopez_clean, \n       alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  F_CaloriesConsumed by Condition_label\nt = -4.8578, df = 453.45, p-value = 8.188e-07\nalternative hypothesis: true difference in means between group Control and group Experimental is less than 0\n95 percent confidence interval:\n     -Inf -41.6571\nsample estimates:\n     mean in group Control mean in group Experimental \n                  196.6818                   259.7313 \n\n\nThe difference here is specifying the alternative argument. You can use “less” or “greater” depending if you predict a negative (group A &lt; group B) or positive difference (group A &gt; group B).\n\n2.2.3 Activity 5 - Calculating Cohen’s d\nRaw/unstandardised effect sizes are great for putting results in context, particularly when the units are comparable across studies. For our outcome in this study, differences in calories are easy to put in context.\nAlternatively, it can be useful to calculate standardised effect sizes. This helps for power analyses and when you want to compare across comparable studies with slightly different measurement scales.\nThere are different formulas for calculating Cohen’s d, but if you know the t-value and degrees of freedom, you can calculate Cohen’s d through:\n\\(d = \\frac{2t}{\\sqrt{df}} = \\frac{-9.725}{21.49} = -0.45\\)\nNote the different formulas make different assumption and have various rounding errors from the statistics available, so this value will be slightly different to what we calculate shortly.\nIt is important to know the concepts before you use shortcuts, but there is the cohens_d() function from the effectsize package which uses the same format as t.test().\n\ncohens_d(F_CaloriesConsumed ~ Condition_label, \n         data = lopez_clean)\n\n\n\n\nCohens_d\nCI\nCI_low\nCI_high\n\n\n-0.4523004\n0.95\n-0.6366884\n-0.2674345\n\n\n\n\n\n\n\n\n\n\n\nTry this\n\n\n\nGreat work following along so far, but now it is time to test your understanding on a new set of variables. Use the variables CalEstimate and Condition_label from lopez_clean. We can ask the question: “What is the difference in estimated calories consumed between the experimental and control groups?”\n\nCreate a violin-boxplot to visualise the difference between CalEstimate and Condition_label from lopez_clean.\n\nApply the Welch t-test to get your inferential statistics and answer the following questions:\n\nHypothesis testing: Assuming \\(\\alpha\\) = .05, the difference between the experimental and control groups on estimated calories consumed was \nstatistically significant\nnot statistically significant.\nEffect size: Rounded to 2 decimals, the raw effect size was an average difference of  estimates calories between the two groups. Expressed as a standardised effect size, this difference equates to Cohen’s d = .\nConfidence interval: Rounded to 2 decimals, the 95% confidence interval for the mean difference is \n-4.08\n-0.03\n39.85\n0.33 to \n-4.08\n-0.03\n39.85\n0.33. The 95% confidence interval for Cohen’s d is \n-4.08\n-0.03\n39.85\n0.33 to \n-4.08\n-0.03\n39.85\n0.33.\n\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe violin-boxplot shows little difference between the two groups on estimated calories consumed.\n\nlopez_clean %&gt;% \n  ggplot(aes(y = CalEstimate, x = Condition_label, fill = Condition_label)) +\n  geom_violin(alpha = 0.5) + \n  geom_boxplot(width = 0.2, \n               fatten = NULL) + \n  stat_summary(fun = \"mean\", \n               geom = \"point\") +\n  stat_summary(fun.data = \"mean_cl_boot\", # confidence interval\n               geom = \"errorbar\", \n               width = 0.1) +\n  scale_fill_viridis_d(option = \"E\") + \n  scale_y_continuous(name = \"Estimated Calories Consumed (kcals)\") +\n  scale_x_discrete(name = \"Study Condition\") + \n  guides(fill = FALSE) + \n  theme_classic()\n\n\n\n\n\n\n\nFor our inferential statistics, a Welch t-test showed the difference is not statistically significant, t (455.06) = 1.60, p = .110.\n\nt.test(formula = CalEstimate ~ Condition_label, \n       data = lopez_clean)\n\n\n    Welch Two Sample t-test\n\ndata:  CalEstimate by Condition_label\nt = 1.6001, df = 455.06, p-value = 0.1103\nalternative hypothesis: true difference in means between group Control and group Experimental is not equal to 0\n95 percent confidence interval:\n -4.080399 39.846433\nsample estimates:\n     mean in group Control mean in group Experimental \n                  133.0328                   115.1498 \n\n\nThe control group estimated they consumed 17.88 (95% CI = [-4.08, 39.85]) more calories than the experimental group, but the difference was not significant. Expressed as a standardised effect size, this equates to Cohen’s d = 0.15 (95% CI = [-0.03, 0.33]).\n\ncohens_d(CalEstimate ~ Condition_label, \n         data = lopez_clean)\n\n\n\n\nCohens_d\nCI\nCI_low\nCI_high\n\n\n0.1490887\n0.95\n-0.0341294\n0.3321449",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#09-categorical",
    "href": "02-lm-categorical.html#09-categorical",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.3 Linear regression with one categorical predictor",
    "text": "2.3 Linear regression with one categorical predictor\nNow you know how to calculate a t-test in R, we can turn to simple linear regression as a more flexible tool for modelling the difference between two groups. As a reminder, there is a chapter in the Handy Workbook (McAleer, 2023) dedicated to manually calculating simple linear regression if you want to work through what the functions are doing behind the scenes.\n\n2.3.1 Activity 6 - Descriptive statistics\nFor all the information we want to include in a report, calculating descriptive statistics is helpful for the reader to show the context behind the results you present. Here, we can report the mean and standard deviation of our outcome per group.\n\nlopez_clean %&gt;% \n  group_by(Condition_label) %&gt;% \n  summarise(mean_cals = round(mean(F_CaloriesConsumed), 2), \n            sd_cals = round(mean(F_CaloriesConsumed), 2))\n\n\n\n\nCondition_label\nmean_cals\nsd_cals\n\n\n\nControl\n196.68\n196.68\n\n\nExperimental\n259.73\n259.73\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want some reinforcement of how these skills apply to published research, look at Table 1 in Lopez et al. (2023). The means and standard deviations here (and Cohen’s d from Activity 5) exactly reproduce the values they report, apart from the SD for the control group (maybe there is a typo in their article).\n\n\n\n2.3.2 Activity 7 - Using the lm() function\nFor our research question of “is there a difference in actual calories consumed between the control and experimental group?”, we can address it with simple linear regression. In this study, we can make causal conclusions as it was an experiment to randomly allocate people into one of two groups, but you can also use regression to compare two self-selecting groups when you cannot make a causal conclusion in isolation. Think carefully about what you can conclude from your design.\nLike Chapter 1, we start by defining our regression model with a formula in the pattern outcome ~ predictor and specify the data frame you want to use. We must then use the summary() function around your model object to get all the statistics you need.\nThere are two ways you can use a categorical predictor. First, we can code groups numerically which people called dummy codingEntering a categorical predictor using two values such as 0 and 1.. You code your first group 0 and you code your second group as 1, which maps on directly to how the regression model works. Let’s look at the output.\n\n# Condition as a factor containing 0 and 1\nlm_cals_numbers &lt;- lm(formula = F_CaloriesConsumed ~ Condition, \n                      data = lopez_clean)\n\nsummary(lm_cals_numbers)\n\n\nCall:\nlm(formula = F_CaloriesConsumed ~ Condition, data = lopez_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-230.90  -99.09  -24.15   62.83  828.04 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  196.682      8.888  22.130  &lt; 2e-16 ***\nCondition1    63.049     12.966   4.863 1.59e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 139.4 on 462 degrees of freedom\nMultiple R-squared:  0.04869,   Adjusted R-squared:  0.04663 \nF-statistic: 23.64 on 1 and 462 DF,  p-value: 1.591e-06\n\n\nCompared to when we had a continuous predictor in Chapter 1, the output is identical. We just need to remember what the key numbers represent. The intercept is the predicted value of your outcome when your predictor is set to 0. When we have two groups coded as 0 and 1, this means the intercept is essentially the mean value of group 0 (here, the control group). We call this the reference groupIn a dummy-coded variable, the first level of your variable, typically 0.. You can confirm this by comparing the intercept estimate 196.68 to the mean value of the control group we calculated in Activity 6.\nThe slope estimate then represents how we predict the outcome to change for every 1-unit increase in the predictor. Since we coded the predictor 0 and 1, this just represents the shift from group 1 to group 2. We call the group we code as 1 the target groupIn a dummy-coded variable, the second level of your variable, typically 1.. You see the target group appended to the variable name, which is Condition1 here. So, for a categorical predictor, the slope represents the mean difference between the reference group (0) and the target group (1): 63.05. In contrast to the t-test, this is our raw/unstandardised effect size for the mean difference we do not need to manually calculate.\n\n\n\n\n\n\nDoes it matter if the slope is positive or negative?\n\n\n\nWhen you have a categorical predictor, the sign is only important for interpreting which group is bigger or smaller. The absolute size is relevant for the effect size where a larger absolute value indicates a larger effect. Whether the slope is positive or negative depends on the order of the groups and which has a larger mean. If the reference is larger than the target, you will get a negative slope. If the target is larger than the reference, you will get a positive slope.\n\n\nLike the continuous predictor, we get values for \\(R^2\\) and adjusted \\(R^2\\), showing we explain .046 (in other words, 4.6%) variance in the outcome through our condition manipulation. We then get the model fit statistics, but with a single predictor, the p-value is identifical to the slope.\nAlternatively, you can use character labels for your categorical predictor and it will still work. This time, we use Condition_label. By default, it will set the order of the reference and target groups alphabetically, but you can manually specify the order by setting factor levels.\n\n# Condition_label as characters\nlm_cals_labels &lt;- lm(formula = F_CaloriesConsumed ~ Condition_label, \n                     data = lopez_clean)\n\nsummary(lm_cals_labels)\n\n\nCall:\nlm(formula = F_CaloriesConsumed ~ Condition_label, data = lopez_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-230.90  -99.09  -24.15   62.83  828.04 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  196.682      8.888  22.130  &lt; 2e-16 ***\nCondition_labelExperimental   63.049     12.966   4.863 1.59e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 139.4 on 462 degrees of freedom\nMultiple R-squared:  0.04869,   Adjusted R-squared:  0.04663 \nF-statistic: 23.64 on 1 and 462 DF,  p-value: 1.591e-06\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to test specifying the factor order to see how it affects the output, try running this code prior to the regression model:\n\n# Specify group order of Experimental then Control\nlopez_clean &lt;- lopez_clean %&gt;% \n  mutate(Condition_label = factor(Condition_label, \n                                  levels = c(\"Experimental\", \"Control\")))\n\n\n\n\n\n\n\n\n\nHow are t-tests and regression the same?\n\n\n\n\n\nIf you are interested in the relationship between the two concepts, we said a t-test was a specific application of the general linear model. In the t-test calculations, it expresses the mean difference between groups by the standard error of the difference. In essence, it describes the difference in standard errors, which we can describe with a t-distribution to calculate p-values.\nIn regression, we frame the model as how much variance you explain compared to the total amount of variance. The more variance your predictor explains, the less unexplained variance there is left over. For the slope estimate though, this is identical to the t-test as we estimate the mean difference between groups plus the standard error around the mean difference. We calculate a p-value for the slope from a t-distribution, so you get a t-value in the output.\nYou can see the process is identical by comparing the key values from the regression output to the Student t-test. We can recreate the mean difference to compare to the slope, the t-value is the same, the p-value is the same, the degrees of freedom are the same, and the 95% confidence intervals below are the same.\nSo, when you have a single categorical predictor, it is the exact same process as the Student t-test, just expressed slightly different. The only downside to this procedure is it is much more difficult to recreate the Welch t-test.\n\n\n\n\n2.3.3 Activity 8 - Calculating confidence intervals\nThe only thing we are missing is our confidence intervals around the estimates which we can calculate through the confint() function.\n\nconfint(lm_cals_numbers)\n\n                2.5 %    97.5 %\n(Intercept) 179.21657 214.14704\nCondition1   37.56915  88.52983\n\n\nNow, we can summarise the three key concepts of inferential statistics as:\n\nHypothesis testing: p &lt; .001, suggesting we can reject the null hypothesis assuming \\(\\alpha\\) = .05. The experimental group ate significantly more calories of soup than the control group.\nEffect size: \\(b_1\\) = 63.05, suggesting the experimental group ate on average 63 more calories than the control group.\nConfidence interval: [37.57, 88.53], showing the precision around the slope estimate.\n\n\n\n\n\n\n\nTry this\n\n\n\nNow it is time to test your understanding on a new set of variables. This time, use CalEstimate as your outcome, Condition_label as your predictor, and use lopez_clean as your data. We can ask the same question as Activity 5: “What is the difference in estimated calories consumed between the experimental and control groups?”.\nApply simple linear regression to get your inferential statistics and answer the following questions:\n\nHypothesis testing: Assuming \\(\\alpha\\) = .05, Condition is a \nstatistically significant\nnon-significant predictor of estimates calories consumed.\nEffect size: Rounded to 2 decimals, the Condition slope coefficient means there was a mean difference of \n133.03\n7.68\n-17.88\n11.19.\nConfidence interval: Rounded to 2 decimals, the lower bound of the slope is \n117.94\n-39.88\n148.12\n4.11 and the upper bound is \n117.94\n-39.88\n148.12\n4.11.\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe conclusions are the same as when we calculated the t-test, where condition is not a statistically significant predictor of estimated calories consumed. As a regression model, we get the same conclusions expressed in a slightly different way. Condition is a negative but non-significant predictor (p = .111). The control group ate 17.88 (\\(b_1\\) = -17.88, 95% CI = [-39.88, 4.11]) more calories than the experimental group. We explain very little variance in estimated calories consumed (adjusted \\(R^2\\) = .003), so the condition manipulation had little effect.\n\n# Create lm object for condiiton label as a predictor\nlm_cal_est &lt;- lm(CalEstimate ~ Condition_label, \n                 data = lopez_clean)\n\n# summary of the model object\nsummary(lm_cal_est)\n\n\nCall:\nlm(formula = CalEstimate ~ Condition_label, data = lopez_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-130.03  -83.03  -33.03   44.85  666.97 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  133.033      7.679  17.324   &lt;2e-16 ***\nCondition_labelExperimental  -17.883     11.192  -1.598    0.111    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 119.9 on 459 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.005531,  Adjusted R-squared:  0.003365 \nF-statistic: 2.553 on 1 and 459 DF,  p-value: 0.1108\n\n# confidence intervals around estimates\nconfint(lm_cal_est)\n\n                                2.5 %     97.5 %\n(Intercept)                 117.94256 148.123015\nCondition_labelExperimental -39.87763   4.111599\n\n\n\n\n\n\n2.3.4 Activity 9 - Standardising predictors\nFor simple linear regression with two levels of a categorical predictor, centering the variable does not help, but we can standardise our outcome to express the estimate in standard deviations rather than the raw units. This is analogous to calculating Cohen’s d as we express the standardised mean difference. In contrast to continuous predictors, we only need to standardise the outcome, rather than both the outcome and predictor(s). We then use the standardised variable as our outcome.\n\n# Be careful with the bracket placement to subtract the mean first\nlopez_clean &lt;-lopez_clean %&gt;% \n  mutate(actual_calories_std = (F_CaloriesConsumed - mean(F_CaloriesConsumed)) / sd(F_CaloriesConsumed))\n\n# Condition as a factor containing 0 and 1\nlm_cals_std &lt;- lm(formula = actual_calories_std ~ Condition, \n                      data = lopez_clean)\n\nsummary(lm_cals_std)\n\n\nCall:\nlm(formula = actual_calories_std ~ Condition, data = lopez_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6173 -0.6941 -0.1692  0.4401  5.8000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.20749    0.06225  -3.333 0.000928 ***\nCondition1   0.44163    0.09082   4.863 1.59e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9764 on 462 degrees of freedom\nMultiple R-squared:  0.04869,   Adjusted R-squared:  0.04663 \nF-statistic: 23.64 on 1 and 462 DF,  p-value: 1.591e-06\n\n\nNote, the estimate may be slightly different to directly calculating Cohen’s d as there are a few formulas. If you compare to Activity 5, we got d = 0.45 there and 0.44 here. Between the estimate and 95% confidence intervals, they are off by .02, so it does not have a material impact on the results.\n\n\n\n\n\n\nTip\n\n\n\nAs before, once you know how it works conceptually, there is a shortcut where you do not need to standardise all your variables first. The effectsize package has a handy function called standardize_parameters() which you can apply to your initial lm() object.\n\nstandardize_parameters(lm_cals_numbers)\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n(Intercept)\n-0.2074898\n0.95\n-0.3298249\n-0.0851547\n\n\nCondition1\n0.4416297\n0.95\n0.2631529\n0.6201065",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#checking-assumptions",
    "href": "02-lm-categorical.html#checking-assumptions",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.4 Checking assumptions",
    "text": "2.4 Checking assumptions\nFor the inferential statistics to work as intended, the model makes certain assumptions about the data you are putting into it and the accuracy of the inferences depends on how sensible these assumption are.\n\n2.4.1 Activity 10 - Diagnostic plots for linear regression\nWe have the same assumptions for simple linear regression now we have a categorical predictor:\n\nThe outcome is interval/ratio level data.\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\nThe predictors have non-zero variance.\nThe relationship between the outcome and predictor is linear.\nThe residuals should be normally distributed.\nThere should be homoscedasticity.\n\nAssumptions 1-4 are pretty straight forward as they relate to your understanding of the design or a simple check on the data for non-zero variance (the responses are not all the exact same value).\nAssumptions 5-7 require diagnostic checks on the residuals from the model. In contrast to continuous predictors, they are a little harder to identify patterns in. As we only have two values on the x-axis (0 and 1), all the residuals will be organised into vertical lines and the trend lines to help spot patterns do not look quite right.\n\n2.4.2 Checking linearity\nWhen you have a categorical predictor with two levels, you meet linearity by default, so you do not need to check this assumption directly. When you have two levels, you can only fit a straight line between the values.\n\nplot(lm_cals_numbers, \n     which = 1)\n\n\n\n\n\n\n\n\n2.4.3 Checking normality\nThe qq-plot is still the same to interpret. Now, this example presents a useful case of decision making in data analysis we explore more in Chapter 4. The plot here is approaching signs of violating normality as there is a clear curve to the data points with 3 and 118 the largest deviations (you can see these on the violin-boxplot as the two highest values in the control group). For this chapter, we are sticking with it and it would be consistent with how the original authors analysed the data, but note this would be a key decision to make and justify when reporting the analysis.\n\nplot(lm_cals_numbers, \n     which = 2)\n\n\n\n\n\n\n\n\n2.4.4 Checking homoscedasticity\nThe scale-location plot is harder to interpret when you have a categorical predictor. Like linearity, the points are all arranged in two vertical lines as we only have two levels. You are looking out for the spread of the two lines to be roughly similar. They look fine here, just points 118 and 3 separated a little from the other points.\n\nplot(lm_cals_numbers, \n     which = 3)\n\n\n\n\n\n\n\n\n2.4.5 Checking influential cases\nFinally, we have our plots for identifying influential cases. First, we get Cook’s distance for all the observations in your data. We see points 3 and 118 come up yet again, but although they are the largest deviations, they do not necessarily have worrying Cook’s distance values. There are different thresholds in the literature, but estimates range from 1, 0.5, to 4/n. It would only be under this final most conservative estimate (0.009) we would highlight several observations for further inspection.\n\nplot(lm_cals_numbers, \n     which = 4)\n\n\n\n\n\n\n\nFinally, the fifth plot shows residual values against leverage. Like Chapter 1, we cannot see the Cook’s distance threshold it uses in the plot as none of the points are a large enough deviation, despite 3 and 188 sticking out again.\n\nplot(lm_cals_numbers, \n     which = 5)\n\n\n\n\n\n\n\n\n2.4.6 Checking all the assumptions\nNow we have covered the individual diagnostic plots, there is a handy function called check_model() from the performance package. Like the plot() function, the output for linearity, homoscedasticity, and influential observations does not plot right as we only have two values for the predictor and the plot lines do not really work. Do not worry, you have not done anything wrong.\n\ncheck_model(lm_cals_numbers)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat might explain the funky normality?\n\n\n\nThe posterior predictive check here provides an insight into why we get potential problems with normality. The outcome is ratio but cannot be smaller than 0 as we cannot have negative calories. So, in the green line for the observed data, the data are a little skewed as it drops off prior to 0. However, the model does not know that and it happily predicts normally distributed values which go below 0, creating a mismatch between what the model predicts and the values we enter into it.\nRemember the models will work regardless of the data you put into them, it is important to keep your role in mind to recognise when you need to be cautious about what you can learn from the model. This is what we will explore in Chapters 4 and 5 as we consider data screening and alternative model distributions.\n\n\n\n\n\n\n\n\nTry this\n\n\n\nIn activity 8, you should have calculated the effect of condition (Condition_label) on estimated calories consumed (CalEstimate) for your independent task. Using the model object lm_cal_est, work through assumptions for simple linear regression and make a note of whether you think it meets the assumptions, or there might be any problems. Some of the assumptions you consider what you know about the design, while others you need the diagnostic plots.\n\nThe outcome is interval/ratio level data.\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\nThe predictors have non-zero variance.\nThe relationship between the outcome and predictor is linear.\nThe residuals should be normally distributed.\nThere should be homoscedasticity.\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nThe outcome is interval/ratio level data.\n\nThe outcome is nicely ratio as estimated calories have a logical 0 point (no calories) and the units are in equal measurements..\n\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\n\nOur predictor is categorical with two levels.\n\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\n\nThe short answer is this appears to be the case in this study. The longer answer is there might have been an issue with the participants apparently completing the study in groups of 3. It is not entirely clear in the data how they recorded this, but grouped data collection does present a potential problem with independence which you will not learn about until the final lecture on mixed effects models and the original authors did not seem to address it.\n\nThe predictors have non-zero variance.\n\nWe have observations from both levels of the predictor.\n\nThe relationship between the outcome and predictor is linear.\n\nWe meet linearity by default with two levels, so we do not need the plot here.\n\nThe residuals should be normally distributed.\n\nLike the actual calories consumed, this is firmly a clear deviation from what we expect and provides a good example of when it does not look right. If we were to analyse the data fully, we would explore the impact of this and alternative models, but for this chapter, we are going to note our concern and remember the authors analysed the data like this.\n\nplot(lm_cal_est, \n     which = 2)\n\n\n\n\n\n\n\n\nThere should be homoscedasticity.\n\nLooking at the spread of each group, it looks fine with a similar range until both groups are more sparsely distributed above 1.\n\nplot(lm_cal_est, \n     which = 3)\n\n\n\n\n\n\n\nIn summary, normality is a clear concern and something we will return to for your options in Chapter 4 and the course materials. For now, we will stick with the model but recognise we should be cautious.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#reporting-your-results",
    "href": "02-lm-categorical.html#reporting-your-results",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.5 Reporting your results",
    "text": "2.5 Reporting your results\nNow we have some results to go with, there are a few recommendations on how to communicate that information. If you need a reminder of APA style for formatting numbers, you can see this PDF online for a little cheat sheet for numbers and statistics.\n\nExplain to the reader what your linear regression model was. For example, what was your outcome and predictor variable?\nReport descriptive statistics to help contextualise your findings. For example, the mean/standard deviation for your outcome per group.\nProvide an appropriate data visualisation to help communciate key patterns to the reader. For example, a violin-boxplot for how each group responded on your outcome.\nReport all three key inferential statistics concepts for the coefficient: the slope, the confidence interval around your slope, and the p-value for hypothesis testing. When you have one predictor in simple linear regression, you typically focus on the slope as your key effect size that helps address your research question and hypothesis. APA style rounds numbers to 2 decimal places when numbers can be bigger than 1, and 3 decimals with no leading zero when it cannot be bigger than 1. When you report the unstandardised slope, you use the symbol \\(b_1\\) but for the standardised slope, you use Beta instead \\(\\beta_1\\).\nProvide an overview of the model fit statistics for whether your model explained a significant amount of variance in your outcome. Remember: the p-value for your model will be the same as for the slope in simple linear regression.\n\nFor our main example, we could summarise the findings as:\n“Our research question was: is there a difference in actual calories consumed between the control and experimental group? To test this, we applied simple linear regression using condition as a predictor with two levels (control and experimental) and actual calories consumed as our outcome. Figure 1 shows a violin-boxplot of the difference between the control and experimental groups.\n\n\n\n\n\n\n\n\nOur model explained a statistically significant amount of variance in our outcome (adjusted \\(R^2\\) = .047, F(1, 462) = 23.64, p &lt; .001). Condition was a positive predictor, where the experimental group consumed on average 63 more calories than the control group (\\(b_1\\) = 63.05, 95% CI = [37.57, 88.53], p &lt; .001). Expressed as a standardised effect size, the experimental group consumed 0.44 (95% CI = [0.26, 0.62]) more standard deviations than the control group.”\nNote: we have not included an APA formatted Figure title here as it is not easy to format in our book, so refer to the course materials for guidance.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#09-bonus",
    "href": "02-lm-categorical.html#09-bonus",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.6 One- and paired-samples tests",
    "text": "2.6 One- and paired-samples tests\nYou might find yourself in a situation where you want to test a continuous variable against a fixed value or compare conditions in the same participants. In experimental designs, you might have multiple conditions from the same participants which causes problems if you analyse the data as if they were groups as above. You will learn a more flexible approach in the final lecture on mixed effects models but for simple cases when you have two conditions, the following approach will work.\nFor this demonstration, we will use data from experiment 1 of Bem (2011), an (in)famous study that almost single-handedly started the replication crisis in psychology. Briefly, participants completed a computer task adapted from priming experiments where they could select one of two windows. They had to guess which window had an image hidden behind it and the images contained different stimuli like erotic or neutral/control images. Across many trials of the participants guessing the location, Bem calculated the proportion of successful trials which could range between 0 (never correct), 50 (50%, chance), and 100 (always correct). The headline finding was participants demonstrated precognition - or the ability to see into the future - to guess above chance levels, but what does the data look like?\nWe are working with a new data set, so please save the following data file: Bem_2011.csv. Right click the link and select “save link as”, or clicking the link will save the files to your Downloads. Make sure that you save the file as “.csv”. Save or copy the file to your data/ folder within 02_regression_categorical. Read in the data file to the object bem_data to be consistent with the tasks below.\n\n2.6.1 Activity 11 - One-sample comparing against a fixed value\nThere are scenarios where you want to compare a single continuous variable against a fixed value. For example, do your participants respond significantly above or below chance?\n\n2.6.1.1 Expressed as a t-test\nAs a t-test, we need to specify two arguments:\n\nx - This is the continuous variable you want to analyse and compare the mean value of. We must use the base R operator $ to specify the column from your data.\nmu - This is the fixed value you want to test your variable against.\n\nIn this scenario, we want to compare the hit rate to erotic images against a value of 50. This will tell us if the hit rate is significantly above or below chance.\n\nt.test(x = bem_data$Erotic.Hits.PC,\n       mu = 50)\n\n\n    One Sample t-test\n\ndata:  bem_data$Erotic.Hits.PC\nt = 2.5133, df = 99, p-value = 0.01358\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 50.66075 55.61703\nsample estimates:\nmean of x \n 53.13889 \n\n\nThe output is similar to the independent samples t-test. We get the p-value for hypothesis testing, the mean estimate of the variable, and it’s 95% confidence interval. To express it as an effect size, you can subtract 50 from each value. So, participants responded 3.14% above chance - statistically significant, but hardly convincing evidence for precognition.\n\n2.6.1.2 Expressed as a linear model\nWe can also express this as a linear model, but we must first add a small wrangling step. In the one-sample t-test, we can manually enter a fixed value to compare the mean against. In a linear model, we must compare against 0 by subtracting the fixed value from your variable. So, we subtract 50 from all the observations, so they become a kind of deviation from 50.\n\nbem_data &lt;- bem_data %&gt;% \n  mutate(erotic_deviation = Erotic.Hits.PC - 50)\n\nIn contrast to previous linear models, we only add a fixed intercept and do not add a predictor. This recreates the one-sample t-test by estimating the mean value of your outcome.\n\nlm_erotic &lt;- lm(erotic_deviation ~ 1, \n                data = bem_data)\n\nsummary(lm_erotic)\n\n\nCall:\nlm(formula = erotic_deviation ~ 1, data = bem_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.139  -8.694   2.417   7.972  30.194 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    3.139      1.249   2.513   0.0136 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.49 on 99 degrees of freedom\n\n\nThis process has the benefit of directly producing your effect size, as the intercept estimate is the deviation from your fixed value (here, 50). As we calculated manually before, the erotic hit rate is 3.14% above your fixed value. If you remember back to the linear model explanations, this is where the p-value for the intercept is finally useful as it tests against 0.\nIf you compare to the one-sample t-test, all of these values are identical. You also have the option of calculating confidence intervals around the estimate, calculating a standardised effect size, and checking assumptions by applying the previous linear regression activities.\n\n2.6.2 Activity 12 - Paired-samples comparing conditions\nAlternatively, you might want to compare two conditions from the same participants in paired samples/within-subjects design. For example, is the hit-rate significantly higher for erotic images compared to control images?\n\n2.6.2.1 Expressed as a t-test\nTo conduct a paired-samples t-test, we must specify three arguments:\n\nx - This is the first level of your condition as a column. You need your data in wide format, so the condition levels are spread across two columns per participant. We must use the base R operator $ to specify the column from your data.\ny - This is the second level of your condition as a column.\npaired - This instructs R you want a paired-samples t-test to compare conditions within participants.\n\nIn this scenario, we want to compare the hit rate for erotic images to control images.\n\nt.test(x = bem_data$Erotic.Hits.PC,\n       y = bem_data$Control.Hits.PC, \n       paired = TRUE)\n\n\n    Paired t-test\n\ndata:  bem_data$Erotic.Hits.PC and bem_data$Control.Hits.PC\nt = 1.8563, df = 99, p-value = 0.06638\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.2277431  6.8388542\nsample estimates:\nmean difference \n       3.305556 \n\n\nThe output is almost identical to the one-sample t-test, but this time the effect size is the mean difference between conditions, not just the mean per condition. Behind the scenes, a paired-samples t-test is actually a one-sample t-test in disguise as it uses the difference between conditions as the outcome.\nAs an aside, Bem (2011) reported a significant difference here, but only because he reported a one-tailed test (alternative = \"greater\"). This is an example where you ideally need a strong (ideally pre-registered) prediction as it makes a material impact on the inferences you would make.\n\n2.6.2.2 Expressed as a linear model\nFinally, we can express a paired-samples t-test as a linear model. We must apply a small data wrangling step to calculate a difference score between conditions. This is so the linear model compares the estimate against 0 of no difference. So, for this example, we create a variable for the difference between erotic and control images.\n\nbem_data &lt;- bem_data %&gt;% \n  mutate(erotic_control = Erotic.Hits.PC - Control.Hits.PC)\n\nLike the one-sample scenario, we only add a fixed intercept for the new difference variance and do not add a predictor. This recreates the paired-samples t-test by estimating the mean value of your difference score.\n\nlm_paired &lt;- lm(erotic_control ~ 1, \n                data = bem_data)\n\nsummary(lm_paired)\n\n\nCall:\nlm(formula = erotic_control ~ 1, data = bem_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.861  -9.556   2.250   9.194  35.583 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    3.306      1.781   1.856   0.0664 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.81 on 99 degrees of freedom\n\n\nThis process directly produces your effect size again, as the intercept estimate is the deviation from 0 for your difference score. As we saw in the paired-samples t-test output, there was a 3.31% higher hit rate for erotic images compared to control (as we calculated erotic - control).\nIf you compare to the paired-samples t-test, all of these values are identical. You also have the option of calculating confidence intervals around the estimate, calculating a standardised effect size, and checking assumptions by applying the previous linear regression activities.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#09-test",
    "href": "02-lm-categorical.html#09-test",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.7 Test Yourself",
    "text": "2.7 Test Yourself\nFor this chapter’s knowledge check section, we have something a little different. Instead of purely conceptual questions about functions, we have another example of linear regression from Lopez et al. (2023). Feel free to create this model yourself, but we will show you some output and ask you questions based on it.\nFor this model, we focus on ounces of soup consumed (M_postsoup) rather than calories by each Condition group (Condition_label). You might have a good idea about the results based on the chapter, but you will still need to interpret the output accurately.\nQuestion 1. In the violin-boxplot below, the experimental group consumed more soup in ounces than the control group: \nTRUE\nFALSE.\n\n\n\n\n\n\n\n\nQuestion 2 For the next few questions, we have the output from a linear regression model and we would like you to interpret it.\n\n\n\nCall:\nlm(formula = M_postsoup ~ Condition_label, data = lopez_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.410  -4.467  -1.089   2.833  37.333 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   8.8675     0.4007  22.130  &lt; 2e-16 ***\nCondition_labelExperimental   2.8426     0.5846   4.863 1.59e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.285 on 462 degrees of freedom\nMultiple R-squared:  0.04869,   Adjusted R-squared:  0.04663 \nF-statistic: 23.64 on 1 and 462 DF,  p-value: 1.591e-06\n\n\nThe outcome variable in this model is \nActual ounces of soup consumed\nExperimental condition and the predictor variable is \nActual ounces of soup consumed\nExperimental condition.\nQuestion 3 In this model, the reference group is \nControl\nExperimental and the target group is \nControl\nExperimental\nQuestion 4 Rounded to 2 decimals, we predict a value of  for our reference group.\nQuestion 5 The predictor is \nstatistically significant\nnon-significant and \npositive\nnegative.\nQuestion 6 The target group consumed on average  ounces \nless\nmore soup than the reference group.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#words-from-this-chapter",
    "href": "02-lm-categorical.html#words-from-this-chapter",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.8 Words from this Chapter",
    "text": "2.8 Words from this Chapter\nBelow you will find a list of words that were used in this chapter that might be new to you in case it helps to have somewhere to refer back to what they mean. The links in this table take you to the entry for the words in the PsyTeachR Glossary. Note that the Glossary is written by numerous members of the team and as such may use slightly different terminology from that shown in the chapter.\n\n\nterm\ndefinition\n\n\n\ndummy coding\nEntering a categorical predictor using two values such as 0 and 1.\n\n\nreference group\nIn a dummy-coded variable, the first level of your variable, typically 0.\n\n\nStudent t-test\nCalculating a t-value based on the mean difference divided by the pooled standard deviation. In the Student t-test, we times the pooled standard deviation by a term containing the sample sizes of each group.\n\n\ntarget group\nIn a dummy-coded variable, the second level of your variable, typically 1.\n\n\nWelch t-test\nCalculating a t-value based on the mean difference divided by a term containing the variance of each group. We also correct the degrees of freedom for the difference in variances.\n\n\n\n\n\n\n\nBem, D. J. (2011). Feeling the future: Experimental evidence for anomalous retroactive influences on cognition and affect. Journal of Personality and Social Psychology, 100(3), 407–425. https://doi.org/10.1037/a0021524\n\n\nLopez, A., Choi, A. K., Dellawar, N. C., Cullen, B. C., Avila Contreras, S., Rosenfeld, D. L., & Tomiyama, A. J. (2023). Visual cues and food intake: A preregistered replication of Wansink et al (2005). Journal of Experimental Psychology: General. https://doi.org/10.1037/xge0001503.supp",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#end-of-chapter",
    "href": "02-lm-categorical.html#end-of-chapter",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.9 End of chapter",
    "text": "2.9 End of chapter\nWell done, you have now completed the first core set of chapters for inferential statistics!\nAt this point, you can now address a range of research questions by applying variations of the general linear model. As a researcher, the most important thing is starting with your research question (and where applicable, your hypothesis), designing a study to address that research question, and using an appropriate statistical model for your design and research question. But, before you can identify an appropriate statistical model, you need to know what they look like! This is everything we cover in Research Methods 1 to focus on a select range of foundational skills. You will then build on these modelling techniques in Chapters 12-14 for Research Methods 2.\nYou are now ready to complete the second data analysis journey chapter: Simple Linear Regression. This is where you can test your new skills in a slightly less structured way, from wrangling data, to answering a research question.\nIn the next core chapter, we turn to statistical power and work on how you can conduct a power analysis in R/RStudio.\n\n\n\n\nBem, D. J. (2011). Feeling the future: Experimental evidence for anomalous retroactive influences on cognition and affect. Journal of Personality and Social Psychology, 100(3), 407–425. https://doi.org/10.1037/a0021524\n\n\nLopez, A., Choi, A. K., Dellawar, N. C., Cullen, B. C., Avila Contreras, S., Rosenfeld, D. L., & Tomiyama, A. J. (2023). Visual cues and food intake: A preregistered replication of Wansink et al (2005). Journal of Experimental Psychology: General. https://doi.org/10.1037/xge0001503.supp",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "04-screening-data.html",
    "href": "04-screening-data.html",
    "title": "\n4  Missing data, outliers, and checking assumptions\n",
    "section": "",
    "text": "4.1 Chapter preparation",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data, outliers, and checking assumptions</span>"
    ]
  },
  {
    "objectID": "04-screening-data.html#chapter-preparation",
    "href": "04-screening-data.html#chapter-preparation",
    "title": "\n4  Missing data, outliers, and checking assumptions\n",
    "section": "",
    "text": "4.1.1 Organising your files and project for the chapter\nFor this chapter, we are going to revisit the data sets you worked with in Chapters 1 (Dawtry et al., 2015) and 2 (Lopez et al., 2023). They each presented some useful examples for checking modelling assumptions and the decisions that go into data analysis. We might not use both data sets for each topic we cover, but they will be useful to demonstrate some of the problems and decisions we highlighted in previous chapters, but did not explore solutions.\nBefore we can get started, you need to organise your files and project for the chapter, so your working directory is in order.\n\nIn your folder for research methods and the book Stats_Research_Design, create a new folder called 04_screening_data. Within Stats_Research_Design, create two new folders called data and figures.\nCreate an R Project for 04_screening_data as an existing directory for your chapter folder. This should now be your working directory.\nCreate a new Quarto document and give it a sensible title describing the chapter, such as 04 Missing Data, Outliers, and Assumptions. Save the file in your Stats_Research_Design folder.\nThe Dawtry et al. (2015) data wrangling steps were quite long, so please save this clean version of the data to focus on screening data in this chapter: Dawtry_2015_clean.csv. You will also need to save the data from Lopez et al. (2023) if you have not downloaded it yet: Lopez_2023.csv. Right click the link and select “save link as”, or clicking the link will save the files to your Downloads. Make sure that you save the files as “.csv”. Save or copy the files to your data/ folder within Stats_Research_Design.\n\nYou are now ready to start working on the chapter!\n\n4.1.2 Activity 1 - Read and wrangle the data\nAs the first activity, try and test yourself by completing the following task list to read and wrangle the two data files. There is nothing extra to do with this version of the Dawtry data and one small step for the Lopez data.\n\n\n\n\n\n\nTry this\n\n\n\nTo read and wrangle the data, complete the following tasks:\n\n\nLoad the following packages:\n\nperformance\ntidyverse\n\n\nRead the data file data/Dawtry_2015_clean.csv to the object name dawtry_clean.\nRead the data file data/Lopez_2023.csv to the object name lopez_data.\n\nCreate a new object called lopez_clean based on lopez_data:\n\nCreate a new variable called Condition_label by recoding Condition. “0” is the “Control” group and “1” is the “Experimental” group.\n\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nYou should have the following in a code chunk:\n\n# load the relevant packages\nlibrary(performance)\nlibrary(tidyverse)\n\n# Read the Dawtry_2015_clean.csv file \ndawtry_clean &lt;- read_csv(\"data/Dawtry_2015_clean.csv\")\n\n# Read the Lopez_2023.csv file \nlopez_data &lt;- read_csv(\"data/Lopez_2023.csv\")\n\n# recode condition\nlopez_clean &lt;- lopez_data %&gt;% \n  mutate(Condition_label = case_match(Condition,\n                                      0 ~ \"Control\",\n                                      1 ~ \"Experimental\"))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data, outliers, and checking assumptions</span>"
    ]
  },
  {
    "objectID": "04-screening-data.html#missing-data",
    "href": "04-screening-data.html#missing-data",
    "title": "\n4  Missing data, outliers, and checking assumptions\n",
    "section": "\n4.2 Missing data",
    "text": "4.2 Missing data\nChecking whether data are missing are relatively straight forward. Missing valuesIf a participant or observation contains no data for one or more cells, they have missing data. in a spreadsheet will be recorded as NA and there are a few ways of identifying them. The much more difficult part of missing data is considering why they are missing in the first place. For example, it might be because:\n\nYour participants accidentally missed a question.\nYou made a mistake while setting up your questionnaire/experiment and some responses did not save.\nYour participants intentionally did not want to answer a question.\nYour participants did not turn up to a final testing session.\n\nFor the first two reasons, it is not ideal as we are losing data but there is no systematic pattern to why the data is missing. For the latter two reasons, there might be a relationship between a key variable and whether the data are missing. This is where it is particularly important to consider the role of missing data. We are focusing on data skills here rather than the conceptual understanding, but missing data are commonly categorised as:\n\nMissing completely at randomWhether the data are missing or not is completely unrelated to other variables in the data..\nMissing at randomWe can predict the missing value using other variables in the data..\nMissing not at randomWhether the data are missing or not is causally related to one or more other variables in the data..\n\nFor this course, we do not have time to investigate strategies to address missing data apart from focusing on complete cases and ignoring missing data, but you might find Jakobsen et al. (2017) useful if you want to explore options like data imputation.\n\n4.2.1 Activity 2 - Identifying missing data\nReturning to data skills, the simplest way of getting an overview of whether any data are missing is using the summary() function. For this part, we will focus on dawtry_clean from Dawtry et al. (2015).\n\nsummary(dawtry_clean)\n\n       PS      Household_Income Political_Preference      age      \n Min.   :  1   Min.   :    20   Min.   :1.000        Min.   :19.0  \n 1st Qu.: 77   1st Qu.: 25000   1st Qu.:3.000        1st Qu.:28.0  \n Median :153   Median : 42000   Median :4.000        Median :33.5  \n Mean   :153   Mean   : 54732   Mean   :4.465        Mean   :37.4  \n 3rd Qu.:229   3rd Qu.: 75000   3rd Qu.:6.000        3rd Qu.:46.0  \n Max.   :305   Max.   :350000   Max.   :9.000        Max.   :69.0  \n               NA's   :4        NA's   :4            NA's   :1     \n     gender     Population_Inequality_Gini_Index Population_Mean_Income\n Min.   :1.00   Min.   :14.26                    Min.   : 14205        \n 1st Qu.:1.00   1st Qu.:31.10                    1st Qu.: 47250        \n Median :1.00   Median :35.66                    Median : 58650        \n Mean   :1.48   Mean   :35.51                    Mean   : 58605        \n 3rd Qu.:2.00   3rd Qu.:40.73                    3rd Qu.: 67875        \n Max.   :2.00   Max.   :57.45                    Max.   :138645        \n NA's   :3                                                             \n Social_Circle_Inequality_Gini_Index Social_Circle_Mean_Income\n Min.   : 2.00                       Min.   : 12000           \n 1st Qu.:19.79                       1st Qu.: 36000           \n Median :25.59                       Median : 51060           \n Mean   :26.35                       Mean   : 54294           \n 3rd Qu.:33.27                       3rd Qu.: 66375           \n Max.   :61.36                       Max.   :148500           \n                                                              \n fairness_satisfaction redistribution\n Min.   :1.000         Min.   :1.00  \n 1st Qu.:2.000         1st Qu.:3.25  \n Median :3.000         Median :4.00  \n Mean   :3.539         Mean   :3.91  \n 3rd Qu.:5.000         3rd Qu.:4.75  \n Max.   :9.000         Max.   :6.00  \n                                     \n\n\nWe get a range of summary statistics for each variable but importantly for our purposes here, the final entry is NA's, where relevant. We can see there are 4 missing values for household income, 4 for political preference, 1 for age, and 3 for gender.\n\n\n\n\n\n\nTry this\n\n\n\nIf you explore lopez_clean from Lopez et al. (2023), do we have any missing data to worry about? \nYes\nNo\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes, it looks like there is also a small amount of missing data here. There is 1 for sex, 2 for estimated ounces, and 3 for estimates calories.\n\nsummary(lopez_clean)\n\n ParticipantID       Sex              Age          Ethnicity    \n Min.   :1001   Min.   :0.0000   Min.   :18.00   Min.   :1.000  \n 1st Qu.:1202   1st Qu.:1.0000   1st Qu.:19.00   1st Qu.:3.000  \n Median :1462   Median :1.0000   Median :20.00   Median :3.000  \n Mean   :1456   Mean   :0.8099   Mean   :20.47   Mean   :3.261  \n 3rd Qu.:1704   3rd Qu.:1.0000   3rd Qu.:21.00   3rd Qu.:4.000  \n Max.   :1928   Max.   :3.0000   Max.   :54.00   Max.   :8.000  \n                NA's   :1                                       \n   OzEstimate       CalEstimate      M_postsoup     F_CaloriesConsumed\n Min.   :  0.010   Min.   :  1.0   Min.   : 0.600   Min.   :  13.31   \n 1st Qu.:  2.000   1st Qu.: 50.0   1st Qu.: 5.575   1st Qu.: 123.65   \n Median :  4.000   Median : 90.0   Median : 8.700   Median : 192.97   \n Mean   :  6.252   Mean   :124.6   Mean   :10.203   Mean   : 226.30   \n 3rd Qu.:  8.000   3rd Qu.:160.0   3rd Qu.:13.125   3rd Qu.: 291.11   \n Max.   :100.000   Max.   :800.0   Max.   :46.200   Max.   :1024.72   \n NA's   :2         NA's   :3                                          \n   Condition      Condition_label   \n Min.   :0.0000   Length:464        \n 1st Qu.:0.0000   Class :character  \n Median :0.0000   Mode  :character  \n Mean   :0.4698                     \n 3rd Qu.:1.0000                     \n Max.   :1.0000                     \n                                    \n\n\n\n\n\n\n4.2.2 Activity 3 - Removing missing data\nOnce we know whether missing data are present, we must consider what to do with them. For this chapter, we are only going to control removing participants, but you could apply a data imputation technique at this point if appropriate.\nFor all the modelling techniques we apply in this book, the functions will remove participants who have one or more missing values from any variable involved in the analysis. The functions will give you a warning to highlight when this happens, but it is normally a good idea to remove participants with missing data yourself so you have a note of how many participants you remove.\nFor dawtry_clean, the tidyverse function drop_na() is the easiest way of removing missing data, either participants with any missing data or by specifying individual variables.\n\ndawtry_all_missing &lt;- dawtry_clean %&gt;% \n  drop_na()\n\ndawtry_income_missing &lt;- dawtry_clean %&gt;% \n  drop_na(Household_Income)\n\nWe can compare the number of participants by using the nrow() function to count how many rows are in each object.\n\n# How many rows in the full data? \nnrow(dawtry_clean)\n\n[1] 305\n\n# How many rows when we remove missing data in one variable? \nnrow(dawtry_income_missing)\n\n[1] 301\n\n# How many rows when we remove any missing value?\nnrow(dawtry_all_missing)\n\n[1] 294\n\n\nLike most data skills and statistics concepts, the key skill here comes in decision making; documenting and justifying the approach that you take.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data, outliers, and checking assumptions</span>"
    ]
  },
  {
    "objectID": "04-screening-data.html#outliers",
    "href": "04-screening-data.html#outliers",
    "title": "\n4  Missing data, outliers, and checking assumptions\n",
    "section": "\n4.3 Outliers",
    "text": "4.3 Outliers\nThe next data screening concept revolves around identifying potential outliers. Like missing data, the difficulty here comes in first deciding what an outlier is and then deciding on what to do with it. Leys et al. (2019) mention one study found 14 definitions and 39 unique ways of identifying outliers, so this is our second key area of decision making. Leys et al. categorise outliers into three types:\n\nError outliersA mistake or impossible value in your data..\nInteresting outliersValues in your data that looks extreme until you take another variable or moderator into account..\nRandom outliersValues in your data that are extreme compared to the majority of data points..\n\nEven simpler, we can consider values as legitimate or not legitimate. Error outliers would be not legitimate as they represent a mistake or error, so they would potentially provide misleading results. These are values you can justify removing or correcting as they should not be there in the first place.\nInteresting and random outliers would be legitimate as they are not clear mistakes or errors; they are just different to the majority of values in the data. In most cases, it is not a good idea to remove these kind of values as they potentially tell you something interesting, but you might need to approach the data analysis in a different way to ensure the results are robust to extreme values. Alternatively, you might need to assume a different distibution, which we will start exploring in Chapter 5.\n\n4.3.1 Activity 4- Identifying error outliers\nUnless you can specifically identify values or participants you know contain errors, the main way to check is by ensuring the values are within known limits.\nWe can look at dawtry_clean and the key variables we explored in Chapter 1. Fairness and satisfaction was on a 1-9 scale, so we can check the minimum and maximum values and create a plot. For example, we can isolate the variable and apply the summary() function.\n\ndawtry_clean %&gt;% \n  select(fairness_satisfaction) %&gt;% \n  summary()\n\n fairness_satisfaction\n Min.   :1.000        \n 1st Qu.:2.000        \n Median :3.000        \n Mean   :3.539        \n 3rd Qu.:5.000        \n Max.   :9.000        \n\n\nThe minimum and maximum values are nice and consistent with what we expect.\nFor a visual check, we can also plot the minimum and maximum possible values on a boxplot. This is just a check for you, so you do not need to worry so much about the presentation.\n\ndawtry_clean %&gt;% \n  ggplot(aes(y = fairness_satisfaction, x = \"\")) + # make x blank \n  geom_boxplot() + \n  scale_y_continuous(limits = c(1, 9), \n                     breaks = seq(1, 9, 1)) + \n  geom_hline(yintercept = c(1, 9), # min and max values\n             linetype = 2) # create dashed line\n\n\n\n\n\n\n\n\n\n\n\n\n\nTry this\n\n\n\nIf you explore redistribution from dawtry_clean, the minimum and maximum values are 1-6. Does it look like there are any problematic looking values? \nYes\nNo\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNo, it looks like all values are within the expected 1-6 range.\n\ndawtry_clean %&gt;% \n  select(redistribution) %&gt;% \n  summary()\n\n redistribution\n Min.   :1.00  \n 1st Qu.:3.25  \n Median :4.00  \n Mean   :3.91  \n 3rd Qu.:4.75  \n Max.   :6.00  \n\n\nWe can also confirm this with a visual check.\n\ndawtry_clean %&gt;% \n  ggplot(aes(y = redistribution, x = \"\")) + # make x blank \n  geom_boxplot() + \n  scale_y_continuous(limits = c(1, 6), \n                     breaks = seq(1, 6, 1)) + \n  geom_hline(yintercept = c(1, 6), # min and max values\n             linetype = 2) # create dashed line\n\n\n\n\n\n\n\n\n\n\nIf you did identify error outliers to remove, then you could use filter() to directly remove values outside your known range, or you could first use case_when() to code observations as outliers or not, before deciding to filter them out.\n\n4.3.2 Activity 5- Identifying interesting or random outliers\nIdentifying error outliers relies on manually setting known minimum and maximum values, whereas identifying interesting or random outliers relies on data driven boundaries. For this example, we focus on univariate outliers, where we focus on one variable at a time. When we return to checking assumptions of regression models, you can identify interesting or random outliers through observations with large leverage / Cook’s distance values.\nIn general, we recommend not removing outliers providing you are confident they are not errors. It is better to focus on modelling your outcome in a more robust way. However, it is also important you know how to identify errors for strategies you will come across in published research.\nWe focus here on setting boundaries using the median absolute deviation as recommended by Leys et al. (2019). You will see other approaches in the literature, but this method is useful as it’s influenced less by the very outliers it is trying to identify. We will use lopez_clean from Lopez et al. (2023) for this section.\nThere are two main steps to this process because we have two groups and each group will have different boundaries. If you only have individual variables, then you could just mutate() your data, without the initial group_by() and summarise() step.\nFirst, we group by the condition to get one value per group. We then calculate a few values for the median ounces of soup, 3 times the MAD in line with Leys et al., then calculating the upper and lower bound using these objects.\n\n# create a new object with values per group\nmad_bounds &lt;- lopez_clean %&gt;% \n  group_by(Condition_label) %&gt;% \n  summarise(oz_median = median(M_postsoup), # median of soup in oz\n            oz_MAD = 3 * mad(M_postsoup), # 3 times the MAD\n            lower = oz_median - oz_MAD, # lower bound \n            upper = oz_median + oz_MAD) # upper bound\n\nmad_bounds\n\n\n\n\nCondition_label\noz_median\noz_MAD\nlower\nupper\n\n\n\nControl\n7.8\n14.67774\n-6.87774\n22.47774\n\n\nExperimental\n10.6\n17.56881\n-6.96881\n28.16881\n\n\n\n\n\n\nIn this example, the lower bound is lower than 0 as the smallest possible value. The upper bounds are then between 22 and 28 depending on the group.\nSecond, we must add these values to the other information we have available. We join the data sets using Condition_label. This adds the relevant values to each group. We then use mutate() and case_when() to label values as outliers or not. If they are outside the lower and upper bounds, they are labelled as “outlier”. If they are inside the lower and upper bounds, they are labelled as “no outlier”.\n\nlopez_mad &lt;- lopez_clean %&gt;% \n  inner_join(mad_bounds, by = \"Condition_label\") %&gt;% \n  mutate(oz_outlier = case_when(M_postsoup &lt; lower | M_postsoup &gt; upper ~ \"Outlier\",\n                                M_postsoup &gt;= lower | M_postsoup &lt;= upper ~ \"No Outlier\"))\n\nWe can use these in one of two ways. First, we can visualise the presence of outliers by adding coloured points. These are checks for you again, so you do not need to worry about the plot formatting.\n\nlopez_mad %&gt;% \n  ggplot(aes(x = Condition_label, y = M_postsoup)) + \n  geom_boxplot() + \n  geom_point(aes(colour = oz_outlier)) # needs to be within aes to set dynamic values\n\n\n\n\n\n\n\nWe can see a few values per group flagged as outliers using this criterion. If you did decide to remove outliers, then you could use filter to remove them:\n\nlopez_remove &lt;- lopez_mad %&gt;% \n  filter(oz_outlier == \"No Outlier\")\n\n\n\n\n\n\n\nTry this\n\n\n\nIf you switch to dawtry_clean from Dawtry et al. (2015), apply the MAD procedure to the variable fairness_satisfaction. Does it look like there are any outliers using this criterion? \nYes\nNo.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nNo, none of the values are outside the MAD thresholds. The thresholds are well beyond the minimum and maximum possible values of 1-9 for this variable.\n\ndawtry_mad &lt;- dawtry_clean %&gt;% \n  mutate(fs_median = median(fairness_satisfaction), # median of fairness/satisfaction\n         fs_MAD = 3 * mad(fairness_satisfaction), # 3 times the MAD\n         lower = fs_median - fs_MAD, # lower bound \n         upper = fs_median + fs_MAD,  # upper bound\n         fs_outlier = case_when(fairness_satisfaction &lt; lower | fairness_satisfaction &gt; upper ~ \"Outlier\",\n                                fairness_satisfaction &gt;= lower | fairness_satisfaction &lt;= upper ~ \"No Outlier\"))\n\nFor this variable and it’s bounded scale, no value is above or below the thresholds. You can see this in the data, or add horizontal lines in a plot since we are only plotting one variable. The dashed lines are the MAD thresholds and the solid lines are the minimum and maximum possible values.\n\ndawtry_mad %&gt;% \n  ggplot(aes(y = fairness_satisfaction, x = \"\")) + \n  geom_boxplot() + \n  geom_hline(aes(yintercept = lower), \n             linetype = 2) + # dashed line\n  geom_hline(yintercept = c(1, 9)) + \n  geom_hline(aes(yintercept = upper), \n             linetype = 2) + \n  scale_y_continuous(limits = c(-4, 10), \n                     breaks = seq(-4, 10, 2))\n\n\n\n\n\n\n\n\n\n\nRemember: identifying outliers is a crucial researcher degree of freedom, so pre-register your choice of outlier detection wherever possible, and document how many outliers you removed. We still recommend favouring a more robust model, but you can make an informed decision now you know how to identify outliers in the data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data, outliers, and checking assumptions</span>"
    ]
  },
  {
    "objectID": "04-screening-data.html#checking-assumptions",
    "href": "04-screening-data.html#checking-assumptions",
    "title": "\n4  Missing data, outliers, and checking assumptions\n",
    "section": "\n4.4 Checking assumptions",
    "text": "4.4 Checking assumptions\nThe final section revisits checking assumptions from Chapter 1 to 3. In those chapters, we introduced the concepts and stuck with the output regardless of whether we were happy with the assumptions or not. In this chapter, we will introduce potential solutions.\nAs a reminder, the assumptions for simple linear regression are:\n\nThe outcome is interval/ratio level data.\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\nThe predictors have non-zero variance.\nThe relationship between the outcome and predictor is linear.\nThe residuals should be normally distributed.\nThere should be homoscedasticity.\n\nAssumptions 1-4 are pretty straight forward as they relate to your understanding of the design or a simple check on the data. On the other hand, assumptions 5-7 require diagnostic checks.\nFor this part, we focus on Lopez et al. (2023) as the assumptions did not look quite right in Chapter 2. As a reminder, we can get a quick diagnostic check by running plot() on the model object:\n\n# Condition as a factor containing 0 and 1\nlm_cals_numbers &lt;- lm(formula = F_CaloriesConsumed ~ Condition, \n                      data = lopez_clean)\n\n# Change the panel layout to 2 x 2\npar(mfrow = c(2,2))\n\n# plot the diagnostic plots \nplot(lm_cals_numbers)\n\n\n\n\n\n\n\nAll of the checks look good apart from normality. We have a clear deviation from the line to curve around at lower and higher values along the x-axis. In Chapter 2, we said we would stick with it as it was consistent with the original article’s analyses, but now we will outline options available to you:\n\nParametric tests are robust.\nTreat the data as non-parametric.\nUse an alternative model.\n\n\n4.4.1 Activity 6 - Parametric tests are robust\nOne get out jail free card is doing nothing as the parametric tests are robust to violations of assumptions. You will often hear this and it is largely true under certain conditions. Knief & Forstmeier (2021) report a simulation study where they explore how violating assumptions like linearity, normality, and outliers affect the results of parametric statistical tests like simple linear regression. They found the tests were robust to violations - particularly normality - apart from when there were extreme outliers which could bias the results.\nIn the Lopez et al. example, we identified a few outliers using the 3 times the MAD criterion and the strictest Cook’s distance cut-off in Chapter 2, but normality was the only notable problem in the diagnostic checks. One option would be to feel reassured the results are robust to minor violations and explain that in your report.\nThe second option would be checking the results are robust to the presence of outliers. Remember, we advise excluding outliers as a last resort if you consider them legitimate, but it provides a robustness check to see if you get similar conclusions with and without the outliers. For example, we can exclude outliers using the MAD criterion and check the results:\n\n# remove outliers under 3 * MAD \n\nlm_cals_outliers &lt;- lm(formula = F_CaloriesConsumed ~ Condition, \n                      data = filter(lopez_mad, \n                                    oz_outlier == \"No Outlier\"))\n\nsummary(lm_cals_outliers)\n\n\nCall:\nlm(formula = F_CaloriesConsumed ~ Condition, data = filter(lopez_mad, \n    oz_outlier == \"No Outlier\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-220.65  -87.56  -15.30   65.84  369.35 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  182.573      7.485  24.393  &lt; 2e-16 ***\nCondition     66.906     10.903   6.136 1.85e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 115.7 on 450 degrees of freedom\nMultiple R-squared:  0.07722,   Adjusted R-squared:  0.07517 \nF-statistic: 37.66 on 1 and 450 DF,  p-value: 1.852e-09\n\n\nThe conclusions are very similar. The difference is still statistically significant and instead of a 63 calorie difference for the slope, we get a 67 calorie difference. So, we can stick with the original results and feel reassured that the results are robust to the presence of outliers, given the criterion we used. You would explain to the reader you checked the robustness of the results and what your final conclusion was.\n\n4.4.2 Activity 7 - Treat the data as non-parametric\nThe second option is switching to a non-parametric statistical test which makes fewer assumptions about the data. In Chapter 1, we introduced the Spearman correlation as a non-parametric equivalent. Instead of doubling the number of non-parametric tests you need to learn, we can apply a similar principle to linear regression and recreate non-parametric equivalents.\nIn the demonstration by Lindeløv, 2019, common statistical tests you come across are specific applications of the general linear model. Likewise, you can recreate non-parametric tests by converting continuous variables to ranks and using those in the regression model.\nFor example, instead of a Mann-Whitney U test, you can run wrap the number of calories (F_CaloriesConsumed) in the rank() function:\n\n# Condition_label as numbers\nlm_cals_ranks &lt;- lm(formula = rank(F_CaloriesConsumed) ~ Condition, \n                     data = lopez_clean)\n\nsummary(lm_cals_ranks)\n\n\nCall:\nlm(formula = rank(F_CaloriesConsumed) ~ Condition, data = lopez_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-261.138 -110.361    1.862  111.467  263.967 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   200.03       8.27  24.188  &lt; 2e-16 ***\nCondition      69.11      12.06   5.728 1.84e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 129.7 on 462 degrees of freedom\nMultiple R-squared:  0.0663,    Adjusted R-squared:  0.06428 \nF-statistic: 32.81 on 1 and 462 DF,  p-value: 1.836e-08\n\n\nThis means instead of comparing the raw number of calories between groups, you compare the ranks between groups. The logic here is you reduce the impact of extreme values as they become ranks instead of raw numbers, so like using the median over the mean, the difference between the 99th and 100th number would be one position, instead of a huge difference in actual value. For example, if you had the collection of numbers 1, 2, 3, 4, and 100, there is a difference in raw and ranked values:\n\nc(1, 2, 3, 4, 100)\n\n[1]   1   2   3   4 100\n\nrank(c(1, 2, 3, 4, 100))\n\n[1] 1 2 3 4 5\n\n\nIn the write-up, just note you still report the model and null hypothesis significance testing elements like the p-value, but the slope is less interpretable as an effect size. It is now a difference in ranks, so it will be more useful to calculate the difference in medians to present the reader.\n\n\n\n\n\n\nTry this\n\n\n\nIf you switch to dawtry_clean from Dawtry et al. (2015), apply the same logic to a correlational design. Is there a significant association between fairness_satisfaction and redistribution if you convert both variables to ranks in a regression model? \nYes\nNo.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes, you get a similar finding to the original model where you predict raw values of redistribution from fairness and satisfaction. This time, you are looking at the association in ranks instead. There is still a statistically significant negative relationship and this recreates the process behind the Spearman correlation.\n\n# Variables as ranks\nlm_dawtry_ranks &lt;- lm(formula = rank(redistribution) ~ rank(fairness_satisfaction), \n                     data = dawtry_clean)\n\nsummary(lm_dawtry_ranks)\n\n\nCall:\nlm(formula = rank(redistribution) ~ rank(fairness_satisfaction), \n    data = dawtry_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-217.133  -43.673   -7.611   37.367  228.315 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                 257.88387    7.46475   34.55   &lt;2e-16 ***\nrank(fairness_satisfaction)  -0.68552    0.04239  -16.17   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 64.56 on 303 degrees of freedom\nMultiple R-squared:  0.4633,    Adjusted R-squared:  0.4615 \nF-statistic: 261.6 on 1 and 303 DF,  p-value: &lt; 2.2e-16\n\n\nFor further reinforcement of how this is the same process underlying Spearman’s correlation, take the square root of the \\(R^2\\) value in this model and compare it to the value of \\(r_s\\) from Chapter 1.\n\n\n\n\n4.4.3 Use an alternative model\nThe third and final option is considering whether a parametric test is the right choice in the first place. The default assumption in psychology research is assuming everything is linear and normal as it is convenient and applies to many scenarios, but there are some outcomes and models which will always be inappropriate to model as linear and normal. For example, if your outcome is dichotomous (0 or 1; correct or incorrect), then you must assume a binomial distributionInstead of the parameters mean and standard deviation in a normal distribution, you have N trials with n number of successes. and use something like a logistic regressionModelling the log-odds of a dichotomous outcome as a linear combination of one or more predictors. model. That is what we will explore in the next chapter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data, outliers, and checking assumptions</span>"
    ]
  },
  {
    "objectID": "04-screening-data.html#test-yourself",
    "href": "04-screening-data.html#test-yourself",
    "title": "\n4  Missing data, outliers, and checking assumptions\n",
    "section": "\n4.5 Test yourself",
    "text": "4.5 Test yourself\nTo end the chapter, we have some knowledge check questions to test your understanding of the concepts we covered in the chapter.\nFor this chapter’s knowledge check section, instead of purely conceptual questions about functions, we have an example model and output and we would like you to consider the presence of missing data, outliers, and modelling assumptions.\nWe have a simulated study where our research question is “Are scientists perceived to be more objective than highly-educated lay people”? Participants were randomly allocated to rate the characteristics of scientists or highly-educated lay people. The outcome was a 0-10 scale made up of 10 items of how objective they perceive the target to be, from not very objective to very objective.\nQuestion 1. Based on the description of the study, the mostly likely design is: \nOne-sample\nCorrelational\nWithin-subjects\nBetween-subjects\nQuestion 2. Look at the following summary table of the data we are working with. Is there any missing data present? \nYes\nNo\n\n\n      id                   group      objectivity   \n Length:600         lay       :300   Min.   :0.689  \n Class :character   scientists:300   1st Qu.:3.509  \n Mode  :character                    Median :4.232  \n                                     Mean   :4.190  \n                                     3rd Qu.:4.897  \n                                     Max.   :7.959  \n                                     NA's   :9      \n\n\nQuestion 3. Look at the following boxplot of the data we are working with. We have applied the criterion 3 times the MAD to highlight potential extreme values.\n\nLooking at the highly-educated group, are there any potential extreme values? \nYes\nNo\nLooking at the scientist group, are there any potential extreme values? \nYes\nNo\n\n\n\n\n\n\n\n\n\nQuestion 4. If we fit a linear regression model, the effect of group is \nstatistically significant\nnot statistically significant and the \nhighly-educated lay person\nscientist scored higher on objectivity.\n\n\n\nCall:\nlm(formula = objectivity ~ group, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.7514 -0.6257  0.0392  0.6479  3.5185 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.94113    0.05944  66.299  &lt; 2e-16 ***\ngroupscientists  0.49932    0.08428   5.924 5.33e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.024 on 589 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.05624,   Adjusted R-squared:  0.05464 \nF-statistic:  35.1 on 1 and 589 DF,  p-value: 5.331e-09\n\n\nQuestion 5. If we check the modelling assumptions and focus on normality and homoscedasticity, does it look like we meet the assumptions for linear regression? \nYes\nNo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nFor normality in the first plot, it does not look like there is a significant deviation in expected values against observed values. We have a few data points highlighted at the top and bottom of the points, but there is not a consistent deviation to the pattern.\nLikewise, for homoscedasticity in the second plot, the red line is horizontal and there does not appear to be a noticeable difference in the variance between each group.\n\n\n\nQuestion 6. If you were concerned about the presence of potential outliers from question 3, are the regression results robust if we remove those outliers? \nYes\nNo\n\n\n\nCall:\nlm(formula = objectivity ~ group, data = filter(df, obj_outlier == \n    \"No Outlier\"))\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8339 -0.6167  0.0405  0.6370  2.6338 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.94113    0.05713  68.990  &lt; 2e-16 ***\ngroupscientists  0.49062    0.08135   6.031 2.89e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9845 on 584 degrees of freedom\nMultiple R-squared:  0.05864,   Adjusted R-squared:  0.05702 \nF-statistic: 36.38 on 1 and 584 DF,  p-value: 2.887e-09\n\n\n\n\n\n\n\n\nExplain this answer\n\n\n\n\n\nYes, there is very little difference in the two results, so you would confident the potential outliers are not driving the results. They are both statistically significant and there is very little different in the main effect size of interest, where the slope changes from 0.499 in question 4 to 0.491 here.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data, outliers, and checking assumptions</span>"
    ]
  },
  {
    "objectID": "04-screening-data.html#words-from-this-chapter",
    "href": "04-screening-data.html#words-from-this-chapter",
    "title": "\n4  Missing data, outliers, and checking assumptions\n",
    "section": "\n4.6 Words from this Chapter",
    "text": "4.6 Words from this Chapter\nBelow you will find a list of words that were used in this chapter that might be new to you in case it helps to have somewhere to refer back to what they mean. The links in this table take you to the entry for the words in the PsyTeachR Glossary. Note that the Glossary is written by numerous members of the team and as such may use slightly different terminology from that shown in the chapter.\n\n\nterm\ndefinition\n\n\n\nbinomial distribution\nInstead of the parameters mean and standard deviation in a normal distribution, you have *N* trials with *n* number of successes.\n\n\nError outliers\nA mistake or impossible value in your data.\n\n\nInteresting outliers\nValues in your data that looks extreme until you take another variable or moderator into account.\n\n\nlogistic regression\nModelling the log-odds of a dichotomous outcome as a linear combination of one or more predictors.\n\n\nMissing at random\nWe can predict the missing value using other variables in the data.\n\n\nMissing completely at random\nWhether the data are missing or not is completely unrelated to other variables in the data.\n\n\nMissing data\nIf a participant or observation contains no data for one or more cells, they have missing data.\n\n\nMissing not at random\nWhether the data are missing or not is causally related to one or more other variables in the data.\n\n\nRandom outliers\nValues in your data that are extreme compared to the majority of data points.\n\n\n\n\n\n\n\nDawtry, R. J., Sutton, R. M., & Sibley, C. G. (2015). Why Wealthier People Think People Are Wealthier, and Why It Matters: From Social Sampling to Attitudes to Redistribution. Psychological Science, 26(9), 1389–1400. https://doi.org/10.1177/0956797615586560\n\n\nJakobsen, J. C., Gluud, C., Wetterslev, J., & Winkel, P. (2017). When and how should multiple imputation be used for handling missing data in randomised clinical trials – a practical guide with flowcharts. BMC Medical Research Methodology, 17(1), 162. https://doi.org/10.1186/s12874-017-0442-1\n\n\nKnief, U., & Forstmeier, W. (2021). Violating the normality assumption may be the lesser of two evils. Behavior Research Methods, 53(6), 2576–2590. https://doi.org/10.3758/s13428-021-01587-5\n\n\nLeys, C., Delacre, M., Mora, Y. L., Lakens, D., & Ley, C. (2019). How to Classify, Detect, and Manage Univariate and Multivariate Outliers, With Emphasis on Pre-Registration. International Review of Social Psychology, 32(1), 5. https://doi.org/10.5334/irsp.289\n\n\nLopez, A., Choi, A. K., Dellawar, N. C., Cullen, B. C., Avila Contreras, S., Rosenfeld, D. L., & Tomiyama, A. J. (2023). Visual cues and food intake: A preregistered replication of Wansink et al (2005). Journal of Experimental Psychology: General. https://doi.org/10.1037/xge0001503.supp",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Missing data, outliers, and checking assumptions</span>"
    ]
  },
  {
    "objectID": "04-screening-data.html#end-of-chapter",
    "href": "04-screening-data.html#end-of-chapter",
    "title": "\n3  Missing data, outliers, and checking assumptions\n",
    "section": "\n3.7 End of Chapter",
    "text": "3.7 End of Chapter\nThat is the final chapter you will complete for the Research Methods 1 component of the course!\nWe hope you take a moment to think about everything you have achieved so far. Think back to how you felt when you were completing Chapters 1 and 2 and learning what a function is. We must get through a huge amount of content in a short space of time on the MSc conversion programmes, so we hope you are all very proud in how far you have come. We cannot guarantee everyone will come out of the course loving R and data skills, but we hope you all recognise you can do this.\nThe next few chapters you will cover in Research Methods 2, so make sure you refresh your memory of the RM1 content prior to starting as the topics build on what you already know around data skills and statistics.\n\n\n\n\nDawtry, R. J., Sutton, R. M., & Sibley, C. G. (2015). Why Wealthier People Think People Are Wealthier, and Why It Matters: From Social Sampling to Attitudes to Redistribution. Psychological Science, 26(9), 1389–1400. https://doi.org/10.1177/0956797615586560\n\n\nJakobsen, J. C., Gluud, C., Wetterslev, J., & Winkel, P. (2017). When and how should multiple imputation be used for handling missing data in randomised clinical trials – a practical guide with flowcharts. BMC Medical Research Methodology, 17(1), 162. https://doi.org/10.1186/s12874-017-0442-1\n\n\nKnief, U., & Forstmeier, W. (2021). Violating the normality assumption may be the lesser of two evils. Behavior Research Methods, 53(6), 2576–2590. https://doi.org/10.3758/s13428-021-01587-5\n\n\nLeys, C., Delacre, M., Mora, Y. L., Lakens, D., & Ley, C. (2019). How to Classify, Detect, and Manage Univariate and Multivariate Outliers, With Emphasis on Pre-Registration. International Review of Social Psychology, 32(1), 5. https://doi.org/10.5334/irsp.289\n\n\nLopez, A., Choi, A. K., Dellawar, N. C., Cullen, B. C., Avila Contreras, S., Rosenfeld, D. L., & Tomiyama, A. J. (2023). Visual cues and food intake: A preregistered replication of Wansink et al (2005). Journal of Experimental Psychology: General. https://doi.org/10.1037/xge0001503.supp",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Missing data, outliers, and checking assumptions</span>"
    ]
  },
  {
    "objectID": "03-lm-multiple.html",
    "href": "03-lm-multiple.html",
    "title": "\n3  Multiple Linear Regression\n",
    "section": "",
    "text": "3.1 Chapter preparation\nIn this chapter, we need a few different datasets, so we will introduce them as we need them.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "03-lm-multiple.html#chapter-preparation",
    "href": "03-lm-multiple.html#chapter-preparation",
    "title": "\n3  Multiple Linear Regression\n",
    "section": "",
    "text": "3.1.1 Organising your files and project for the chapter\nBefore we can get started, you need to organise your files and project for the chapter, so your working directory is in order.\n\nIn your folder for statistics and research design Stats_Research_Design, create a new folder called 03_regression_multiple. Within 03_regression_multiple, create two new folders called data and figures.\nCreate an R Project for 03_regression_multiple as an existing directory for your chapter folder. This should now be your working directory.\nCreate a new Quarto document and give it a sensible title describing the chapter, such as 03 Multiple Regression. Save the file in your 03_regression_multiple folder.\nWe are working with a few data sets in this chapter, so please save the following zip file containing all the data files you need: Chapter 03 Data. Right click the link and select “save link as”, or clicking the link will save the file to your Downloads. Extract the files and make sure that you save them as “.csv”. Save or copy the files to your data/ folder within 03_regression_multiple.\n\nYou are now ready to start working on the chapter!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "03-lm-multiple.html#coding-schemes",
    "href": "03-lm-multiple.html#coding-schemes",
    "title": "\n3  Multiple Linear Regression\n",
    "section": "\n3.2 Different categorical predictor coding schemes",
    "text": "3.2 Different categorical predictor coding schemes\n\n3.2.1 Introduction to the dataset\nTo demonstrate different predictor coding schemes, we will use data from James et al. (2015) who wanted to find non-pharmacological interventions for reducing intrusive memories of traumatic events. They compared four conditions:\n\nNo-task control: Participants completed a 10-minute filler task.\nReactivation + Tetris: Participants were shown a series of images from a trauma film to reactivate traumatic memories. After a filler task, participants played Tetris for 12 minutes.\nTetris Only: Participants played Tetris for 12 minutes in isolation.\nReactivation Only: Participants completed the reactivation task in isolation.\n\nTheir research question was: Would the reactivation and Tetris condition lead to fewer intrusive memories? They predicted the reactivation and Tetris group will have fewer intrusive memories in the week after experimental trauma exposure compared to the other three groups.\nThis means there is one predictor variable Condition with four levels, one for each experimental condition. We then have one outcome variable post_intrusions for the number of intrusive memories they recorded in the week after the experiment. To support the hypothesis, post_intrusions should decrease in group 2 compared to the three other groups.\nWe first need to load some packages and the data for the tasks throughout this chapter. If you do not have any of the packages, make sure you install them first.\n\n# wrangling and visualisation functions \nlibrary(tidyverse)\n# Regression interaction plots\nlibrary(sjPlot)\n# Standardise model coefficients\nlibrary(effectsize)\n# VIF and other regression functions\nlibrary(car)\n# Interaction estimates\nlibrary(emmeans)\n# Checking model assumptions\nlibrary(performance)\n\n# Load data for coding schemes\njames_data &lt;- read_csv(\"data/James_2015.csv\") %&gt;% \n  mutate(Condition = as.factor(Condition)) %&gt;% \n  rename(post_intrusions = Days_One_to_Seven_Number_of_Intrusions)\n\n\n3.2.2 Exploratory data analysis\nWhen starting any data analysis, it is important to visualise the data for some exploratory data analysis. Using the skills you developed in Data Skills for Reproducible Research, you can explore the data to understand its properties and look for potential patterns. We can create a boxplot to get a brief overview of how the number of intrusive memories changes across the four condition groups.\n\njames_data %&gt;% \n  ggplot(aes(x = Condition, y = post_intrusions)) + \n  geom_boxplot() + \n  scale_y_continuous(name = \"Number of Intrusive Memories\") +\n  scale_x_discrete(name = \"Experimental Task\", \n                   label = c(\"Control\", \"Reactivation + Tetris\", \"Tetris\", \"Reactivation\")) + \n  theme_classic()\n\n\n\n\n\n\n\nWe can see group 2 (reactivation + Tetris) has the lowest score, but we need inferential statistics to test the hypothesis.\n\n3.2.3 No specific coding\nFor a starting point, we can see what it looks like if you enter post_intrusions as the outcome and Condition as the predictor in simple linear regression with no specific coding scheme.\n\njames_nocoding &lt;- lm(post_intrusions ~ Condition, \n                     data = james_data)\n\nsummary(james_nocoding)\n\n\nCall:\nlm(formula = post_intrusions ~ Condition, data = james_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1111 -1.8889 -0.8333  1.1111 10.8889 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.1111     0.7485   6.828 2.89e-09 ***\nCondition2   -3.2222     1.0586  -3.044  0.00332 ** \nCondition3   -1.2222     1.0586  -1.155  0.25231    \nCondition4   -0.2778     1.0586  -0.262  0.79381    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.176 on 68 degrees of freedom\nMultiple R-squared:  0.1434,    Adjusted R-squared:  0.1056 \nF-statistic: 3.795 on 3 and 68 DF,  p-value: 0.01409\n\n\nAlthough we use Condition as a single predictor, we actually get three different predictors. In regression, it can only compare two groups at a time, so it will apply dummy coding to your groups. You get k-1 predictors, where there will be one fewer predictors than there are groups.\nUsing what you learnt in chapters 1 and 2, we can see its a significant model, but the only significant predictor is Condition2. Group 1 is the reference group as the intercept, so Condition2 tells you the mean difference between group 1 and group 2. We compare each group successively to the reference, so Condition3 is the mean difference between group 1 and group 3 etc.\n\n3.2.4 Dummy coding\nTo demonstrate different coding schemes, we will start with dummy coding. Dummy coding is the default in R and when we entered Condition as a predictor, this is what R does behind the scenes. However, we have no control over the process and the reference group/intercept will be first in numerical or alphabetical order, then each successive k-1 group will be added as separate predictors.\nInstead, we can define our own dummy coding to control the reference and target groups. This is particularly useful when you have specific hypotheses like in James et al. (2015), as we are interested in the combined reactivation and Tetris group (group 2). This means we can code for condition 2 as the reference group / intercept, and the other groups are coded as individual predictors.\nFor dummy coding, you will have k-1 predictors, meaning one fewer predictor than the number of groups. Since we have four groups, we will need to create three predictors. In the code below, our reference group will always be set to 0. Then, for each dummy coded predictor, we will set the target group to 1. Each target group will be 1 for when it is a predictor, but reactivation and Tetris (condition 2) will always be set to 0.\n\n# Dummy code condition 2 as 0 for each comparison, and each successive group as 1\njames_data &lt;- james_data %&gt;% \n  # For each group at a time, code as 1, with the default set to 0 for all other groups\n  mutate(RT_control = case_when(Condition == 1 ~ 1, \n                                .default = 0),\n         RT_tetris = case_when(Condition == 3 ~ 1, \n                               .default = 0),\n         RT_reactivation = case_when(Condition == 4 ~ 1, \n                                     .default = 0))\n\nWe can get an overview of how this looks by checking the distinct values against Condition.\n\njames_data %&gt;% \n  distinct(Condition, RT_control, RT_tetris, RT_reactivation)\n\n\n\n\nCondition\nRT_control\nRT_tetris\nRT_reactivation\n\n\n\n1\n1\n0\n0\n\n\n2\n0\n0\n0\n\n\n3\n0\n1\n0\n\n\n4\n0\n0\n1\n\n\n\n\n\n\nThe three predictors show how each comparison is dummy coded. For RT_control, group 1 is coded 1 and all the others are coded 0. For RT_tetris, group 3 is coded 1 and all the others are coded 0 etc. Group 2 is the only one without ever being coded 1.\n\n\n\n\n\n\nNote\n\n\n\nRemember the interpretation of the intercept is what the outcome value is when the predictors are set to 0. Setting all the predictors to 0 would indicate group 2 in this case, so that represents our reference group. See the predictors as little switches to turn on and off, and when they are all turned off (0), that represents the reference group.\n\n\nWe can now see what this looks like as our regression model containing dummy coded predictors.\n\njames_dummy &lt;- lm(post_intrusions ~ RT_control + RT_tetris + RT_reactivation, \n                  data = james_data)\n\nsummary(james_dummy)\n\n\nCall:\nlm(formula = post_intrusions ~ RT_control + RT_tetris + RT_reactivation, \n    data = james_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1111 -1.8889 -0.8333  1.1111 10.8889 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)       1.8889     0.7485   2.523  0.01396 * \nRT_control        3.2222     1.0586   3.044  0.00332 **\nRT_tetris         2.0000     1.0586   1.889  0.06312 . \nRT_reactivation   2.9444     1.0586   2.781  0.00700 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.176 on 68 degrees of freedom\nMultiple R-squared:  0.1434,    Adjusted R-squared:  0.1056 \nF-statistic: 3.795 on 3 and 68 DF,  p-value: 0.01409\n\n\nThis gives us a similar result to before, some of the coefficients are even the same, but we are controlling what the reference group is for the intercept, and what each target comparison is. They are all positive predictors showing the number of intrusive memories is higher in each control group compared to the combined reactivation and Tetris group. However, the difference to Tetris in isolation is not statistically significant.\nFor these initial steps, it is also a good opportunity to sense check how the comparisons work. The intercept is the mean for our reference group - reactivation plus Tetris. Each coefficient is then the mean difference for each target group against the reference. We can show this is the case by comparing the means.\n\n# Calculate and isolate the mean for group 2 as our reference group\nRT_mean &lt;- james_data %&gt;% \n  # Isolate group 2\n  filter(Condition == 2) %&gt;% \n  summarise(mean_intrusions = mean(post_intrusions)) %&gt;% \n  # Isolate the first value\n  pluck(1)\n\n# For each other group, calculate the mean difference between the group mean and RT mean\njames_data %&gt;% \n  # Omit group 2\n  filter(Condition != 2) %&gt;% \n  # Get mean difference for all other groups\n  group_by(Condition) %&gt;% \n  summarise(mean_difference = mean(post_intrusions) - RT_mean)\n\n\n\n\nCondition\nmean_difference\n\n\n\n1\n3.222222\n\n\n3\n2.000000\n\n\n4\n2.944444\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe can see the mean for group 2 and the mean differences align with exactly what we expect. It is important to work through the process now as the group means align with the model estimates. This will not be the case when we have multiple predictors since the coefficients become partial effects.\n\n\n\n3.2.5 Deviation coding\nFinally, we have deviation coding. This is more useful once we get to interactions later, but it is easier to see the logic behind what it is doing when we have no other predictors to worry about.\nRemember, in dummy coding, the intercept is the reference group mean, and each dummy coded predictor is the mean difference against the reference group.\nIn deviation coding, the interpretation of the intercept changes to be the grand mean of all observations, i.e., taking the mean of all four groups. The coefficients are then the difference between each comparison group with the grand mean for a main effect. We still need to create a new predictor for k-1 groups, but instead of coding 0 and 1, we use 0.5 and -0.5.\nYou might see different ways of deviation coding. When you use 0.5/-0.5, you can calculate the effect of the group as 0.5 times the slope, which tells you the difference to the grand mean. For 1/-1. you calculate the effect of the group as 1 times the slope, which tells you the difference to the grand mean. We will typically use 0.5/-0.5 for consistency with other materials, but be aware you might see each approach.\n\n# Create deviation coding for k-1 groups\n# In this method, group 2 as our reference will be -0.5 in each comparison. \n# Variables not included in the predictor are set to 0\n# The target group is set to 0.5 for each predictor\njames_data &lt;- james_data %&gt;% \n  mutate(control_deviation = case_when(Condition == 1 ~ 0.5,\n                                       Condition %in% c(3, 4) ~ 0,\n                                       Condition == 2 ~ -0.5),\n         tetris_deviation = case_when(Condition == 3 ~ 0.5,\n                                       Condition %in% c(1, 4) ~ 0,\n                                       Condition == 2 ~ -0.5),\n         reactivation_deviation = case_when(Condition == 4 ~ 0.5,\n                                       Condition %in% c(1, 3) ~ 0,\n                                       Condition == 2 ~ -0.5))\n\nAs before, we can get an overview of how this looks by checking the distinct values against Condition.\n\njames_data %&gt;% \n  distinct(Condition, \n           control_deviation, \n           tetris_deviation, \n           reactivation_deviation)\n\n\n\n\n\n\n\n\n\n\nCondition\ncontrol_deviation\ntetris_deviation\nreactivation_deviation\n\n\n\n1\n0.5\n0.0\n0.0\n\n\n2\n-0.5\n-0.5\n-0.5\n\n\n3\n0.0\n0.5\n0.0\n\n\n4\n0.0\n0.0\n0.5\n\n\n\n\n\n\nThe three predictors show how each comparison is deviation coded. For control_deviation, group 1 is coded 0.5, the predictors to ignore are coded 0, and the target group is coded -0.5. For tetris_deviation, group 3 is coded 0.5 etc. Group 2 is always coded -0.5 in this case.\n\n\n\n\n\n\nNote\n\n\n\nRemember the interpretation of the intercept is what the outcome value is when the predictors are set to 0. The interpretation shifts here as the intercept represents the grand mean across the group. When the predictors are set to 0, the outcome is in the middle of -0.5 and 0.5, so its the mid point or average. When you do not have interactions, this might not be as useful, but its easier to understand the logic when there are no partial effects.\n\n\nWe can now see what this looks like for our regression model containing deviation coded predictors.\n\njames_deviation &lt;- lm(post_intrusions ~ control_deviation + tetris_deviation + reactivation_deviation, \n                      data = james_data)\n\nsummary(james_deviation)\n\n\nCall:\nlm(formula = post_intrusions ~ control_deviation + tetris_deviation + \n    reactivation_deviation, data = james_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.1111 -1.8889 -0.8333  1.1111 10.8889 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)             3.93056    0.37427  10.502 7.11e-16 ***\ncontrol_deviation       2.36111    1.29652   1.821    0.073 .  \ntetris_deviation       -0.08333    1.29652  -0.064    0.949    \nreactivation_deviation  1.80556    1.29652   1.393    0.168    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.176 on 68 degrees of freedom\nMultiple R-squared:  0.1434,    Adjusted R-squared:  0.1056 \nF-statistic: 3.795 on 3 and 68 DF,  p-value: 0.01409\n\n\nThis time, the results look a little different. None of the coefficients are significant, but importantly, the model is exactly the same. We are still explaining the same amount of variance in the outcome, but the way we are expressing it is different.\nDeviation coding is tricky at first to appreciate what its doing, so working through the logic is even more important than for dummy coding. This time, we need to calculate the grand mean of all observations, then compare the coefficients against the grand mean.\n\n# Calculate and isolate the grand mean of all groups\ngrand_mean &lt;- james_data %&gt;% \n  summarise(mean_intrusions = mean(post_intrusions)) %&gt;% \n  pluck(1)\n\n# Calculate the mean number of intrusions for each group\n# Then calculate the deviations as 2 times the mean difference \njames_data %&gt;% \n  group_by(Condition) %&gt;% \n  summarise(mean_intrusions = mean(post_intrusions),\n            deviations = (grand_mean - mean(post_intrusions)) * 2)\n\n\n\n\nCondition\nmean_intrusions\ndeviations\n\n\n\n1\n5.111111\n-2.3611111\n\n\n2\n1.888889\n4.0833333\n\n\n3\n3.888889\n0.0833333\n\n\n4\n4.833333\n-1.8055556\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe can see the grand mean and deviations align with exactly what we expect. It is trickier than dummy coding, but this is what deviation coding does. Note in the model you do not get the group 2 contrast as that’s always the reference group and we are ignoring the sign of the deviation to check its consistency. Also pay attention to calculating 2 times the difference, since we used 0.5/-0.5 as the coding scheme.\nIt is important to work through the process now as the deviations align with the model estimates. This will not be the case when we have multiple predictors since the coefficients become partial predictors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "03-lm-multiple.html#multiple-linear-regression-with-individual-predictors",
    "href": "03-lm-multiple.html#multiple-linear-regression-with-individual-predictors",
    "title": "\n3  Multiple Linear Regression\n",
    "section": "\n3.3 Multiple linear regression with individual predictors",
    "text": "3.3 Multiple linear regression with individual predictors\n\n3.3.1 Introduction to the dataset\nTo demonstrate multiple linear regression models, we will use data from Evans (2024) (the stage two manuscript is currently under review) who performed a multi-site registered replication report containing 2218 participants. They wanted to replicate Jones & Kavanagh (1996) - an influential study in organisational psychology on unethical workplace behaviour. The variables can be split into two types. The first are situational factors and relate to experimental manipulations on a vignette that participants read:\n\nWorkplace environment (QWE): Your workplace environment is described as high or low quality.\nManager influence (MI): Your manager is described as behaving ethically or unethically.\nPeer influence (PI): Your peers are described as behaving ethically or unethically.\n\nThe second group of variables are individual and relate to the participant rather than being experimentally manipulated:\n\nLocus of control (LOCTot): A measure of whether someone attributes events internally or externally.\nSocial desirability (SocDesTot): A measure of whether someone would respond in a way that would make them look better than they would typically act.\nMachiavellianism (MachTot): A personality trait that is part of the dark-triad, showing a lack of empathy and willingness to manipulate others.\n\nThe outcome in the study was then the participant’s unethical workplace behaviour intention (BehIntTot). The vignette described a situation in a company and the unethical behaviour was whether you would manipulate expense requests to claim more money than you really should. There were four questions which were added up to range from 4 to 20, with higher values meaning greater intention to behave unethically.\nOur research question for this study is: What is the effect of individual and situational factors on unethical workplace behaviour intentions? We will not pose specific hypotheses, but we essentially predict that these six factors will affect someone’s unethical workplace behaviour intention.\nTo start with, we will load the data and set the factor levels for independent variables.\n\n# Load data for multiple linear regression\nevans_data &lt;- read_csv(\"data/Evans_2023.csv\") %&gt;% \n  mutate(QWE = factor(QWE, levels = c(\"low\", \"high\")),\n         MI = factor(MI, levels = c(\"unethical\", \"ethical\")),\n         PI = factor(PI, levels = c(\"unethical\", \"ethical\")))\n\n\n3.3.2 Visualise the relationship\nWhen starting any data analysis, it is important to visualise the data for some exploratory data analysis. Using the skills you developed in Data Skills for Reproducible Research, explore the data understand its properties and look for potential patterns.\nPlease try this yourself and explore different variables and their relationship to unethical workplace behaviour. For example, we can look at the effect of quality of workplace environment:\n\nevans_data %&gt;% \n  ggplot(aes(x = QWE, y = BehIntTot)) + \n  geom_boxplot() + \n  scale_y_continuous(name = \"Sum of Behavioural Intentions\") +\n  scale_x_discrete(name = \"Quality of Workplace Environment\", label = c(\"Low\", \"High\")) +\n  theme_classic()\n\n\n\n\n\n\n\nAlternatively, we might look at the relationship between unethical behaviour intention and Machiavellianism:\n\nevans_data %&gt;% \n  ggplot(aes(x = MachTot, y = BehIntTot)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  scale_y_continuous(name = \"Sum of Behavioural Intentions\") +\n  scale_x_continuous(name = \"Machiavellianism\") +\n  theme_classic()\n\n\n\n\n\n\n\n\n3.3.3 Model 1: Situational factors\nNow you understand the data a little better, it is time to apply our modelling techniques to address our research question. We will go with the hierarchical approach to entering predictors to enter them in two steps. First, situational factors, then the individual factors on top of them.\n\nindividual_model1 &lt;- lm(BehIntTot ~ QWE + MI + PI, \n                        data = evans_data)\n\nsummary(individual_model1)\n\n\nCall:\nlm(formula = BehIntTot ~ QWE + MI + PI, data = evans_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.5855 -2.5855 -0.5956  2.9452 10.3357 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  10.5855     0.1628  65.018  &lt; 2e-16 ***\nQWEhigh      -0.3905     0.1629  -2.397  0.01663 *  \nMIethical    -0.5307     0.1630  -3.256  0.00115 ** \nPIethical    -0.4592     0.1630  -2.817  0.00488 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.837 on 2214 degrees of freedom\nMultiple R-squared:  0.01068,   Adjusted R-squared:  0.009341 \nF-statistic: 7.968 on 3 and 2214 DF,  p-value: 2.774e-05\n\n\nAll three predictors are statistically significant and are all negative. Since we coded the positive manipulations as the target groups, this shows people reported lower unethical workplace behaviour intentions in the high/ethical groups.\nWe explain a significant amount of variance in unethical behaviour intentions, but note the \\(R^2\\) and adjusted \\(R^2\\) values. They are approximately .01 or 1%, showing with 2000 plus participants, we can reject the null, but we explain only a modest proportion of variance.\nAt this point, it is an important lesson to see how the presence of other predictors affects the estimates of another predictor. For a demonstration, we will create another regression model, but only including one situational variable at a time. Feel free to create the other two yourself and compare.\n\nindividual_model1a &lt;- lm(BehIntTot ~ QWE, \n                         data = evans_data)\n\nsummary(individual_model1a)\n\n\nCall:\nlm(formula = BehIntTot ~ QWE, data = evans_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.0938 -2.0938 -0.7052  2.9062 10.2948 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  10.0938     0.1151  87.685   &lt;2e-16 ***\nQWEhigh      -0.3886     0.1635  -2.377   0.0176 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.851 on 2216 degrees of freedom\nMultiple R-squared:  0.002542,  Adjusted R-squared:  0.002092 \nF-statistic: 5.648 on 1 and 2216 DF,  p-value: 0.01756\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you compare the coefficient for QWE in model 1 to this new isolated model, we get slightly different estimates. Its close, but not exactly the same. That is because in a multiple linear regression model, the predictors become partial effects. We are trying to model different sources of variance, so what might be explained by a predictor in one model, might change in a model with additional predictors. So, you might find predictors that are statistically significant in one model are no longer significant in another model.\n\n\n\n3.3.4 Model 2: Situational and individual factors\nNow we have explored the effect of situational factors on unethical behaviour intentions, we can add the individual factors to see how well they contribute to the model.\n\nindividual_model2 &lt;- lm(BehIntTot ~ QWE + MI + PI + LOCTot + SocDesTot + MachTot, \n                        data = evans_data)\n\nsummary(individual_model2)\n\n\nCall:\nlm(formula = BehIntTot ~ QWE + MI + PI + LOCTot + SocDesTot + \n    MachTot, data = evans_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.0856 -2.7105 -0.4456  2.8414 11.8721 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  7.61519    1.00062   7.610 4.02e-14 ***\nQWEhigh     -0.41735    0.15513  -2.690  0.00719 ** \nMIethical   -0.47421    0.15532  -3.053  0.00229 ** \nPIethical   -0.39941    0.15526  -2.572  0.01016 *  \nLOCTot       0.02330    0.01687   1.381  0.16744    \nSocDesTot   -0.08860    0.01984  -4.467 8.35e-06 ***\nMachTot      0.18967    0.01773  10.697  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.652 on 2211 degrees of freedom\nMultiple R-squared:  0.1049,    Adjusted R-squared:  0.1025 \nF-statistic: 43.19 on 6 and 2211 DF,  p-value: &lt; 2.2e-16\n\n\nWith three more variables, we explain a larger proportion of variance. Before, adjusted \\(R^2\\) was around 1%, now it is .10 or 10%. So adding these situational variables seem to explain a larger chunk of variance. All the predictors apart from locus of control are statistically significant. Almost all the significant predictors are negative, suggesting lower unethical behaviour intention, apart from Machiavellianism. This makes sense as it means higher in a personality trait for unempathic behaviour is associated with higher intention for unethical behaviour.\n\n3.3.4.1 Standardised coefficients\nAt this point with a mix of predictors, we can also check the standardised coefficients to see which predictors appear to be the strongest on a standard deviation scale.\n\nstandardize_parameters(individual_model2)\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n(Intercept)\n0.1659965\n0.95\n0.0871282\n0.2448649\n\n\nQWEhigh\n-0.1082689\n0.95\n-0.1871863\n-0.0293515\n\n\nMIethical\n-0.1230192\n0.95\n-0.2020337\n-0.0440047\n\n\nPIethical\n-0.1036139\n0.95\n-0.1826004\n-0.0246273\n\n\nLOCTot\n0.0296680\n0.95\n-0.0124626\n0.0717985\n\n\nSocDesTot\n-0.0989384\n0.95\n-0.1423766\n-0.0555001\n\n\nMachTot\n0.2421870\n0.95\n0.1977892\n0.2865848\n\n\n\n\n\n\nMachiavellianism seems to be the strongest predictor here, showing for every 1 standard deviation increase in Machiavellianism, we expect unethical behaviour intentions to increase by 0.24 standard deviations.\n\n3.3.4.2 Checking assumptions\nAt this point, we can check whether there are any red flags by checking the assumption plots. Remember, in multiple linear regression, we want:\n\nThe outcome is interval/ratio level data.\nThe predictor variables are interval/ratio or categorical (with two levels at a time).\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\nThe predictors have non-zero variance.\nThe relationship between the outcome and predictors is linear.\nThe residuals should be normally distributed.\nThere should be homoscedasticity.\n\n\n# Change the panel layout to 2 x 2\npar(mfrow=c(2,2))\n\n# plot the diagnostic plots\nplot(individual_model2)\n\n\n\n\n\n\n\nThere do not appear to be any major red flags (apart from the outcome looks discrete which we will get onto). Plot 1 shows homoscedasticity as the residuals are roughly consistent across the x-axis. The qq plot in plot 2 is fine, just with values at the lower and upper scale moving away from the line. Combined with plots 1 and 3, there are characteristic signs that the outcome may not be truly interval. The residuals are organised into lines as remember our outcome is the sum of a Likert question, so there are only so many possible values. We will come back to this idea in chapter 5. Finally, plot 4 is not flagging any observations as high in leverage for potential outliers.\nSo far, this is the same as chapters 1 and 2. We have a new assumption to check when we have multiple predictors though. We do not want predictors to be too heavily correlated with each other, known as multicollinearity. We can check this using a function from car called vif().\n\nvif(individual_model2)\n\n      QWE        MI        PI    LOCTot SocDesTot   MachTot \n 1.000447  1.002926  1.002201  1.140099  1.211972  1.266108 \n\n\nVIF stands for variance inflation factor and a conservative estimate is values that are less than 2.5 are fine. Values above 2.5 suggest your predictors are too heavily correlated. Some people suggest less than 10 is OK, but siding with caution is normally a good approach. Here, the highest value is 1.27, so it does not look like there is anything to worry about.\nLike Chapters 1 and 2, we can also use the check_model() function from performance to get some extra plots we do not get in the base R version\n\ncheck_model(individual_model2)\n\n\n\n\n\n\n\nThe posterior predictive check reinforces our suspicions about the discrete looking data as the model happily predicts normal looking estimates, but the observed data have notable peaks where certain values are more common. We also get a plot for collinearity which does not suggest we have anything to worry about.\n\n3.3.5 Model comparison\nNow we have two competing models, we can see whether model 2 is the better fitting model and is worth adding three additional predictors in. We are trying to avoid overfitting as \\(R^2\\) will almost always increase with more variables, but it might not be actually doing anything worthwhile.\n\n3.3.5.1 Comparing models using anova()\n\nFirst, we can compare the two models using analysis of variance (ANOVA) to see whether model 2 explains significantly more variance than model 1.\n\nanova(individual_model1, \n      individual_model2)\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n2214\n32591.48\nNA\nNA\nNA\nNA\n\n\n2211\n29487.44\n3\n3104.04\n77.5814\n0\n\n\n\n\n\n\nIt does seem to be the case here. Careful of the rounding as the p-value is tiny and its rounded to 0 in the rendering of the book. The difference between the models is statistically significant, suggesting model 2 explains more variance than model 1.\n\n3.3.5.2 Comparing R2 for each model\nSecond, we can compare \\(R^2\\) for each model to see if model 2 explains more variance in our outcome than model 1. This is more of a prompt to look at your two model objects and compare the values.\nRemember, adjusted \\(R^2\\) is particularly useful here as it corrects for model complexity. It penalises for the number of predictors in your model, so there will be a bigger difference in \\(R^2\\) and adjusted \\(R^2\\) when the model is unnecessarily complex. This also supports our case as adjusted \\(R^2\\) increases from approximately .01 to .10.\n\n3.3.5.3 Checking model fit using AIC\nFinally, we can check the Akaike Information Criterion (AIC) as a measure of model fit. It calculates prediction error in the model and it can only be interpreted as a relative value. There is not an inherent good or bad threshold, its relative to the data and models you are working with.\nAs it represents prediction error, lower AIC values mean better model fit relative to another model. If a more complicated model has a higher AIC than a less complicated model, there is potential overfitting when you have redundant predictors, so its better to retain the less complex model.\n\nAIC(individual_model1)\n\n[1] 12265.16\n\nAIC(individual_model2)\n\n[1] 12049.17\n\n\nThe AIC value decreases in model 2, suggesting that although we have a more complex model, the additional predictors are not redundant, there is less prediction error and we are learning something useful by including these extra predictors.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "03-lm-multiple.html#multiple-linear-regression-with-interactions",
    "href": "03-lm-multiple.html#multiple-linear-regression-with-interactions",
    "title": "\n3  Multiple Linear Regression\n",
    "section": "\n3.4 Multiple linear regression with interactions",
    "text": "3.4 Multiple linear regression with interactions\nFinally, we will explore how you can express different types of interactions in multiple linear regression models.\n\n3.4.1 Coding schemes for interactions\nAt the start of this chapter and previously in Chapter 1, we explored different coding schemes for predictors. For continuous predictors, we can enter variables as raw or centered values. For categorical predictors, we can enter variables as dummy or deviation coded values.\nWhen you have multiple linear regression models, changing the coding scheme does little else but helps you interpret the intercept. However, once you start adding interaction terms, it is super important to know the difference between what the coefficients mean when you center the predictors or not.\nIn preparation for later, lets deviation code the categorical variables and center the continuous variables.\n\nevans_data &lt;- evans_data %&gt;% \n  # Start by deviation coding the categorical variables as 0.5 and -0.5\n  mutate(QWE_deviation = case_when(QWE == \"high\" ~ 0.5, \n                                   .default = -0.5),\n         MI_deviation = case_when(MI == \"ethical\" ~ 0.5, \n                                  .default = -0.5),\n         PI_deviation = case_when(PI == \"ethical\" ~ 0.5, \n                                  .default = -0.5),\n         # Then center the continuous predictors by subtracting the mean\n         LOCTot_center = LOCTot - mean(LOCTot),\n         SocDesTot_center = SocDesTot - mean(SocDesTot),\n         MachTot_center = MachTot - mean(MachTot))\n\n\n3.4.2 Categorical by categorical interactions\nWe will demonstrate these principles by focusing on two predictors at a time, but note these concepts just extend everything you learnt so far. You could have just two interacting predictors, or you could have a mix of individual and interacting predictors.\nFirst, we will explore categorical by categorical interactions. This looks at the moderating effect of one predictor on the difference in the outcome between levels of another predictor.\n\n3.4.2.1 Dummy coded predictors\nFor a starting point, let’s look at what the model looks like for our original dummy coded predictors. Previously, we separated predictors with +, but to get the interaction between predictors, we use * instead.\n\ninteraction_model1_dummy &lt;- lm(BehIntTot ~ MI * PI, \n                               data = evans_data)\n\nsummary(interaction_model1_dummy)\n\n\nCall:\nlm(formula = BehIntTot ~ MI * PI, data = evans_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4693 -2.4693 -0.4841  3.1418 10.2177 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          10.4693     0.1632  64.156  &lt; 2e-16 ***\nMIethical            -0.6870     0.2297  -2.992  0.00281 ** \nPIethical            -0.6112     0.2298  -2.660  0.00787 ** \nMIethical:PIethical   0.3130     0.3263   0.959  0.33759    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.841 on 2214 degrees of freedom\nMultiple R-squared:  0.008527,  Adjusted R-squared:  0.007184 \nF-statistic: 6.347 on 3 and 2214 DF,  p-value: 0.0002781\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are different symbols you can use for model formulas. Using * is shorthand for “give me the individual and interaction effects of these variables”. You could also specify it fully by using predictor1 + predictor 2 + predictor1:predictor2. You can find a full list of formula syntax in places like this Econometrics blog {target=“_blank”}.\n\n\nManager influence and peer influence are both significant, but the interaction between them is not statistically significant.\n\n3.4.2.2 Deviation coded predictors\nCompare that to the same model, but using the deviation coded predictors instead. Look at the coefficients for the individual predictors and the interaction. Which are different and which are the same between the models?\n\ninteraction_model1_deviation &lt;- lm(BehIntTot ~ MI_deviation * PI_deviation, \n                                   data = evans_data)\n\nsummary(interaction_model1_deviation)\n\n\nCall:\nlm(formula = BehIntTot ~ MI_deviation * PI_deviation, data = evans_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4693 -2.4693 -0.4841  3.1418 10.2177 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                9.89847    0.08158 121.341  &lt; 2e-16 ***\nMI_deviation              -0.53053    0.16315  -3.252  0.00116 ** \nPI_deviation              -0.45467    0.16315  -2.787  0.00537 ** \nMI_deviation:PI_deviation  0.31297    0.32630   0.959  0.33759    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.841 on 2214 degrees of freedom\nMultiple R-squared:  0.008527,  Adjusted R-squared:  0.007184 \nF-statistic: 6.347 on 3 and 2214 DF,  p-value: 0.0002781\n\n\nYou will see that the model fit and coefficient for the interaction is identical, but the individual predictor coefficients changed. This happens because of the interpretation of the intercept and coefficient in dummy coding. When you dummy code these variables, the intercept is the value for the outcome when both manager influence is unethical and peer influence is unethical. The coefficient for manager influence is then how we expect the outcome to change for ethical managers, when there is an unethical peer influence. These are simple effects, which can be useful information, but it might not be what you are expecting.\nIn the deviation model, this turns into something more familiar to those working with ANOVA to compare categorical variables. The intercept is the grand mean now, so the individual predictors become main effects, or the effect of one predictor, when the second predictor is held constant at 0. For instance, an ethical manager decreases the unethical behaviour intention by 1.06 (2 * -0.53 because of the -0.5/0.5 deviation coding scheme) compared to the grand mean.\nBoth approaches provide useful information, but think about what information you are looking for. Typically, main effects are more useful when you want to know the individual effect of predictors on the outcome. Simple effects are then normally more useful when you are trying to break down each element of an interaction and you want the conditional effect of one predictor on another.\nIts always about your research question and what information you want to know.\n\n3.4.2.3 Plotting interactions\nIn these examples, the data set is great for a large sample of different variables, but there are not really any meaningful interactions to explore. But, we can still demonstrate what you would do if there was an interaction to explore.\nFirst, plotting is your friend. Sometimes you can plot your design fine using ggplot2, but you often miss out on the inferential parts for the confidence intervals around estimates, particularly when there are interactions to explore. So, the sjPlot package can be useful as it can take regression models and plot the model predictions. To decompose an interaction, it is normally more useful to use the original / dummy-coded variables, as we are interested in the simple effects here, and the units will be more interpretable for the average reader.\n\n# sjPlot makes it easy to plot estimates from the model\nplot_model(interaction_model1_dummy, # You enter the model object\n           type = \"pred\", # The type of plot you want to create\n           terms = c(\"MI\", \"PI\")) # The predictors you want to plot \n\n\n\n\n\n\n\nIt is up to you and your research question which way you plot the interaction. You can explore the moderating effect of predictor 1 on predictor 2, or the moderating effect of predictor 2 on predictor 1. You will need to consider which addresses your research question. For example, we can flip it around:\n\n# sjPlot makes it easy to plot estimates from the model\nplot_model(interaction_model1_dummy, # You enter the model object\n           type = \"pred\", # The type of plot you want to create\n           terms = c(\"PI\", \"MI\")) # The predictors you want to plot \n\n\n\n\n\n\n\nsjPlot is built on ggplot2, so the output may not be publication quality immediately, but you can edit some elements like a regular ggplot2 object. Annoyingly, its not easy to edit the x-axis and legend labels, so its normally better to edit the underlying variable name and labels instead.\n\nplot_model(interaction_model1_dummy,\n           type = \"pred\", \n           terms = c(\"MI\", \"PI\")) + \n  scale_y_continuous(breaks = seq(8, 14, 2), \n                     limits = c(8, 14), \n                     name = \"Unethical Workplace Behaviour Intention\") + \n  theme_classic() + \n  labs(title = \"\")\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.\n\n\n\n\n\n\n\n\n\n3.4.2.4 Breaking down model estimates\nAnother way of breaking down interactions is by reporting the estimated marginal means. When there is an interaction, you often want to known whether the mean difference in predictor 1 is different across the levels of predictor 2. Plotting shows you this visually, but you might want to report effect sizes in your write-up.\nThe emmeans package can take a model object and report the estimated marginal means providing you tell it which variables you want to break it down by. Remember, you can switch the order of the variables depending on which predictor you want as the moderator.\nFor example, we can report the estimated marginal means and their confidence intervals of all four groups.\n\nemmeans(interaction_model1_dummy, # model object\n        ~ MI | PI) # which order you want predictors\n\nPI = unethical:\n MI        emmean    SE   df lower.CL upper.CL\n unethical  10.47 0.163 2214    10.15    10.79\n ethical     9.78 0.162 2214     9.47    10.10\n\nPI = ethical:\n MI        emmean    SE   df lower.CL upper.CL\n unethical   9.86 0.162 2214     9.54    10.18\n ethical     9.48 0.166 2214     9.16     9.81\n\nConfidence level used: 0.95 \n\n\nWe can also report the mean difference moderated by the second predictor by wrapping it in the contrast() function.\n\ncontrast(emmeans(interaction_model1_dummy, \n                 ~ MI | PI))\n\nPI = unethical:\n contrast         estimate    SE   df t.ratio p.value\n unethical effect    0.344 0.115 2214   2.992  0.0028\n ethical effect     -0.344 0.115 2214  -2.992  0.0028\n\nPI = ethical:\n contrast         estimate    SE   df t.ratio p.value\n unethical effect    0.187 0.116 2214   1.614  0.1067\n ethical effect     -0.187 0.116 2214  -1.614  0.1067\n\nP value adjustment: fdr method for 2 tests \n\n\nNote you get all combinations of these comparisons, so when you have two levels in a predictor, you get the same contrast but flipped around for each level of the moderator.\n\n\n\n\n\n\nImportant\n\n\n\nOne final warning, you can look at these variables in many ways, but think about your plan and what will address your research question and hypothesis.\n\n\n\n3.4.3 Continuous by categorical interactions\nSecond, we have categorical by continuous interactions. This is where you have one categorical predictor and one continuous predictor. It tells you whether the relationship between your continuous predictor and your outcome is moderated by your categorical predictor.\nFor this model, we will focus on whether workplace quality (QWE_deviation) moderates the relationship between Machiavellianism (MachTot_center) and unethical behaviour intentions (BehIntTot). Now you know the difference between raw and centered predictors, make sure you use the centered variables to interpret the individual predictors as main effects.\n\ninteraction_model2_deviation &lt;- lm(BehIntTot ~ QWE_deviation * MachTot_center, \n                                   data = evans_data)\n\nsummary(interaction_model2_deviation)\n\n\nCall:\nlm(formula = BehIntTot ~ QWE_deviation * MachTot_center, data = evans_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.0748 -2.7977 -0.5381  2.9165 11.5690 \n\nCoefficients:\n                             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   9.90061    0.07817 126.647  &lt; 2e-16 ***\nQWE_deviation                -0.41162    0.15635  -2.633  0.00853 ** \nMachTot_center                0.22810    0.01591  14.341  &lt; 2e-16 ***\nQWE_deviation:MachTot_center -0.04788    0.03181  -1.505  0.13246    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.681 on 2214 degrees of freedom\nMultiple R-squared:  0.0892,    Adjusted R-squared:  0.08796 \nF-statistic: 72.28 on 3 and 2214 DF,  p-value: &lt; 2.2e-16\n\n\nThere is no interaction again, but if we did want to explore it, we can plot the simple effects by creating a dummy-coded model.\n\ninteraction_model2_dummy &lt;- lm(BehIntTot ~ QWE * MachTot, \n                               data = evans_data)\n\nplot_model(interaction_model2_dummy, \n           type = \"pred\", \n           terms = c(\"MachTot\", \"QWE\"))\n\n\n\n\n\n\n\nIn a continuous by categorical interaction, it will tell you whether the slope between your outcome and continuous predictor is different/moderated by the levels of your categorical predictor. For example, whether the relationship flips from positive to negative, one is very weak, or the gradient of the slopes are just different. Here, we can see the relationship is slightly weaker in high quality workplaces compared to low quality, but ultimately that difference is not statistically significant as shown by the interaction term in the model.\n\n3.4.4 Continuous by continuous interactions\nFinally, we have continuous by continuous interactions. This is where you have two continuous predictors. It tells you whether the relationship between your first continuous predictor and your outcome is moderated by your second continuous predictor.\nFor this model, we will focus on whether locus of control (LOCTot_center) moderates the relationship between Machiavellianism (MachTot_center) and unethical behaviour intentions (BehIntTot). Make sure you use the centered variables to interpret the individual predictors as main effects.\n\ninteraction_model3_deviation &lt;- lm(BehIntTot ~ LOCTot_center * MachTot_center, \n                                   data = evans_data)\n\nsummary(interaction_model3_deviation)\n\n\nCall:\nlm(formula = BehIntTot ~ LOCTot_center * MachTot_center, data = evans_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-8.0970 -2.7347 -0.5072  2.8787 12.0243 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   9.9073451  0.0810659 122.213   &lt;2e-16 ***\nLOCTot_center                 0.0357878  0.0168673   2.122    0.034 *  \nMachTot_center                0.2172115  0.0168029  12.927   &lt;2e-16 ***\nLOCTot_center:MachTot_center -0.0007797  0.0027161  -0.287    0.774    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.685 on 2214 degrees of freedom\nMultiple R-squared:  0.08733,   Adjusted R-squared:  0.0861 \nF-statistic: 70.62 on 3 and 2214 DF,  p-value: &lt; 2.2e-16\n\n\nPlotting is always helpful for interpreting interactions, but it is crucial for continuous by continuous interactions.\nThe interpretation relies on simple slopes analysis. As there are not discrete values of your moderator, simple slopes works by creating discrete values from your continuous predictor. Typically, this is 1 standard deviation below the mean, the mean, and 1 standard deviation above the mean. This tells you as you increase levels of your moderator, how does it affect the relationship between your first predictor and the outcome?\n\ninteraction_model3_dummy &lt;- lm(BehIntTot ~ LOCTot * MachTot, \n                               data = evans_data)\n\nplot_model(interaction_model3_dummy, \n           type = \"pred\", \n           terms = c(\"MachTot\", \"LOCTot\"))\n\n\n\n\n\n\n\nIn a simple slopes analysis, it will tell you whether the slope between your outcome and continuous predictor is different/moderated by the fixed points of your second predictor. For example, whether the relationship flips from positive to negative, one is very weak, or the gradient of the slopes are just different. Here, the interaction was non-significant, and there is very little difference between the three lines, they are almost exactly parallel to each other.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "03-lm-multiple.html#test-yourself",
    "href": "03-lm-multiple.html#test-yourself",
    "title": "\n3  Multiple Linear Regression\n",
    "section": "\n3.5 Test Yourself",
    "text": "3.5 Test Yourself\nNow you have followed along to a guided example, it is important you can transfer your knowledge to a new scenario. So, we have a new data set for you to try out your understanding of these techniques on. Follow the instructions below and answer the questions. The solution is there to check your answers against, but make sure you try the activities independently first.\n\n3.5.1 Introduction to the dataset\nWe will use data from Tulloch et al. who performed an unpublished replication of Troy et al. (2017). Troy et al. conducted three studies exploring the effect of emotion regulation and socioeconomic status on depressive symptoms. The data you will use comes from a dissertation project where the team replicated study one to investigate if socioeconomic status moderates the link between cognitive reappraisal ability and depressive symptoms. For the purposes of this task, the key variables from the study included:\n\nSocioeconomic Status (SES; SES) - Current annual family income was rated on a 1 to 12 scale aligning with income bands. The bands were converted from the original study’s values in US dollars, to the replication study’s target population of UK (in British Pounds) and Malaysia (in Malaysian Ringgit). Its an odd way of recording this data, but go with it…\nCountry (Country) - Categorical variable showing whether the participant completed the study in Malaysia or the UK.\nCognitive Reappraisal Ability (CRA; CRA_mean) - A scale measuring cognitive reappraisal ability. The variable was calculated from the mean of eight items measured on a 1 (strongly disagree) - 7 (strongly agree) scale, with higher values meaning higher cognitive reappraisal ability.\nDepressive Symptoms (DS_sum) - This was measured as current non-clinical symptoms of depression from the Center for Epidemiological Studies Depression Scale. Participants reported the presence of depression symptoms in the past week across five items on a 0 (none of the time) to 3 (all of the time) scale. The variable was calculated from the sum of the five items, meaning the scale could range from 0 to 15, with higher values meaning greater depression symptoms in the past week.\nLife Stress (LS_mean) - This was measured as perceptions of life stress in the past two years. Participants completed four questions on a 1 (never) to 5 (very often) scale. The variable was calculated from the mean of four questions, with higher values meaning greater life stress in the past two years.\n\nIn the original Troy et al. study, they investigated if socioeconomic status moderates the link between cognitive reappraisal ability and depressive symptoms. In study one, they applied a multiple regression model with depressive symptoms as the outcome, life stress as an individual predictor, and an interaction between cognitive reappraisal ability and socioeconomic status. They found that:\n\nLife stress was a significant positive predictor of depressive symptoms.\nCognitive reappraisal was a significant negative predictor of depressive symptoms.\nThere was a significant interaction between cognitive reappraisal and socioeconomic status. They found that there was a stronger negative relationship between cognitive reappraisal and depressive symptoms for lower values of socioeconomic status compared to higher values (i.e., simple slopes analysis).\n\nIn the replication study, the team wanted to replicate the study to see if they would observe similar findings by including two different countries of origin (Malaysia and UK) compared to the original sample from the United States. The research questions were:\n\nDoes country of origin, life stress, cognitive reappraisal ability, and socioeconomic status predict depressive symptoms?\nDoes socioeconomic status moderate the relationship between cognitive reappraisal ability and depressive symptoms?\n\n\n# Load data for coding schemes\n# We are applying the coding scheme, just so its crystal clear whether your answers align\ntulloch_data &lt;- read_csv(\"data/Tulloch_2022.csv\") %&gt;% \n  mutate(Country = factor(Country, \n                          levels = c(\"Malaysia\", \"UK\")),\n         Country_deviation = case_when(Country == \"UK\" ~ 0.5, \n                                       .default = -0.5),\n         SES_center = SES - mean(SES), \n         CRA_center = CRA_mean - mean(CRA_mean),\n         LS_center = LS_mean - mean(LS_mean))\n\n\n\n\n\n\n\nTry this\n\n\n\nFit two models:\n\nPredict depressive symptoms (DS_sum) from country of origin (Country_deviation), life stress (LS_center), cognitive reappraisal (CRA_center), and socioeconomic status (SES_center) as individual predictors.\nPredict depressive symptoms (DS_sum) from country of origin (Country_deviation), life stress (LS_center), and the interaction between cognitive reappraisal (CRA_center) and socioeconomic status (SES_center).\n\nFrom here, apply what you learnt in the guided examples above to this new independent task and complete the questions below to check your understanding.\n\n\n\n3.5.2 Model 1: Individual predictors\n\nRounding to three decimals, adjusted \\(R^2\\) for the model is , meaning the predictors explain % of the variance in our outcome of depressive symptoms.\nRounding to two decimals, the slope for Country_deviation is , suggesting depressive symptoms were higher in \nthe UK\nMalaysia.\nRounding to two decimals, the slope for LS_center is , suggesting life stress is a \npositive\nnegative predictor.\nRounding to two decimals, the slope for CRA_center is , suggesting cognitive reappraisal is a \npositive\nnegative predictor.\nRounding to two decimals, the slope for SES_center is , suggesting socioeconomic status is a \npositive\nnegative predictor.\nConverting the model to standardised predictors, \ncountry of origin\nlife stress\ncognitive reappraisal\nsocioeconomic status has the largest effect on depressive symptoms.\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\ntulloch_model1 &lt;- lm(DS_sum ~ Country_deviation + LS_center + CRA_center + SES_center,\n                     data = tulloch_data) \n\nsummary(tulloch_model1)\n\n\nCall:\nlm(formula = DS_sum ~ Country_deviation + LS_center + CRA_center + \n    SES_center, data = tulloch_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-7.301 -2.069 -0.484  2.064  8.623 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        4.57426    0.22915  19.962  &lt; 2e-16 ***\nCountry_deviation  1.32867    0.56926   2.334  0.02070 *  \nLS_center          1.69695    0.31853   5.327 2.95e-07 ***\nCRA_center        -0.48039    0.18290  -2.626  0.00937 ** \nSES_center        -0.21012    0.09833  -2.137  0.03396 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.996 on 180 degrees of freedom\nMultiple R-squared:  0.2557,    Adjusted R-squared:  0.2391 \nF-statistic: 15.46 on 4 and 180 DF,  p-value: 6.901e-11\n\n\n\nstandardize_parameters(tulloch_model1)\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n(Intercept)\n0.0000000\n0.95\n-0.1265450\n0.1265450\n\n\nCountry_deviation\n0.1890952\n0.95\n0.0292300\n0.3489603\n\n\nLS_center\n0.3628109\n0.95\n0.2284307\n0.4971912\n\n\nCRA_center\n-0.1769852\n0.95\n-0.3099507\n-0.0440197\n\n\nSES_center\n-0.1724731\n0.95\n-0.3317415\n-0.0132047\n\n\n\n\n\n\n\n\n\n\n3.5.3 Model 2: Interaction term\n\nRounding to three decimals, adjusted \\(R^2\\) for the model is , meaning the predictors explain % of the variance in our outcome of depressive symptoms.\nRounding to two decimals, the slope for Country_deviation is , suggesting depressive symptoms were higher in \nthe UK\nMalaysia.\nRounding to two decimals, the slope for LS_center is , suggesting life stress is a \npositive\nnegative predictor.\nRounding to two decimals, the slope for CRA_center is, suggesting cognitive reappraisal is a \npositive\nnegative predictor.\nRounding to two decimals, the slope for SES_center is , suggesting socioeconomic status is a \npositive\nnegative predictor.\nRounding to two decimals, the slope for CRA_center:SES_center is , and it is a \nsignificant\nnon-significant predictor.\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\ntulloch_model2 &lt;- lm(DS_sum ~ Country_deviation + LS_center + CRA_center*SES_center,\n                     data = tulloch_data) \n\nsummary(tulloch_model2)\n\n\nCall:\nlm(formula = DS_sum ~ Country_deviation + LS_center + CRA_center * \n    SES_center, data = tulloch_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.2692 -2.0546 -0.4714  1.9568  8.6498 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            4.57570    0.22980  19.911  &lt; 2e-16 ***\nCountry_deviation      1.31934    0.57172   2.308  0.02216 *  \nLS_center              1.69024    0.32026   5.278 3.75e-07 ***\nCRA_center            -0.48954    0.18632  -2.627  0.00935 ** \nSES_center            -0.21228    0.09889  -2.147  0.03318 *  \nCRA_center:SES_center  0.01761    0.06357   0.277  0.78208    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.004 on 179 degrees of freedom\nMultiple R-squared:  0.256, Adjusted R-squared:  0.2352 \nF-statistic: 12.32 on 5 and 179 DF,  p-value: 2.832e-10\n\n\n\nstandardize_parameters(tulloch_model2)\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n(Intercept)\n0.0007205\n0.95\n-0.1262589\n0.1276998\n\n\nCountry_deviation\n0.1877666\n0.95\n0.0272046\n0.3483285\n\n\nLS_center\n0.3613759\n0.95\n0.2262574\n0.4964944\n\n\nCRA_center\n-0.1803568\n0.95\n-0.3158157\n-0.0448979\n\n\nSES_center\n-0.1742427\n0.95\n-0.3344240\n-0.0140615\n\n\nCRA_center:SES_center\n0.0182935\n0.95\n-0.1120138\n0.1486008\n\n\n\n\n\n\n\n\n\n\n3.5.4 Model comparison\n\nComparing the two models using ANOVA, model 2 \ndoes\ndoes not explain significantly more variance in depressive symptoms than model 1.\nAdjusted \\(R^2\\) is larger in \nmodel 1\nmodel 2.\nThe AIC value is lower in \nmodel 1\nmodel 2.\nLooking at these model comparison criteria, \nmodel 1\nmodel 2 seems to be the most appropriate model.\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nanova(tulloch_model1, tulloch_model2)\n\n\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n180\n1616.120\nNA\nNA\nNA\nNA\n\n\n179\n1615.428\n1\n0.6925931\n0.0767439\n0.7820792\n\n\n\n\n\n\n\nAIC(tulloch_model1)\n\n[1] 937.9814\n\nAIC(tulloch_model2)\n\n[1] 939.9021",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "03-lm-multiple.html#words-from-this-chapter",
    "href": "03-lm-multiple.html#words-from-this-chapter",
    "title": "\n3  Multiple Linear Regression\n",
    "section": "\n3.6 Words from this Chapter",
    "text": "3.6 Words from this Chapter\nBelow you will find a list of words that were used in this chapter that might be new to you in case it helps to have somewhere to refer back to what they mean. The links in this table take you to the entry for the words in the PsyTeachR Glossary. Note that the Glossary is written by numerous members of the team and as such may use slightly different terminology from that shown in the chapter.\n\n\n\n\n\n\n\nEvans, T. R. (2024). Unethical Behaviour in the Workplace: A Direct and Conceptual Replication of Jones & Kavanagh (1996). https://doi.org/10.31234/osf.io/a2rj9\n\n\nJames, E. L., Bonsall, M. B., Hoppitt, L., Tunbridge, E. M., Geddes, J. R., Milton, A. L., & Holmes, E. A. (2015). Computer Game Play Reduces Intrusive Memories of Experimental Trauma via Reconsolidation-Update Mechanisms: Psychological Science, 26(8), 1201–1215. https://doi.org/10.1177/0956797615583071\n\n\nJones, G. E., & Kavanagh, M. J. (1996). An experimental examination of the effects of individual and situational factors on unethical behavioral intentions in the workplace. Journal of Business Ethics, 15, 511–523.\n\n\nTroy, A. S., Ford, B. Q., McRae, K., Zarolia, P., & Mauss, I. (2017). Change the things you can: Emotion regulation is more beneficial for people from lower than from higher socioeconomic status. Emotion, 17(1), 141–154. https://doi.org/10.1037/emo0000210",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-glm.html",
    "href": "05-glm.html",
    "title": "\n5  Generalised Linear Regression\n",
    "section": "",
    "text": "5.1 Chapter preparation\nIn this chapter, we need a few different datasets, so we will introduce them as we need them.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalised Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-glm.html#chapter-preparation",
    "href": "05-glm.html#chapter-preparation",
    "title": "\n5  Generalised Linear Regression\n",
    "section": "",
    "text": "5.1.1 Organising your files and project for the chapter\nBefore we can get started, you need to organise your files and project for the chapter, so your working directory is in order.\n\nIn your folder for statistics and research design Stats_Research_Design, create a new folder called 05_regression_generalised. Within 05_regression_generalised, create two new folders called data and figures.\nCreate an R Project for 05_regression_generalised as an existing directory for your chapter folder. This should now be your working directory.\nCreate a new Quarto document and give it a sensible title describing the chapter, such as 05 Generalised Linear Regression. Save the file in your 05_regression_generalised folder.\nWe are working with a few data sets in this chapter, so please save the following zip file containing all the data files you need: Chapter 05 Data. Right click the link and select “save link as”, or clicking the link will save the file to your Downloads. Extract the files and make sure that you save them as “.csv”. Save or copy the files to your data/ folder within 05_regression_generalised.\n\nYou are now ready to start working on the chapter!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalised Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-glm.html#logistic-regression",
    "href": "05-glm.html#logistic-regression",
    "title": "\n5  Generalised Linear Regression\n",
    "section": "\n5.2 Logistic Regression",
    "text": "5.2 Logistic Regression\n\n5.2.1 Introduction to the dataset\nFor the guided examples, we have two datasets, but we will introduce them in turn. To demonstrate logistic regression, we will use data from Bostyn et al. (2018).\nBostyn et al. studied the classic trolley-dilemma in moral philosophy. In the typical format, a trolley (streetcar/tram) is travelling along tracks and will run over five people. However, you have the choice to pull a lever and divert the trolley to run over just one person. Although this would save lives, you must manually intervene to essentially kill one person. This is typically always a hypothetical scenario as it’s difficult to ask an ethics board for permission to run over people. In this study, Bostyn et al. compared a hypothetical version of the trolley-dilemma with a (seemingly) realistic version. Participants chose between not intervening and allowing five mice in a cage to be shocked with electricity or they could intervene and one mouse would be shocked instead. Do not worry, the mice were never shocked but participants did not know it at the time.\nTheir research question was: would there be a difference in moral decision-making when participants completed a hypothetical compared to a real version of the trolley-dilemma task? There was seemingly no hypothesis, so we will just focus on addressing the research question.\nThere is one independent variable (STUDY) to randomly allocate participants into one of two groups:\n\nThe “real” group (group 1 in the data) attended a lab session where they saw two cages of real mice. They had 20 seconds to decide whether to press a button and divert the electric shock to the cage with one mouse.\nThe hypothetical group (group 2 in the data) completed a more traditional version of the study where they just responded on a computer for whether they would intervene or not after reading a description of allowing five mice to be shocked or choosing for one mouse to be shocked.\n\nParticipants completed a bunch of additional measures on moral judgement, empathy, psychopathy etc., but our main outcome of interest is whether they chose to intervene (1) or not (0) from the variable DECISION. They label these two decisions as deontological when people intervened and consequentialist when people did not intervene. For plotting, we will use the labels, but for the analyses we will use the 0s and 1s.\nWe first need to load some packages and the data for this task. If you do not have any of the packages, make sure you install them first.\nAs the first activity, try and test yourself by completing the following task list to practice your data wrangling skills. Create a final object called bostyn_data to be consistent with the tasks below. If you just want to focus on fitting the models, then you can just copy the code in the solution.\n\n\n\n\n\n\nTry this\n\n\n\nTo wrangle the data, complete the following tasks:\n\n\nLoad the following packages. If you do not have them already, remember you will need to install them first if you are working on your own computer:\n\ntidyverse\nemmeans\nordinal\nsjPlot\n\n\nYou can complete the following tasks individually or combining things in one pipe, you just need a final object called bostyn_data. Read the data file data/bostyn_2018.csv.\nThere are loads of variables in the data which you can explore in your own time, but for this chapter, just select the first four variables from STUDY to DECISION.\nThere are a few missing values in DECISION, so remove the NAs at this point.\n\nTo help with plotting and modelling later, create two new variables:\n\nCreate a new variable called study where you create a factor to recode the values from STUDY. 1 should be relabelled “Real” and 2 should be relabelled as “Hypothetical”.\nCreate a new variable called label where you create a factor to recode the values from DECISION. 0 should be relabelled “DEO” for deontological and 1 should be relabelled as “CON” for consequentialist.\n\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nYou should have the following in a code chunk:\n\n# wrangling and visualisation functions \nlibrary(tidyverse)\n# Parameter estimates\nlibrary(emmeans)\n# Ordinal regression for later\nlibrary(ordinal)\n# plotting model objects\nlibrary(sjPlot)\n\n# Bostyn data for logistic regression\nbostyn_data &lt;-read_csv(\"data/Bostyn_2018.csv\") %&gt;% \n  select(STUDY:DECISION) %&gt;%  # just select first four variables\n  drop_na(DECISION) %&gt;% \n  mutate(study = factor(STUDY, \n                        levels = c(1, 2), \n                        labels = c(\"Real\", \"Hypothetical\")),\n         label = factor(DECISION, \n                           levels = c(0, 1), \n                           labels = c(\"DEO\", \"CON\")))\n\n\n\n\n\n5.2.2 Exploratory data analysis\nWhen starting any data analysis, it is important to visualise the data for some exploratory data analysis. Using the skills you developed in data skills for reproducible research, you can explore the data to understand its properties and look for potential patterns. In this scenario, a bar plot might be handy as we have a dichotomous outcome, so we can explore the frequency of responses across each group.\n\nbostyn_data %&gt;% \n  ggplot(aes(x = study, fill = label)) + \n  geom_bar(position = \"dodge\") + \n  scale_x_discrete(name = \"Study\") + \n  scale_y_continuous(name = \"Frequency\", \n                     breaks = seq(0, 200, 50), \n                     limits = c(0,200)) + \n  scale_fill_viridis_d(name = \"Decision\", option = \"E\") +\n  theme_minimal()\n\n\n\n\n\n\n\nIn the real group, there is a bigger difference in the number of people choosing to intervene and make a consequentialist response (1s in the data) compared to the hypothetical group where there is a smaller difference. The number of people choosing not to intervene and make a deontological response (0s in the data) is smaller in both groups, but it’s the proportional difference which changes.\nInstead of counts, we can also express binary 0 and 1 data as a proportion or probability of intervening. If you take the mean of binary data, it tells you the proportion, so we can calculate the proportion of participant’s choosing to intervene as a consequentialist decision.\n\nbostyn_data %&gt;% \n  group_by(study) %&gt;% \n  summarise(proportion_con = mean(DECISION))\n\n\n\n\nstudy\nproportion_con\n\n\n\nReal\n0.8645833\n\n\nHypothetical\n0.7469880\n\n\n\n\n\n\nParticipants in the real group (86%) were more likely to intervene than participants in the hypothetical group (75%). These are just descriptive statistics though, so you need to think about modelling to make inferences from these data.\n\n5.2.3 Would linear regression work?\n\n\n\n\n\n\nWarning\n\n\n\nThe most important part of data analysis is your role as the decision maker and thinking about what would be the most appropriate choice of model. Sometimes, there are equally valid approaches, while other times there is one better suited approach. If you apply an inappropriate test, R will gladly follow your instructions. Its your job to think critically about your modelling approach and check its an appropriate choice.\n\n\nWhen we have a dichotomous outcome coded 0 and 1, it has numerical value, so R will happily apply regular linear regression.\n\nbostyn_model1 &lt;- lm(DECISION ~ study, data = bostyn_data)\n\nsummary(bostyn_model1)\n\n\nCall:\nlm(formula = DECISION ~ study, data = bostyn_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8646  0.1354  0.1354  0.1354  0.2530 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.86458    0.02698  32.041   &lt;2e-16 ***\nstudyHypothetical -0.11760    0.04912  -2.394   0.0173 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3739 on 273 degrees of freedom\nMultiple R-squared:  0.02056,   Adjusted R-squared:  0.01698 \nF-statistic: 5.732 on 1 and 273 DF,  p-value: 0.01733\n\n\nIt is somewhat interpretable as when you take the mean of binary data, you get the proportion. So, the intercept represents the proportion of consequentialist responses for those in the real group, and the slope coefficient represents the mean difference in proportion, suggesting there are .12/12% fewer consequentialist responses in the hypothetical group, which is statistically significant at \\(\\alpha\\) = .05.\nAlthough it works and we can make sense of the results, the question is whether it is an informative and appropriate way of modelling a binary outcome. Lets check the assumptions to see if anything looks off.\n\n# Change the panel layout to 2 x 2\npar(mfrow=c(2,2))\n\n# plot the diagnostic plots\nplot(bostyn_model1)\n\n\n\n\n\n\n\nNow we can see the problems. Because we only have two possible outcomes, the assumption checks are all off. The qq plot in panel 2 particularly shows the problem, as there is a giant chunk missing where the values between 0 and 1 should be, at least theoretically if we assume normal residuals.\n\n# plot the qq plot\nplot(bostyn_model1, \n     which = 2)\n\n\n\n\n\n\n\nNow we can apply logistic regression to more appropriately model the dichotomous nature of the outcome.\n\n5.2.4 Logistic regression\n\n5.2.4.1 Generalised linear regression\nIn a linear model, we model the outcome as a product of an intercept, a slope, and error. Its that final error part which is why we have worried about normality so far. In a regular linear regression model, we need the model residuals (the difference between the expected and observed values) to be normally distributed.\nLogistic regression is an adaptation of the linear regression framework we have worked with so far, where we can replace that error component. This is called generalised linear regression and we can specify a link and link function, instead of normal residuals being built into the model.\nSo far, we have always used lm(), but R comes with another function called glm() for generalised linear regression models. To demonstrate how we can set the link family, we can use it to recreate a regular linear regression model, but this time explicitly calling a normal distribution as the link (using its alternate name a Gaussian distribution) and “identity” as the link function.\n\nbostyn_model1_glm &lt;- glm(DECISION ~ study, \n                     family = gaussian(link = \"identity\"), \n                     data = bostyn_data)\n\nsummary(bostyn_model1_glm)\n\n\nCall:\nglm(formula = DECISION ~ study, family = gaussian(link = \"identity\"), \n    data = bostyn_data)\n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.86458    0.02698  32.041   &lt;2e-16 ***\nstudyHypothetical -0.11760    0.04912  -2.394   0.0173 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 0.1398019)\n\n    Null deviance: 38.967  on 274  degrees of freedom\nResidual deviance: 38.166  on 273  degrees of freedom\nAIC: 243.34\n\nNumber of Fisher Scoring iterations: 2\n\n\nThe link specifies the distribution family and the link function is the transformation for the model units. So, for our normal/Gaussian distribution, we have an identity link which essentially means no transformation, just the values we input. A normal distribution is described by a mean and standard deviation, so we just interpret the raw units in the model.\nThe results here are the same to the third decimal to show its an identical process. The only difference to our regular model output is the model fit statistics. Regular linear regression uses ordinal least squares to fit the model, whereas generalised linear regression fits the models using maximum likelihood. Holding everything else constant, this requires greater computing power, so its overkill to use glm() when you just want to assume normal/Gaussian errors, but its important to show the process behind these procedures.\n\n\n\n\n\n\nNote\n\n\n\nIn this chapter, we are focusing on logistic regression and ordinal regression as variations of generalised linear regression. There are more possibilities though, so type ?family into the console to see the different families you could enter for the link and link function.\n\n\n\n5.2.4.2 Logistic regression in glm()\n\nAfter that brief detour, we will now actually fit the logistic regression model using glm(). As we have a dichotomous outcome, one useful distribution is the binomial. This applies when you want to calculate the probability of an dichotomous outcome from a series of successes (1s) and failures (0s). This models that probability nicely as it has a boundary of 0 or 0% (no successes) and 1 or 100% (all successes). Traditionally, we refer to successes and failures, but you can apply it to any binary outcome where one response is labelled 0 and the other response is labelled 1.\nTo apply the linear model element, we then use the “logit” link function as the unit transformation. Probability ranges between 0 and 1, odds range between 0 and infinity, but log odds (the logit bit) range between minus infinity and infinity, meaning we can fit straight lines to it. Let’s see what a logistic regression model looks like.\n\nbostyn_model2 &lt;- glm(DECISION ~ study, \n                     family = binomial(link = \"logit\"), \n                     data = bostyn_data)\n\nsummary(bostyn_model2)\n\n\nCall:\nglm(formula = DECISION ~ study, family = binomial(link = \"logit\"), \n    data = bostyn_data)\n\nCoefficients:\n                  Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)         1.8539     0.2109   8.790   &lt;2e-16 ***\nstudyHypothetical  -0.7713     0.3290  -2.344   0.0191 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 251.53  on 274  degrees of freedom\nResidual deviance: 246.17  on 273  degrees of freedom\nAIC: 250.17\n\nNumber of Fisher Scoring iterations: 4\n\nconfint(bostyn_model2)\n\nWaiting for profiling to be done...\n\n\n                     2.5 %    97.5 %\n(Intercept)        1.45962  2.289875\nstudyHypothetical -1.41563 -0.120060\n\n\nThe model looks reassuringly similar. We have our coefficients for the intercept and slope, with their standard errors and p-values. There are no p-values for the model fit though as remember they are based on maximum likelihood, rather than least squares.\nThe main difference here is the interpretation of the coefficients. The link function is logit, so we get them in log odds, meaning the logarithm of an odds ratio (how likely something is to happen). These are not intuitive to interpret, so we can take the exponential to transform the slope coefficient from log odds to an odds ratio with its 95% CI.\n\n# Take the exponential of the second coefficient to avoid typing it manually\nexp(coef(bostyn_model2)[2]) #subset second component\n\nstudyHypothetical \n        0.4624211 \n\n# Exponential of the confidence interval\nexp(confint(bostyn_model2))\n\nWaiting for profiling to be done...\n\n\n                      2.5 %    97.5 %\n(Intercept)       4.3043235 9.8737011\nstudyHypothetical 0.2427726 0.8868672\n\n\nAn odds ratio is a ratio of the odds of an event happening in one group compared to the odds of an event happening in another group. In this context, a ratio of making a consequentialist decision (1s compared to 0s) in the real group compared to the hypothetical group.\nAn odds ratio of 1 means they are equally likely to happen. An odds ratio of less than 1 means something is less likely to happen, whereas more than 1 means something is more likely to happen. The odds ratio is 0.46 here, suggesting the odds of making a consequentialist decision is lower in the hypothetical group (our target group) compared to the real group (the reference group). Odds ratios below 1 can be tricky to interpret, so you can flip them around by taking the reciprocal.\n\n# Take the reciprocal of the exponential of the second coefficient \n1 / exp(coef(bostyn_model2)[2])\n\nstudyHypothetical \n         2.162531 \n\n# Reciprocal of the exponential of the confidence interval\n1 / exp(confint(bostyn_model2))\n\nWaiting for profiling to be done...\n\n\n                      2.5 %    97.5 %\n(Intercept)       0.2323245 0.1012791\nstudyHypothetical 4.1190801 1.1275645\n\n\nExpressed this way, there are 2.16 (95% CI = [1.13, 4.12]) lower odds of making a consequentialist decision in the hypothetical group compared to the real group. In this model, it is statistically significant but we can see there is a decent amount of uncertainty around the estimate, spanning from 1.13 to 4.12.\n\n\n\n\n\n\nImportant\n\n\n\nIn this scenario, you would get the same reciprocal odds ratio if you changed the groups around with correction as the reference group. Taking the reciprocal is the same information expressed differently, but its important you are clear about the model estimates to avoid confusing your reader. If you do this yourself, make sure you explain what the model estimates were and how you took the reciprocal.\n\n\nReturning to the idea of model comparison, we could compare the two GLM models using anova() and it will tell you the change in deviance (relating the maximum likelihood fitting process), but we can focus on comparing the AIC values for model fit between the two models. Lower values of AIC are better and if we subtract model 1 (the linear model) from model 2 (the logistic model), a negative difference would indicate better model fit from the ordinal model.\n\n# AIC of model 1\nAIC(bostyn_model1_glm)\n\n[1] 243.3384\n\n# AIC of model 2\nAIC(bostyn_model2)\n\n[1] 250.1702\n\n# difference in AIC values\nAIC(bostyn_model2) - AIC(bostyn_model1_glm)\n\n[1] 6.831779\n\n\nInterestingly, the second model actually has worse fit! There is less prediction error when we use the regular linear regression model compared to the logistic regression model. Given the assumptions and what we know of the design though, the logistic regression model would still be the sensible choice here. This reinforces why it is so important to recognise you as the decision maker and considering different sources of information.\n\n5.2.4.3 Calculating the model probability estimates\nFinally, there is a relationship between probability, odds, and log odds. We can use the coefficients to produce model estimates for the probability of a consequentialist decision in each group. Voas & Watt (2024) recommend this as the key output of a logistic regression model for helping to communicate your findings.\n\n\n\n\n\n\nWarning\n\n\n\nRelatedly, the whole purpose of Voas & Watt (2024) is to warn against a common misinterpretation of logistic regression. Often, people interpret the odds ratio as an increase in the likelihood of a response. For example, our odds ratio of 2.16 mistakenly interpreted as a consequentialist decision being just over twice as likely in the real group than the hypothetical group. In reality, it is a 2.16 higher odds of making a consequentialist decision, and the difference will be clearer once we express it as probabilities.\n\n\nThe logic is the same as for regular linear regression models, the intercept is the reference group, and the slope coefficient for your predictor is the shift across units of your outcome. First, we can calculate the model estimated probability for the reference group by taking the exponential of the intercept log odds divided by 1 plus the exponential intercept log odds:\n\\[ p = \\dfrac{e^{b_0}}{1 + e^{b_0}}\\]\nOr as code:\n\n# Estimated probability in the reference group\n# We use coef() to isolate the model coefficients and subset the 1st item - the intercept\nexp(coef(bostyn_model2)[1]) / (1 + exp(coef(bostyn_model2)[1]))\n\n(Intercept) \n  0.8645833 \n\n\nThis means the estimated probability of a consequentialist decision in the real group is .86 or 86%.\nThen for the hypothetical group, we repeat the process, but adding together the intercept and slope coefficient. Given the slope was negative, this is the equivalent of subtracting the slope. As an equation:\n\\[ p = \\dfrac{e^{b_0 + b_1}}{1 + e^{b_0 + b_1}}\\]\nThen as code:\n\n# Estimated probability in the target group\n# We use coef() to isolate the model coefficients and subset the 1st item - the intercept \n# We then add the 2nd item in coef() - the slope \nexp(coef(bostyn_model2)[1] + coef(bostyn_model2)[2]) / (1 + exp(coef(bostyn_model2)[1] + coef(bostyn_model2)[2]))\n\n(Intercept) \n   0.746988 \n\n\nThis means the estimated probability of a consequentialist decision in the hypothetical group is .75 or 75%. These values line up nicely with our original summary statistics, but keep in mind they are model estimates. They are closer here as its a very simple model and we are demonstrating the principles, but they will not line up this close when you have more complex models with marginal effects to understand.\n\n5.2.4.4 Using emmeans to get model estimates\nAnother way of getting model estimates is using the emmeans() function from the emmeans package (Lenth, 2024). This can be particularly useful in more complicated models when you want marginal effects across interactions, but you might prefer to see the estimates in this format instead of reconstructing them from the coefficients.\n\n# logistic regression model as the object\n# Specifying Condition as the predictor we want marginal effects for\nemmeans(bostyn_model2, specs = \"study\")\n\n study        emmean    SE  df asymp.LCL asymp.UCL\n Real           1.85 0.211 Inf     1.441      2.27\n Hypothetical   1.08 0.252 Inf     0.588      1.58\n\nResults are given on the logit (not the response) scale. \nConfidence level used: 0.95 \n\n\nThese are still in log odds units, so you can calculate the probability using the same $ p = $ method. For example, the estimated probability of a consequentialist decision in the real group is:\n\nexp(1.85) / (1 + exp(1.85))\n\n[1] 0.8641271\n\n\nWhich corresponds nicely with what we reconstructed from the model coefficients.\n\n5.2.4.5 Plotting logistic regression models\nLike any model, it will make your life and the lives of your reader much easier to supplement your statistics with a plot informatively showing the model estimates. We can use the sjPlot function (Lüdecke, 2024) to take the logistic regression model and plot the predicted probabilites.\n\n# sjPlot makes it easy to plot estimates from the model\nplot_model(bostyn_model2, # You enter the model object\n           type = \"pred\", # The type of plot you want to create\n           terms = \"study\") # The predictor(s)\n\n\n\n\n\n\n\nPlus some additional arguments and ggplot() layers to tidy things up.\n\nplot_model(bostyn_model2,\n           type = \"pred\",\n           terms = \"study\",\n           axis.title = c(\"Real or Hypothetical Scenario\", \"Proportion Consequentialist Decision\"),\n           axis.lim = c(0.5, 1)) + \n  theme_classic() + \n  labs(title = \"\")\n\nScale for y is already present.\nAdding another scale for y, which will replace the existing scale.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalised Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-glm.html#ordinal-regression",
    "href": "05-glm.html#ordinal-regression",
    "title": "\n5  Generalised Linear Regression\n",
    "section": "\n5.3 Ordinal Regression",
    "text": "5.3 Ordinal Regression\n\n5.3.1 Introduction to the dataset\nTo demonstrate ordinal regression, we will use data from Brandt et al. (2014). The aim of Brandt et al. was to replicate a relatively famous social psychology study (Banerjee et al., 2012) on the effect of recalling unethical behaviour on the perception of brightness.\nIn common language, unethical behaviour is considered as “dark”, so the original authors designed a priming experiment where participants were randomly allocated to recall an unethical behaviour or an ethical behaviour from their past. Participants then completed a series of measures including their perception of how bright the testing room was. Brandt et al. were sceptical and wanted to replicate this study to see if they could find similar results.\nParticipants were randomly allocated (ExpCond) to recall an unethical behaviour (1; n = 49) or an ethical behaviour (0; n = 51). The key outcome was their perception of how bright the room was (welllit), from 1 (not bright at all) to 7 (very bright). The research question was: Does recalling unethical behaviour lead people to perceive a room as darker than if they recall ethical behaviour?\n\n# Brandt data for ordinal regression\nBrandt_data &lt;- read_csv(\"data/Brandt_unlit.csv\") %&gt;% \n  mutate(ExpCond = as.factor(case_when(ExpCond == 1 ~ 0, # Ethical \n                             ExpCond == -1 ~ 1))) # Unethical\n\nRows: 100 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (7): BasicFollowedDirections, ExpCond, flagged, ppnr, bright, welllit, W...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n5.3.2 Exploratory data analysis\nWhen starting any data analysis, it is important to visualise the data for some exploratory data analysis. Using the skills you developed in data skills for reproducible research, you can explore the data to understand its properties and look for potential patterns. We can create a boxplot to get a brief overview of how perceived brightness changes depending on the experimental condition.\n\nBrandt_data %&gt;% \n  ggplot(aes(x = ExpCond, y = welllit)) + \n  geom_boxplot() + \n  scale_x_discrete(name = \"Ethical Group\", labels = c(\"Ethical\", \"Unethical\")) + \n  scale_y_continuous(name = \"Brightness Rating\", breaks = 1:7) + \n  theme_minimal()\n\n\n\n\n\n\n\nThere are some signs of the outcome being ordinal, but its not super obvious. We can create a histogram showing the distribution of brightness rating by group, to see how discrete the data are.\n\nBrandt_data %&gt;% \n  ggplot(aes(x = welllit, fill = ExpCond)) + \n  geom_histogram(position = \"dodge\") + \n  scale_x_continuous(name = \"Brightness Rating\", breaks = 1:7) + \n  scale_y_continuous(name = \"Count\", breaks = seq(0, 20, 5), limits = c(0, 20)) + \n  theme_minimal()\n\n\n\n\n\n\n\nThat’s much better. Now, we can see the clearly ordinal nature of brightness rating and how participants can only respond on the integer 1-7 scale.\n\n5.3.3 Would linear regression work?\n\n\n\n\n\n\nWarning\n\n\n\nThe most important part of data analysis is your role as the decision maker and thinking about what would be the most appropriate choice of modelling. Sometimes, there are equally valid approaches, while other times there is one better suited approach. If you apply an inappropriate test, R will gladly follow your instructions. Its your job to think critically about your modelling approach and check its an appropriate choice.\n\n\nR will happily let us apply simple linear regression to predict brightness rating (welllit) from the categorical predictor of experimental condition (ExpCond). This is by far the most common approach to tackling ordinal outcomes in psychology, particularly when you take the mean or sum of multiple ordinal items, so it looks a little more continuous. If you meet the other assumptions of linear regression, it can often be a pragmatic and robust approach.\n\n# Linear model predicting brightness rating from condition \nbrandt_model1 &lt;- lm(welllit ~ ExpCond, data = Brandt_data)\n\n# model summary\nsummary(brandt_model1)\n\n\nCall:\nlm(formula = welllit ~ ExpCond, data = Brandt_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.1569 -0.3673  0.2379  0.8431  1.8431 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   5.1569     0.1779  28.992   &lt;2e-16 ***\nExpCond1      0.2105     0.2541   0.828    0.409    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.27 on 98 degrees of freedom\nMultiple R-squared:  0.006953,  Adjusted R-squared:  -0.00318 \nF-statistic: 0.6861 on 1 and 98 DF,  p-value: 0.4095\n\n# Confidence intervals around estimates\nconfint(brandt_model1)\n\n                 2.5 %    97.5 %\n(Intercept)  4.8038772 5.5098483\nExpCond1    -0.2937809 0.7147492\n\n\nThe intercept suggests the mean brightness rating for participants in the ethical group was 5.16. The coefficient and model are not statistically significant - explaining very little variance in the outcome - and suggests those in the unethical group had a mean increase in brightness rating of 0.21 [-0.29, 0.71].\nThe first problem comes in the interpretation. As a linear model expecting normally distributed residuals, we get means for the intercept and coefficient, and they could - at least theoretically - be any values. So, we get an average of 5.16 and 5.37 for our two groups, but the scale items could only be integers from 1-7. So, you can question how informative it is to treat these data as interval, when they are only labels for arbitrary numbers ranging from 1 (not bright at all) to 7 (very bright).\nThe second problem comes in the assumptions of the model. If the means are around the middle of the scale, it can behave approximately normal. If the means are closer to the boundaries, then you start running into problems as the residuals that the model thinks are theoretically boundless, hit a boundary. Let’s look at the checks for this model.\n\n# Change the panel layout to 2 x 2\npar(mfrow=c(2,2))\n\n# plot the diagnostic plots\nplot(brandt_model1)\n\n\n\n\n\n\n\nThe qq plot in pane 2 of the residuals against the theoretical distribution is key here. Most of the points follow the dashes line, but it shifts away towards the lower boundary. There are also telltale signs of discrete ordinal data as the residuals move in set points. This is because the outcome can only be 1-7, but the theoretical quantiles are boundless.\nSo, while the assumptions do not look catastrophic, we will explore how we can model the ordinal nature of the outcome better.\n\n5.3.4 Ordinal regression\nOrdinal regression models are a kind of logistic regression model. They are also known as cumulative link or ordered logit models. They work on the same logic as logistic regression where we want to model the probability of a discrete outcome, but instead of only two possible outcomes, ordinal regression models split a scale up into an ordered series of discrete outcomes. Between each scale point, the model estimates the threshold and sees how the outcome shifts across your predictor(s).\nTo get an idea of cumulative probability, we can plot the Brandt et al. data to see how the cumulative percentage of responses across the scale shifts for each group.\n\nBrandt_data %&gt;% \n  ggplot(aes(x = welllit, colour = ExpCond)) + \n  stat_ecdf() +\n  scale_x_continuous(breaks = 1:7, name = \"Brightness Perception\") + \n  scale_y_continuous(breaks = seq(0, 1, 0.2), name = \"Cumulative Percent\") + \n  theme_minimal()\n\n\n\n\n\n\n\nFor each group, the lines corresponds with the cumulative percentage at each scale point. When the line is horizontal, there is no change. When the line moves vertically, it represents the next percentage increase, adding to the cumulative percentage from the previous shift, until you reach 100%.\nOrdinal regression models estimate these shifts in probability and you get k-1 estimates for the thresholds, meaning you get one fewer estimate than the total number of scale points. Behind the scenes, the model interprets the outcome as a Gaussian looking latent variable, meaning the ordinal scale points are tapping into some kind of construct, but honouring there are discrete scale points.\nThe coefficient in the model represents the probability shift of a 1 scale point increase from one distribution to another on the scale of your predictors. So, for every 1-unit increase in a continuous predictor, you see how the probability shifts, or how the probability shifts for the difference between two groups in a categorical predictor. A positive slope coefficient would mean an increase in probability along your predictor scale, while a negative slope coefficient would mean a decrease in probability along your predictor scale.\nWe will fit ordinal regression models using the ordinal(Christensen, 2023) package as it nicely supports cumulative link models as regular regression models we have used until this point or scaled up to mixed effects models.\nFirst, we need to convert the outcome into a factor, so there are a discrete ordered number of points.\n\nBrandt_data &lt;- Brandt_data %&gt;% \n  mutate(WellLitFactor = factor(welllit))\n\nlevels(Brandt_data$WellLitFactor)\n\n[1] \"1\" \"3\" \"4\" \"5\" \"6\" \"7\"\n\n\nWe can see here the scale points are in ascending order as we want them to be, and no one in the data ever responded 2. Next, we can fit our ordinal regression model using the same format as all our regression models.\n\n# clm = cumulative link model\nbrandt_model2 &lt;- clm(WellLitFactor ~ ExpCond, data = Brandt_data)\n\nsummary(brandt_model2)\n\nformula: WellLitFactor ~ ExpCond\ndata:    Brandt_data\n\n link  threshold nobs logLik  AIC    niter max.grad cond.H \n logit flexible  100  -152.47 316.94 6(0)  1.10e-07 6.9e+01\n\nCoefficients:\n         Estimate Std. Error z value Pr(&gt;|z|)\nExpCond1   0.2594     0.3606   0.719    0.472\n\nThreshold coefficients:\n    Estimate Std. Error z value\n1|3  -4.4751     1.0181  -4.395\n3|4  -1.7798     0.3403  -5.231\n4|5  -1.0845     0.2921  -3.713\n5|6   0.1309     0.2703   0.484\n6|7   1.9504     0.3460   5.637\n\nconfint(brandt_model2)\n\n              2.5 %    97.5 %\nExpCond1 -0.4465061 0.9702407\n\n\nIn the first segment of the output, its somewhat similar to the other models we have fitted, but there is no intercept at the top. We just have the slope coefficients for each predictor (1 in this case), then the threshold coefficients for k-1 scale points.\nAs this is an adapted version of logistic regression, the units are in log odds. So, they are easier to interpret as an odds ratio by taking the exponential.\n\nexp(brandt_model2$beta)\n\nExpCond1 \n1.296112 \n\nexp(confint(brandt_model2))\n\n             2.5 %  97.5 %\nExpCond1 0.6398599 2.63858\n\n\nThe results correspond with what we plotted previously, we have an odds ratio of 1.30 [0.64, 2.64], meaning there are 1.3 higher odds of a brighter rating in the unethical condition compared to the ethical condition. However, the confidence interval is pretty wide here and the p-value is not statistically significant, so we cannot conclude there is a shift in probability between the two groups here.\nReturning to the idea of model comparison, it will not let us compare them using anova() as we cannot compare the same outcome when its numeric vs a factor. However, we can compare the AIC values for model fit between the two models. Lower values of AIC are better and if we subtract model 1 (the linear model) from model 2 (the ordinal model), a negative difference would indicate better model fit from the ordinal model.\n\nAIC(brandt_model2) - AIC(brandt_model1)\n\n[1] -18.67021\n\n\nThat does seem to be the case here. Although both models were non-significant, modelling the outcome as ordinal - reassuringly - fits the data better with less prediction error.\n\n5.3.4.1 Breaking down threshold coefficients\nWe would normally focus on the coefficient as the main outcome of the modelling, but we can also break down the threshold coefficients so they are not a mystery. These represent the estimates for the cumulative probability of choosing a given scale point. These relate to the reference group, so we see the cumulative probability of the ethical condition. Remember, these are still in log odds, so we can calculate the probability like we did for logistic regression by dividing the odds ratio by 1 plus the odds ratio.\n\n\n\n\n\n\nNote\n\n\n\nThe code below uses coef() function and subsetting (the [1]etc.) to avoid writing out the numbers manually and reduce copy and paste errors. Try coef(brandt_model2) in the console to see the coefficients it returns.\n\n\n\n# Cumulative percentage for ethical group choosing less than 3 - coefficient 1\nexp(coef(brandt_model2)[1]) / (1 + exp(coef(brandt_model2)[1]))\n\n       1|3 \n0.01126058 \n\n# Cumulative percentage for ethical group choosing less than 4 - coefficient 2\nexp(coef(brandt_model2)[2]) / (1 + exp(coef(brandt_model2)[2]))\n\n      3|4 \n0.1443283 \n\n# Cumulative percentage for ethical group choosing less than 5- coefficient 3\nexp(coef(brandt_model2)[3]) / (1 + exp(coef(brandt_model2)[3]))\n\n      4|5 \n0.2526636 \n\n# Cumulative percentage for ethical group choosing less than 6- coefficient 4\nexp(coef(brandt_model2)[4]) / (1 + exp(coef(brandt_model2)[4]))\n\n      5|6 \n0.5326661 \n\n# Cumulative percentage for ethical group choosing less than 7- coefficient 5\nexp(coef(brandt_model2)[5]) / (1 + exp(coef(brandt_model2)[5]))\n\n      6|7 \n0.8754855 \n\n\nThis means choosing less than 3 (remember no one responded 2) represents .011 or 1.12%. Choosing less than 4 represents .144 or 14.4%, and so on. Once we get to the final threshold, this represents less than 7 at .875/87.5%. The cumulative probability of 7 or less is then 1/100%, which is why you get k-1 thresholds. So, you can work that final step out as 1 - .875 = .125/12.5%.\nThese cumulative percentage estimates are for the reference group (ethical), so you could calculate the estimate for the unethical group by subtracting the slope coefficient (0.2594) from each threshold. For example, for the first threshold:\n\n# Cumulative percentage for unethical group choosing less than 3\n# Coefficient 1 minus coefficient 6 as the slope is weirdly the last one here, not the first\nexp(coef(brandt_model2)[1] - coef(brandt_model2)[6]) / (1 + exp(coef(brandt_model2)[1] - coef(brandt_model2)[6]))\n\n        1|3 \n0.008710377 \n\n\n\n\n\n\n\n\nNote\n\n\n\nAs before, we are using the coef() function and subsetting (the [1]etc.) to avoid writing out the numbers manually and reduce copy and paste errors. This time, we are using two coefficients to subtract the slope coefficient from each threshold coefficient. So, if you apply this to a new data set, make sure you check how many coefficients there are to subset them accurately.\n\n\nThis means the cumulative probability estimate for less than 3 is .0087 or 0.87% in the unethical group.\n\n5.3.4.2 Different threshold assumptions\nFinally, the default option in ordinal is for flexible thresholds between scale points. This estimates a coefficient for threshold and makes the fewest assumptions about the outcome. Alternative options are equidistant which assume evenly spaced scale points, or symmetric which assume scale points are evenly spaced above and below the scale center. These two make stronger assumptions about the outcome but require fewer parameters, so you would need to think carefully about whether they would suit the outcome you are modelling.\nTo see how the equidistant option would compare:\n\n# clm = cumulative link model with equidistant thresholds\nbrandt_model3 &lt;- clm(WellLitFactor ~ ExpCond, \n                     data = Brandt_data, \n                     threshold = \"equidistant\")\n\nsummary(brandt_model3)\n\nformula: WellLitFactor ~ ExpCond\ndata:    Brandt_data\n\n link  threshold   nobs logLik  AIC    niter max.grad cond.H \n logit equidistant 100  -158.95 323.90 5(0)  7.75e-11 8.3e+01\n\nCoefficients:\n         Estimate Std. Error z value Pr(&gt;|z|)\nExpCond1   0.2728     0.3583   0.761    0.447\n\nThreshold coefficients:\n            Estimate Std. Error z value\nthreshold.1  -3.7721     0.4394  -8.585\nspacing       1.3626     0.1280  10.647\n\n\nThis time, you only get one threshold estimate and a spacing parameter. Instead of calculating the cumulative probability at each threshold, you must successively add them together:\n\n# Cumulative percentage for ethical group choosing less than 3 equally spaced\nexp(coef(brandt_model3)[1] + 0 * coef(brandt_model3)[2]) / (1 + exp(coef(brandt_model3)[1] + 0 * coef(brandt_model3)[2]))\n\nthreshold.1 \n 0.02248644 \n\n# Cumulative percentage for ethical group choosing less than 4 equally spaced\nexp(coef(brandt_model3)[1] + 1 * coef(brandt_model3)[2]) / (1 + exp(coef(brandt_model3)[1] + 1 * coef(brandt_model3)[2]))\n\nthreshold.1 \n 0.08245417 \n\n# Etc.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalised Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-glm.html#test-yourself",
    "href": "05-glm.html#test-yourself",
    "title": "\n5  Generalised Linear Regression\n",
    "section": "\n5.4 Test Yourself",
    "text": "5.4 Test Yourself\nNow you have followed along to a guided example, it is important you can transfer your knowledge to a new scenario. So, we have a new data set for you to try out your understanding of these techniques on. Follow the instructions below and answer the questions. The solution is there to check your answers against, but make sure you try the activities independently first.\n\n5.4.1 Independent logistic regression\nFor an independent activity, we will use the same data set from Irving et al. (2022), but this time using a different question as a binary outcome. If you are coming to this fresh, make sure to remind yourself of the data set outlined in the logistic regression section and load the data.\n\nFrom here, apply what you learnt in the guided example for logistic regression to this new independent task and complete the questions below to check your understanding. Remember you are predicting DV1_3 from Condition to see if the probability of a mistaken causal inference is greater in the correction condition compared to the no correction condition.\n\nThe correction condition is a \nsignificant\nnon-significant and \npositive\nnegative predictor of the third mistaken causal inference question.\nRounding to two decimals, the slope coefficient converted to an odds ratio is . Taking the reciprocal, the odds ratio is  [lower 95% CI = , upper 95% CI = ].\nThis suggests there are  lower odds of making a mistaken causal inference in the \nno correction\ncorrection condition compared to the \nno correction\ncorrection condition.\nRounding to three decimals, the estimated probability of making a mistaken causal inference in the no correction group is  and the estimated probability of making a mistaken causal inference in the correction group is .\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nTo produce a logistic regression model for the third causal question, we will predict DV1_3 from Condition as a single predictor.\n\n\n\n\n5.4.2 Independent ordinal regression\nFor an independent activity, we will use data from Schroeder & Epley (2015). The aim of the study was to investigate whether delivering a short speech to a potential employer would be more effective at landing you a job than writing the speech down and the employer reading it themselves. Thirty-nine professional recruiters were randomly assigned to receive a job application speech as either a transcript for them to read, or an audio recording of the applicant reading the speech.\nThe recruiters then rated the applicants on perceived intellect, their impression of the applicant, and whether they would recommend hiring the candidate. All ratings were originally on a rating scale ranging from 0 (low intellect, impression etc.) to 10 (high impression, recommendation etc.), with the final value representing the mean across several items.\nFor this example, we will focus on the hire rating (variable Hire_Rating to see whether the audio condition had a would lead to higher ratings than the transcript condition (variable CONDITION).\n\nschroeder_data &lt;- read_csv(\"data/Schroeder_hiring.csv\") %&gt;% \n  mutate(condition = as.factor(CONDITION))\n\n\n\n\n\n\n\nTry this\n\n\n\nFrom here, apply what you learnt in the guided example for ordinal regression to this new independent task and complete the questions below to check your understanding. Remember you will need to treat the outcome as a factor to fit the ordinal model. The answers are based on the default approach for estimating flexible thresholds.\n\nThe experimental condition is a \nsignificant\nnon-significant and \npositive\nnegative predictor of hire rating.\nRounding to two decimals, the slope coefficient converted to an odds ratio is  [lower 95% CI = , upper 95% CI = ].\nThis suggests there are  higher odds of a larger hire rating in the \ntranscript\naudio condition compared to the \ntranscript\naudio condition.\nRounding to three decimals, the estimated cumulative probability for the transcript group choosing a hire rating of lower than 3 (hint: coefficient 3) is  and the cumulative probability for the audio group choosing a hire rating of lower than 3 (hint: coefficient 3 minus coefficient 9) is .\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nFirst, we need to convert hire rating to a factor for the ordinal model to work.\n\nschroeder_data &lt;- schroeder_data %&gt;% \n  mutate(hire_factor = factor(Hire_Rating))\n\nWe can explore the distribution of the two groups and see there is a clearer difference here, with more low ratings in the transcript (0) group and more high ratings in the audio (1) group.\n\nschroeder_data %&gt;% \n  ggplot(aes(x = Hire_Rating, fill = condition)) + \n  geom_histogram(position = \"dodge\") + \n  scale_x_continuous(breaks = 0:8, name = \"Hire Rating\") +\n  theme_minimal()\n\n\n\n\n\n\n\nAlternatively, we can visualise the cumulative probability and see how it climbs steeper for the transcript group (more lower responses) and shallow for the audio group (more higher responses).\n\nschroeder_data %&gt;% \n  ggplot(aes(x = Hire_Rating, colour = condition)) + \n  stat_ecdf() +\n  scale_x_continuous(breaks = 0:8, name = \"Hire Rating\") + \n  scale_y_continuous(breaks = seq(0, 1, 0.2), name = \"Cumulative Percent\") + \n  theme_minimal()\n\n\n\n\n\n\n\nFinally, we can fit our ordinal regression model and see the slope coefficient is positive and it is statistically significant.\n\nschroeder_model2 &lt;- clm(hire_factor ~ condition, data = schroeder_data)\n\nsummary(schroeder_model2)\n\nformula: hire_factor ~ condition\ndata:    schroeder_data\n\n link  threshold nobs logLik AIC    niter max.grad cond.H \n logit flexible  39   -77.29 172.59 7(1)  5.69e-10 1.6e+02\n\nCoefficients:\n           Estimate Std. Error z value Pr(&gt;|z|)  \ncondition1   1.5409     0.6146   2.507   0.0122 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThreshold coefficients:\n     Estimate Std. Error z value\n0|1 -2.364790   0.754794  -3.133\n1|2 -0.761366   0.459091  -1.658\n2|3 -0.009953   0.437700  -0.023\n3|4  0.526688   0.459758   1.146\n4|5  1.262387   0.504261   2.503\n5|6  2.197283   0.575101   3.821\n6|7  2.357000   0.589496   3.998\n7|8  3.976969   0.861277   4.618\n\nconfint(schroeder_model2)\n\n               2.5 %   97.5 %\ncondition1 0.3669235 2.791582\n\n\nTo be easier to interpret, we can convert the log odds to the odds ratio by taking the exponential for the slope coefficient and its 95% confidence interval.\n\n# Get beta as odds ratio instead of logit\nexp(schroeder_model2$beta)\n\ncondition1 \n  4.668953 \n\n# Convert CI to odds ratio\nexp(confint(schroeder_model2))\n\n              2.5 %  97.5 %\ncondition1 1.443288 16.3068\n\n\nFinally, as a demonstration, we can calculate the estimated cumulative probability of choosing a rating of less than 3 (0-2) for the transcript group and the audio group. Note we are using coefficient 3 as that is the third threshold. We then subtract coefficient 9 as the slope coefficient is added on to the end.\n\n# Cumulative percentage for transcript group choosing less than 3 - coefficient 1\nexp(coef(schroeder_model2)[3]) / (1 + exp(coef(schroeder_model2)[3]))\n\n      2|3 \n0.4975119 \n\n# Cumulative percentage for audio group choosing less than 3 - coefficient 1 minus coefficient 9\nexp(coef(schroeder_model2)[3] - coef(schroeder_model2)[9]) / (1 + exp(coef(schroeder_model2)[3] - coef(schroeder_model2)[9]))\n\n      2|3 \n0.1749581 \n\n\n\n\n\n\n\n\n\nBostyn, D. H., Sevenhant, S., & Roets, A. (2018). Of Mice, Men, and Trolleys: Hypothetical Judgment Versus Real-Life Behavior in Trolley-Style Moral Dilemmas: Psychological Science, 29(7), 1084–1093. https://doi.org/10.1177/0956797617752640\n\n\nBrandt, M. J., IJzerman, H., & Blanken, I. (2014). Does Recalling Moral Behavior Change the Perception of Brightness? Social Psychology, 45(3), 246–252. https://doi.org/10.1027/1864-9335/a000191\n\n\nChristensen, R. H. B. (2023). Ordinal—regression models for ordinal data. https://CRAN.R-project.org/package=ordinal\n\n\nIrving, D., Clark, R. W. A., Lewandowsky, S., & Allen, P. J. (2022). Correcting statistical misinformation about scientific findings in the media: Causation versus correlation. Journal of Experimental Psychology. Applied. https://doi.org/10.1037/xap0000408\n\n\nLenth, R. V. (2024). Emmeans: Estimated marginal means, aka least-squares means. https://CRAN.R-project.org/package=emmeans\n\n\nLüdecke, D. (2024). sjPlot: Data visualization for statistics in social science. https://CRAN.R-project.org/package=sjPlot\n\n\nSchroeder, J., & Epley, N. (2015). The Sound of Intellect: Speech Reveals a Thoughtful Mind, Increasing a Job Candidate’s Appeal. Psychological Science, 26(6), 877–891. https://doi.org/10.1177/0956797615572906\n\n\nVoas, D., & Watt, L. (2024). The odds are it’s wrong: Correcting a common mistake in statistics. Teaching Statistics. https://doi.org/10.1111/test.12391",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalised Linear Regression</span>"
    ]
  },
  {
    "objectID": "05-glm.html#further-resources",
    "href": "05-glm.html#further-resources",
    "title": "\n5  Generalised Linear Regression\n",
    "section": "\n5.5 Further resources",
    "text": "5.5 Further resources\n\nThe online textbook Introduction to Modern Statistics by Çetinkaya-Rundel and Hardin (2021) outlines logistic regression in chapter 9, then separatly covers the inferential element in chapter 26.\nThe ordinal package vignette by Christensen (2023) outlines ordinal regression and the models you can fit using the package in lots of detail.\n\n\n\n\n\nBostyn, D. H., Sevenhant, S., & Roets, A. (2018). Of Mice, Men, and Trolleys: Hypothetical Judgment Versus Real-Life Behavior in Trolley-Style Moral Dilemmas: Psychological Science, 29(7), 1084–1093. https://doi.org/10.1177/0956797617752640\n\n\nBrandt, M. J., IJzerman, H., & Blanken, I. (2014). Does Recalling Moral Behavior Change the Perception of Brightness? Social Psychology, 45(3), 246–252. https://doi.org/10.1027/1864-9335/a000191\n\n\nChristensen, R. H. B. (2023). Ordinal—regression models for ordinal data. https://CRAN.R-project.org/package=ordinal\n\n\nIrving, D., Clark, R. W. A., Lewandowsky, S., & Allen, P. J. (2022). Correcting statistical misinformation about scientific findings in the media: Causation versus correlation. Journal of Experimental Psychology. Applied. https://doi.org/10.1037/xap0000408\n\n\nLenth, R. V. (2024). Emmeans: Estimated marginal means, aka least-squares means. https://CRAN.R-project.org/package=emmeans\n\n\nLüdecke, D. (2024). sjPlot: Data visualization for statistics in social science. https://CRAN.R-project.org/package=sjPlot\n\n\nSchroeder, J., & Epley, N. (2015). The Sound of Intellect: Speech Reveals a Thoughtful Mind, Increasing a Job Candidate’s Appeal. Psychological Science, 26(6), 877–891. https://doi.org/10.1177/0956797615572906\n\n\nVoas, D., & Watt, L. (2024). The odds are it’s wrong: Correcting a common mistake in statistics. Teaching Statistics. https://doi.org/10.1111/test.12391",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Generalised Linear Regression</span>"
    ]
  }
]