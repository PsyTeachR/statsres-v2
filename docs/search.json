[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics and Research Design",
    "section": "",
    "text": "Overview\nBook Name: Statistics and Research Design.\nSummary: Materials for the Statistics and Research Design course on the MSc Research Methods in Psychological Science programme, University of Glasgow School of Psychology & Neuroscience.\nAuthors: James Bartlett, Guillaume Rousselet, and Lisa DeBruine.\nAim: This course covers advanced statistics concepts you might need in psychological research, but you do not normally learn on standard curricula. The course is split into three segments for each lecturer to outline a core part of your advanced training. The first segment covers the general linear model, building up from simple linear regression to generalised linear regression (Bartlett). The second segment covers statistical fallacies, misconceptions, and bootstrapping (Rousselet). The third segment covers Bayesian approaches to hypothesis testing and estimation (Bartlett).\nContact: This book is a living document and will be regularly checked and updated for improvements. Should you have any issues using the book or queries, please contact James Bartlett.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "1  How to Use this Book",
    "section": "",
    "text": "1.1 Setup",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "instructions.html#sec-setup",
    "href": "instructions.html#sec-setup",
    "title": "1  How to Use this Book",
    "section": "",
    "text": "1.1.1 Install booktem\n# install.packages(\"devtools\")\ndevtools::install_github(\"debruine/booktem\")\n\n1.1.2 Quarto Options\nThe file _quarto.yml contains various options that you can set to change the format and look of your book.\n\n1.1.2.1 Language Options\nThere is some default text for things like the “authors” list and “table of contents” that might need translations. Set the lang key to the 2-letter language code for your language.\nYou can make a custom translation by translating the values in the include/_language.yml file.\nlang: en\n# language: include/_language.yml\n\n1.1.2.2 Project Options\nThe project key defines the inputs and outputs for the book (quarto reference).\n\n\n\n\n\n\nproject key\n\n\n\n\n\nproject:\n  type: book\n  output-dir: docs\n  resources: resources \n\n\n\nThe output-dir key defines the directory where the rendered web files will be saved. This is set to docs in order to be compatible with GitHub Pages, but you can change this if you are working with a different repository that expects the web files to be in a different directory.\nThe resources key specifies a directory that is copied verbatim to the output directory. This is where you should put, for example, data files that you want to make accessible online (sometimes they don’t automatically copy over when linked).\n\n1.1.2.3 Book Options\nThe book key defines options that affect the look and function of the book (quarto reference).\n\n\n\n\n\n\nbook key\n\n\n\n\n\nbook:\n  title: Book\n  subtitle: ~\n  author: ~\n  doi: ~\n  license: CC-BY 4.0\n  description: ~\n  cover-image: images/logos/logo.png\n  image: images/logos/logo.png\n  favicon: images/logos/logo.png\n  cookie-consent: false\n  google-analytics: ~\n  page-navigation: true\n  search: true\n  # comments:\n  #   hypothesis:\n  #     theme: clean\n  #     openSidebar: false\n  downloads: ~\n  sharing: ~\n  sidebar:\n    title: ~\n    logo: ~\n    search: true\n    contents: ~\n    style: floating\n    background: ~\n    foreground: ~\n    border: true\n    alignment: left\n    collapse-level: 3\n    pinned: true\n    header: \"\"\n    footer: \"\"\n  margin-header: ~\n  page-footer:\n    left: ~\n    right: ~\n  chapters:\n  - index.qmd\n  - instructions.qmd\n  appendices:\n  - references.qmd\n\n\n\n\n1.1.2.4 html Options\nThe format key defines options for specific formats, such as html or pdf. We’ll only be using html here (quarto reference).\n\n\n\n\n\n\nformat:html key\n\n\n\n\n\nformat:\n  html:\n    theme:\n      light:\n      - flatly\n      - include/light.scss\n      dark:\n      - darkly\n      - include/dark.scss\n    css:\n    - https://use.fontawesome.com/releases/v5.13.0/css/all.css\n    - include/booktem.css\n    - include/glossary.css\n    - include/style.css\n    df-print: kable\n    code-link: true\n    code-fold: false\n    code-line-numbers: true\n    code-overflow: wrap\n    code-copy: hover\n    highlight-style: a11y\n    mainfont: ~\n    monofont: ~\n    include-after-body: [include/script.js]\n\n\n\n\n1.1.3 Crossrefs\nSection links must start with sec- and look like this: Section 1.1.5.\n## Section Title {#sec-section-title}\n\nInternal links look like this: @sec-section-title\nFigure links must start with fig- and look like this: Figure 1.1.\n\n\n\n\n\n\n\nFigure 1.1: A histogram of a Poisson distribution with lambda = 3\n\n\n\n\nTable links must start with tbl- and look like this: Table 1.1.\n\n\n\nTable 1.1: The authors of this book\n\n\n\n\n\nfirst_name\nlast_name\n\n\n\nLisa\nDeBruine\n\n\nDaniël\nLakens\n\n\n\n\n\n\n\n\n\nSee the quarto documentation for more information.\n\n1.1.4 References\nZotero export - keep updated\n\n1.1.5 Snippets\nSnippets in RStudio provide shortcuts to syntax. For example, in an RMarkdown document, type “r” and shift-tab to expand a code chunk.\nYou can add your own snippets. Under the Tools menu, choose Edit Code Snippets... and paste the following text into the end of the appropriate sections.\n\n1.1.5.1 Markdown\nsnippet gls\n    r glossary(\"${1:term}\")\n    \nsnippet gls2\n    r glossary(\"${1:term}\", \"${2:display}\")\n    \nsnippet h1\n    # ${1:title} {#sec-${2:ref}}\n    \nsnippet h2\n    ## ${1:title} {#sec-${2:ref}}\n    \nsnippet h3\n    ### ${1:title} {#sec-${2:ref}}\n    \nsnippet h4\n    #### ${1:title} {#sec-${2:ref}}\n    \nsnippet h5\n    ##### ${1:title} {#sec-${2:ref}}\n\n1.1.6 Customize\n\n1.1.6.1 Page Footer\nThe default footer includes license YEAR, author, and github and twitter icons, but you can customize this in the _quarto.yml file under page-footer:. See the quarto documentation for more options. See the available icons at https://icons.getbootstrap.com/.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "instructions.html#sec-layout",
    "href": "instructions.html#sec-layout",
    "title": "1  How to Use this Book",
    "section": "\n1.2 Layout",
    "text": "1.2 Layout\n\n1.2.1 Conventions\nThis book will use the following conventions:\n\nCode: list(number = 1, letter = \"A\")\n\nFile paths: data/sales.csv\n\nMenu/interface options: Tools &gt; Global Options… &gt; Pane Layout\n\nR Packages: tidyverse\n\nGlossary items: alphaThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\n\nCitations: Wickham et al. (2022)\n\nInternal links: Section 1.2.1\n\nExternal links: Mastering Shiny\n\nMac-specific: Cmd-Shift-F10\n\nWindows-specific: Ctl-Shift-F10\n\n\nA list of mac and windows keyboard shortcuts.\n\n1.2.2 Figures\nIt is best practice to set a custom ggplot theme, then each subsequent plot will use that theme. You can put this code in R/my_setup.R after loading ggplot2.\nStart with a built-in theme and then add any tweaks with the theme() function.\n\nlibrary(ggplot2)\n\nmy_theme &lt;- theme_minimal(base_size = 16) + \n            theme(panel.background = element_rect(fill = \"red\", \n                                                  color = \"black\", \n                                                  size = 5),\n                  panel.grid = element_blank())\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\ntheme_set(my_theme)\n\n\nggplot(midwest, aes(popdensity, percollege)) +\n  geom_point(alpha = 0.5) +\n  labs(x = \"Population Density\", y = \"Percent College Educated\")\n\n\n\n\n\n\nFigure 1.2: Demographic information of midwest counties from 2000 US census\n\n\n\n\n\n1.2.3 Tables\n\nhead(beaver1)\n\n\n\nBeavers\n\nday\ntime\ntemp\nactiv\n\n\n\n346\n840\n36.33\n0\n\n\n346\n850\n36.34\n0\n\n\n346\n900\n36.35\n0\n\n\n346\n910\n36.42\n0\n\n\n346\n920\n36.55\n0\n\n\n346\n930\n36.69\n0\n\n\n\n\n\n\n\n1.2.4 Callout boxes\nSee the quarto reference for more options.]{.aside}\n\n\n\n\n\n\nNote\n\n\n\n.callout-note: Informational asides.\n\n\n\n\n\n\n\n\nClick to expand\n\n\n\n\n\ncolapse = “true”: Expanded!\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n.callout-tip: Tips\n\n\n\n\n\n\n\n\nWarning\n\n\n\n.callout-warning: Notes to warn you about something.\n\n\n\n\n\n\n\n\nCaution\n\n\n\n.callout-caution: Notes about things that could cause serious errors.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n.callout-important: Notes about things that are important.\n\n\n\n1.2.5 Code and Output\n\n# code chunks\npaste(\"Code\", \"Output\", 1, sep = \" \")\n\n[1] \"Code Output 1\"\n\n\n\n\n\nFilename or header\n\n# code chunks with filename\na &lt;- 1\n\n\n\n\n```{r, fig.width = 2, fig.height = 2}\n# code chunks with visible headers\nhist(rnorm(100000))\n```\n\n\n## Markdown Example\n\n* Inline code: `r nrow(iris)`\n* *Italics*\n* **Bold**\n* [Linked text](https://psyteachr.github.io)\n\n1.2.6 Fonts",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "instructions.html#sec-extras",
    "href": "instructions.html#sec-extras",
    "title": "1  How to Use this Book",
    "section": "\n1.3 Extras",
    "text": "1.3 Extras\n\n1.3.1 Glossary\nBooks are set up with lightweight glossary functions from the glossary package.\n\n# code in R/my_setup.R to initialise the glossary on each page\nlibrary(glossary)\nglossary_path(\"include/glossary.yml\")\nglossary_popup(\"click\") # \"click\", \"hover\" or \"none\"\n\nEdit the file glossary.yml with your glossary terms like this:\nalpha: |\n  The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\np-value: |\n  The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\nLook up a term from the glossary file with glossary(\"alpha\"): alphaThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\nDisplay a different value for the term with glossary(\"alpha\", \"$\\\\alpha$\"): \\(\\alpha\\)The threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\nUse an inline definition instead of the glossary file with glossary(\"beta\", def = \"The second letter of the Greek alphabet\"): betaThe second letter of the Greek alphabet\nJust show the definition with glossary(\"p-value\", show = \"def\"): The probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\nShow the table of terms defined on this page with glossary_table():\n\n\n\n\nterm\ndefinition\n\n\n\nalpha\nThe threshold chosen in Neyman-Pearson hypothesis testing to distinguish test results that lead to the decision to reject the null hypothesis, or not, based on the desired upper bound of the Type 1 error rate. An alpha level of 5% it most commonly used, but other alpha levels can be used as long as they are determined and preregistered by the researcher before the data is analyzed.\n\n\nbeta\nThe second letter of the Greek alphabet\n\n\np-value\nThe probability of the observed data, or more extreme data, if the null hypothesis is true. The lower the p-value, the higher the test statistic, and less likely it is to observe the data if the null hypothesis is true.\n\n\n\n\n\n\n1.3.2 FontAwesome\nThe fontAwesome quarto extension allows you to use the free icons with syntax like:\n{{&lt; fa dragon &gt;}}\n{{&lt; fa brands github 5x \"(github logo)\" &gt;}}\nTo install it, just run this code in the Terminal pane of RStudio (not the Console pane).\nquarto install extension quarto-ext/fontawesome\n\n\n\n\nWickham, H., Bryan, J., & Barrett, M. (2022). Usethis: Automate package and project setup. https://CRAN.R-project.org/package=usethis",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>How to Use this Book</span>"
    ]
  },
  {
    "objectID": "webexercises.html",
    "href": "webexercises.html",
    "title": "Appendix A — Webexercises",
    "section": "",
    "text": "A.1 Example Questions",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "webexercises.html#example-questions",
    "href": "webexercises.html#example-questions",
    "title": "Appendix A — Webexercises",
    "section": "",
    "text": "A.1.1 Fill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 25 is: \n\n\nThe blanks are case-sensitive; if you don’t care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\nA.1.2 Multiple Choice (mcq())\n\n“Never gonna give you up, never gonna: \nlet you go\nturn you down\nrun away\nlet you down”\n“I \nbless the rains\nguess it rains\nsense the rain down in Africa” -Toto\n\nA.1.3 True or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). \nTRUE\nFALSE\n\n\nA.1.4 Longer MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It’s probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\nthe probability that the null hypothesis is truethe probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is truethe probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\nif you repeated the process many times, 95% of intervals calculated in this way contain the true meanthere is a 95% probability that the true mean lies within this range95% of the data fall within this range",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "webexercises.html#checked-sections",
    "href": "webexercises.html#checked-sections",
    "title": "Appendix A — Webexercises",
    "section": "\nA.2 Checked sections",
    "text": "A.2 Checked sections\nCreate sections with the class webex-check to add a button that hides feedback until it is pressed. Add the class webex-box to draw a box around the section (or use your own styles).\n\nI am going to learn a lot: \nTRUE\nFALSE\n\nWhat is a p-value?\n\nthe probability that the null hypothesis is truethe probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is truethe probability of making an error in your conclusion",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "webexercises.html#hidden-solutions-and-hints",
    "href": "webexercises.html#hidden-solutions-and-hints",
    "title": "Appendix A — Webexercises",
    "section": "\nA.3 Hidden solutions and hints",
    "text": "A.3 Hidden solutions and hints\nYou can fence off a solution area that will be hidden behind a button using hide() before the solution and unhide() after, each as inline R code. Pass the text you want to appear on the button to the hide() function.\nIf the solution is a code chunk, instead of using hide() and unhide(), simply set the webex.hide chunk option to TRUE, or set it to the string you wish to display on the button.\nRecreate the scatterplot below, using the built-in cars dataset.\n\n\n\n\n\n\n\n\n\n\nI need a hint\n\nSee the documentation for plot() (?plot)\n\n\n\n\n\nClick here to see the solution\n\nplot(cars$speed, cars$dist)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Webexercises</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Wickham, H., Bryan, J., & Barrett, M. (2022). Usethis: Automate\npackage and project setup. https://CRAN.R-project.org/package=usethis",
    "crumbs": [
      "Appendices",
      "References"
    ]
  },
  {
    "objectID": "01-lm-continuous.html",
    "href": "01-lm-continuous.html",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "",
    "text": "1.1 Chapter preparation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#chapter-preparation",
    "href": "01-lm-continuous.html#chapter-preparation",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "",
    "text": "1.1.1 Introduction to the data set\nFor this chapter, we are using open data from Dawtry et al. (2015). The abstract of their article is:\n\nThe present studies provide evidence that social-sampling processes lead wealthier people to oppose redistribution policies. In samples of American Internet users, wealthier participants reported higher levels of wealth in their social circles (Studies 1a and 1b). This was associated, in turn, with estimates of higher mean wealth in the wider U.S. population, greater perceived fairness of the economic status quo, and opposition to redistribution policies. Furthermore, results from a large-scale, nationally representative New Zealand survey revealed that low levels of neighborhood-level socioeconomic deprivation?an objective index of wealth within participants’ social circles mediated the relation between income and satisfaction with the economic status quo (Study 2). These findings held controlling for relevant variables, including political orientation and perceived self-interest. Social-structural inequalities appear to combine with social-sampling processes to shape the different political attitudes of wealthier and poorer people.\n\nIn summary, the authors investigated why people with more money tend to oppose wealth redistribution policies like higher taxes for higher incomes to decrease inequality in society. We are using data from Study 1A where 305 people completed measures on household income, predicted population income, their predicted social circle income, in addition to measures on support for wealth redistribution and fairness and satisfaction with the current system.\nThey predicted people with higher incomes have social circles with higher incomes, so they are more satisfied with the current system of wealth redistribution and less interested in changing it. In essence, poorer people and richer people have different experiences of how rich and equal their country is. In this chapter, we will explore the relationship between a range of these variables.\n\n1.1.2 Organising your files and project for the chapter\nBefore we can get started, you need to organise your files and project for the chapter, so your working directory is in order. This course builds on Data Skills for Reproducible Research to focus on inferential statistics, so if any concepts sound unfamiliar, make sure you revisit the course book.\nTo keep your work organised for this course, we recommend creating a folder on your computer for the course, then a separate sub-folder for assessments and each chapter of the book. Within each sub-folder, create folders for data and figures, so you have somewhere memorable to save the files and you know where everything is.\n\nIn your folder for statistics and research design Stats_Research_Design, create a new folder called 01_regression_continuous. Within 01_regression_continuous, create two new folders called data and figures.\nCreate an R Project for 01_regression_continuous as an existing directory for your chapter folder. This should now be your working directory.\nCreate a new Quarto document and give it a sensible title describing the chapter, such as 01 Correlations and Regression and save the file in your 01_regression_continuous folder.\nWe are working with a new data set, so please save the following data file: Dawtry_2015.csv. Right click the link and select “save link as”, or clicking the link will save the files to your Downloads. Make sure that you save the file as “.csv”. Save or copy the file to your data/ folder within 01_regression_continuous.\n\nYou are now ready to start working on the chapter!\n\n1.1.3 Activity 1 - Read and wrangle the data\nAs the first activity, try and test yourself by completing the following task list to practice your data wrangling skills. Create a final object called dawtry_clean to be consistent with the tasks below. If you just want to focus on correlations and regression, then you can just type the code in the solution.\n\n\n\n\n\n\nTry this\n\n\n\nTo wrangle the data, complete the following tasks:\n\n\nLoad the following packages. If you do not have them already, remember you will need to install them first if you are working on your own computer:\n\ntidyverse\neffectsize\ncorrelation\nperformance\n\n\nRead the data file data/Dawtry_2015.csv to the object name dawtry_data.\nReverse code two items: redist2 and redist4 to create two new variables redist2_R and redist4_R. See the codebook below, but they are on a 1-6 scale.\nSummarise the data to calculate the mean fairness_satisfaction score, by taking the mean of two items: fairness and satisfaction.\nSummarise the data to calculate the mean redistribution score, by taking the mean of four items: redist1, redist2_R, redist3, and redist4_R.\nCreate a new object called dawtry_clean by joining dawtry_data with your two new variables fairness_satisfaction and redistribution.\nDecrease the number of columns in dawtry_clean by selecting PS, all the columns between Household_Income and redistribution, but removing the two reverse coded items redist2_R and redist4_R.\n\nYour data should look like this to be ready to analyse:\n\n\nRows: 305\nColumns: 11\n$ PS                                  &lt;dbl&gt; 233, 157, 275, 111, 52, 11, 76, 90…\n$ Household_Income                    &lt;dbl&gt; NA, 20.00, 100.00, 150.00, 500.00,…\n$ Political_Preference                &lt;dbl&gt; 5, 5, 5, 8, 5, 3, 4, 3, 2, 3, NA, …\n$ age                                 &lt;dbl&gt; 40, 59, 41, 59, 35, 34, 36, 39, 40…\n$ gender                              &lt;dbl&gt; 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, NA, …\n$ Population_Inequality_Gini_Index    &lt;dbl&gt; 38.78294, 37.21451, 20.75000, 35.3…\n$ Population_Mean_Income              &lt;dbl&gt; 29715, 123630, 60000, 59355, 15360…\n$ Social_Circle_Inequality_Gini_Index &lt;dbl&gt; 28.056738, 24.323388, 14.442577, 2…\n$ Social_Circle_Mean_Income           &lt;dbl&gt; 21150, 65355, 107100, 86640, 56850…\n$ fairness_satisfaction               &lt;dbl&gt; 1.0, 3.5, 5.0, 7.0, 4.5, 2.5, 3.0,…\n$ redistribution                      &lt;dbl&gt; 5.50, 3.25, 3.75, 2.75, 3.00, 3.75…\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nYou should have the following in a code chunk:\n\n# Load the packages below\nlibrary(tidyverse)\nlibrary(effectsize)\nlibrary(correlation)\nlibrary(performance)\n\n# Load the data file\n# This should be the Dawtry_2015.csv file \ndawtry_data &lt;- read_csv(\"data/Dawtry_2015.csv\")\n\n# Reverse code redist2 and redist4\ndawtry_data &lt;- dawtry_data %&gt;%\n  mutate(redist2_R = 7 - redist2,\n         redist4_R = 7 - redist4)\n\n# calculate mean fairness and satisfaction score  \nfairness_satisfaction &lt;- dawtry_data %&gt;% \n  pivot_longer(cols = fairness:satisfaction, \n               names_to = \"Items\", \n               values_to = \"Response\") %&gt;% \n  group_by(PS) %&gt;% \n  summarise(fairness_satisfaction = mean(Response)) %&gt;% \n  ungroup()\n\n# calculate mean wealth redistribution score  \nredistribution &lt;- dawtry_data %&gt;% \n  pivot_longer(cols = c(redist1, redist2_R, redist3, redist4_R), \n               names_to = \"Items\", \n               values_to = \"Response\") %&gt;% \n  group_by(PS) %&gt;% \n  summarise(redistribution = mean(Response)) %&gt;% \n  ungroup()\n\n# join data and select columns for focus\ndawtry_clean &lt;- dawtry_data %&gt;% \n  inner_join(fairness_satisfaction, by = \"PS\") %&gt;% \n  inner_join(redistribution, by = \"PS\") %&gt;% \n  select(PS, Household_Income:redistribution, -redist2_R, -redist4_R)\n\n\n\n\n\n1.1.4 Activity 2 - Explore the data\n\n\n\n\n\n\nTry this\n\n\n\nAfter the wrangling steps, try and explore dawtry_clean to see what variables you are working with. For example, opening the data object as a tab to scroll around, explore with glimpse(), or try plotting some of the individual variables using a histogram.\n\n\nIn dawtry_clean, we have the following variables:\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\nPS\ndouble\nParticipant ID number.\n\n\nHousehold_Income\ndouble\nHousehold income in US Dollars ($).\n\n\nPolitical_Preference\ndouble\nPolitical attitudes: 1 = very liberal/very left-wing/strong Democrat to 7 = very conservative/very right-wing/strong Republican.\n\n\nage\ndouble\nAge in years.\n\n\ngender\ndouble\n1 = “Male”, 2 = “Female.\n\n\nPopulation_Inequality_Gini_Index\ndouble\nMeasure of income inequality from 0 (perfect equality) to 100 (perfect inequality), here where participants estimated population in equality.\n\n\nPopulation_Mean_Income\ndouble\nParticipant estimate of the mean household income in the population ($).\n\n\nSocial_Circle_Inequality_Gini_Index\ndouble\nMeasure of income inequality from 0 (perfect equality) to 100 (perfect inequality), here where participants estimated inequality in their social circle.\n\n\nSocial_Circle_Mean_Income\ndouble\nParticipant estimate of the mean household income in their social circle ($).\n\n\nfairness_satisfaction\ndouble\nPerceived fairness and satisfaction about the current system of wealth redistribution: Mean of two items (1 extremely fair – 9 extremely unfair)\n\n\nredistribution\ndouble\nSupport for wealth distribution: Mean of four items (1 strongly disagree – 6 strongly agree).\n\n\n\nWe will use this data set to demonstrate correlations and regression when you have one continuous predictor.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#correlation",
    "href": "01-lm-continuous.html#correlation",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.2 Correlation",
    "text": "1.2 Correlation\nBefore we cover regression as a more flexible framework for inferential statistics, we think it is useful to start with correlation to get a feel for how we can capture the relationship between two variables. As a reminder, correlations are standardised to range from -1 (a perfect negative correlation) to 1 (a perfect positive correlation). A value of 0 would mean there is no correlation between your variables.\n\n1.2.1 Activity 3 - Visualise the relationship\nTo explore the relationship between two variables, it is useful to create a scatterplot early for yourself, then provide a more professional looking version to help communicate your results. For most of the demonstrations in this chapter, we will try and answer the research question: “Is there a relationship between support for wealth redistribution and fairness and satisfaction with the current system?”\n\n\n\n\n\n\nTry this\n\n\n\nUsing your data visualisation skills from Repro Res, recreate the scatterplot below using the variables fairness_satisfaction and redistribution from dawtry_clean.\n\n\n\n\n\n\n\n\nLooking at the graph, we can describe the relationship as \npositive\nlittle to no correlation\nnegative.\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe scatterplot shows a negative correlation between the two variables. You need to be careful interpreting fairness and satisfaction as it is coded a little counterintuitive. Higher values mean great dissatisfaction.\nAs support for wealth redistribution increases to be more positive, perceived fairness and satisfaction tends to decrease. This makes sense as people who are more dissatisfied with the current system think there should be more wealth redistribution strategies.\nYou should have the following in a code chunk:\n\ndawtry_clean %&gt;% \n  ggplot(aes(x = fairness_satisfaction, y = redistribution)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  scale_x_continuous(name = \"Perceived Fairness and Satisfaction\", \n                     breaks = c(1:9)) + \n  scale_y_continuous(name = \"Support for Wealth Redistribution\", \n                     breaks = c(1:6))\n\n\n\n\n\n1.2.2 Activity 4 - Calculate the correlation coefficient\nVisualising the relationship between two variables is great for our understanding, but it does not tell us anything about the inferential statistics for what we can learn from our sample in hypothesis testing and measures of effect size.\nA correlation is a specific application of the general linear model. We want to capture the covariation between two variables. If you are interested, see the Handy Workbook (McAleer, 2023) for the calculations behind different types of correlation and how it represents the covariance of two variables compared to their total variability. They are not the only methods, but the two most common versions of a correlation are:\n\nPearson’s product-moment correlation (often shortened to the PearsonA standardised measure of the linear relationship between two variables that makes stringent assumptions about the population. correlation) and symbolised by r.\nSpearman’s rank correlation coefficient (often shortened to the SpearmanA standardised measure of the relationship between two variables that assumes a monotonic - but not necessarily a linear - relationship and makes less stringent assumptions about the population. correlation) and symbolised by \\(r_s\\) or sometimes the Greek letter rho \\(\\rho\\).\n\nThere is a function built into R (cor.test()) to calculate the correlation between two variables, but we tend to use the correlation() function from the correlation package as it has more consistent reporting features. The correlation() function requires:\n\nThe name of the data set you are using.\nThe name of the first variable you want to select for the correlation.\nThe name of the second variable you want to select for the correlation.\nThe type of correlation you want to run: e.g. pearson, spearman.\n\nFor our dawtry_clean data, we would run the following code for a two-tailed Pearson correlation:\n\ncorrelation(data = dawtry_clean, \n            select = \"fairness_satisfaction\", \n            select2 = \"redistribution\",  \n            method = \"pearson\", \n            alternative = \"two.sided\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\nfairness_satisfaction\nredistribution\n-0.70034\n0.95\n-0.7533907\n-0.6382316\n-17.07843\n303\n&lt; .001\nPearson correlation\n305\n\n\n\n\n\nYour output will look a little different due to how our book renders tables, but you should get the same information. For the three key concepts of inferential statistics, we get\n\nHypothesis testing: p &lt; .001, suggesting we can reject the null hypothesis assuming \\(\\alpha\\) = .05.\nEffect size: r = -.70, suggesting a strong negative correlation.\nConfidence interval: [-0.75, -0.64], showing the precision around the effect size estimate.\n\nTo summarise: a Pearson correlation showed there was a strong, negative, statistically significant relationship between attitudes on wealth redistribution and perceived fairness and satisfaction, r (303) = -0.70, p &lt; .001, 95% CI = [-0.75, -0.64].\nIf we had reason to use a Spearman correlation instead, all we need to do is change the method argument.\n\ncorrelation(data = dawtry_clean, \n            select = \"fairness_satisfaction\", \n            select2 = \"redistribution\",  \n            method = \"spearman\", \n            alternative = \"two.sided\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nrho\nCI\nCI_low\nCI_high\nS\np\nMethod\nn_Obs\n\n\nfairness_satisfaction\nredistribution\n-0.6806667\n0.95\n-0.738182\n-0.6133274\n7947402\n&lt; .001\nSpearman correlation\n305\n\n\n\n\n\nSimilarly, we could report the results as: a Spearman correlation showed there was a strong, negative, statistically significant relationship between attitudes on wealth redistribution and perceived fairness and satisfaction, \\(r_s\\) (303) = -0.68, p &lt; .001, 95% CI = [-0.74, -0.61].\n\n\n\n\n\n\nTry this\n\n\n\nGreat work following along so far, but now it is time to test your understanding on a new set of variables. Use the variables age and redistribution from dawtry_clean. We can ask the question: “What is the relationship between age and attitudes on wealth redistribution?”\n\nCreate a scatterplot to visualise the relationship between age and redistribution from dawtry_clean.\n\nApply the Pearson correlation to get your inferential statistics and answer the following questions:\n\nHypothesis testing: Assuming \\(\\alpha\\) = .05, the relationship between age and wealth redistribution is \nstatistically significant\nnot statistically significant.\nEffect size: Rounded to 2 decimals, the value for Pearson’s correlation coefficient is \n-0.14\n-0.03\n0.08\n-0.49.\nConfidence interval: Rounded to 2 decimals, the lower bound is \n-0.14\n-0.03\n0.08\n-0.49 and the upper bound is \n-0.14\n-0.03\n0.08\n-0.49.\n\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe scatterplot shows very little correlation between the two variables. The regression line is almost flat and there does not appear to be a clear pattern to the data points.\n\ndawtry_clean %&gt;% \n  ggplot(aes(x = age, y = redistribution)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") + \n  scale_y_continuous(name = \"Attitudes on Wealth Distribution\", \n                     breaks = c(1:6))\n\n\n\n\n\n\n\nFor our inferential statistics, the relationship is not statistically significant and the Pearson correlation coefficient is very weak, r (302) = -0.03, p = .625, 95% CI = [-0.14, 0.08]. Note there is a missing value for age, so we have one few participant / degrees of freedom.\n\ncorrelation(data = dawtry_clean, \n            select = \"age\", \n            select2 = \"redistribution\",  \n            method = \"pearson\", \n            alternative = \"two.sided\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter1\nParameter2\nr\nCI\nCI_low\nCI_high\nt\ndf_error\np\nMethod\nn_Obs\n\n\nage\nredistribution\n-0.0281749\n0.95\n-0.1402227\n0.0845855\n-0.4898215\n302\n0.6246159\nPearson correlation\n304",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#linear-regression-with-one-continuous-predictor",
    "href": "01-lm-continuous.html#linear-regression-with-one-continuous-predictor",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.3 Linear regression with one continuous predictor",
    "text": "1.3 Linear regression with one continuous predictor\nNow you know how to calculate a correlation in R, we can turn to simple linear regression as a more flexible tool for modelling the relationship between variables. In this chapter, we focus on the relationship between a continuous outcomeThe outcome (also known as the dependent variable) is the variable you are interested in seeing a potential change in. and one continuous predictorThe predictor (also known as an independent variable) is the variable you measure or manipulate to see how it is associated with changes in the outcome variable., before extending the framework to one categorical predictor in Chapter 2. There is also a chapter in the Handy Workbook (McAleer, 2023) dedicated to manually calculating simple linear regression.\n\n1.3.1 Activity 5- Calculating descriptive statistics\nFor all the information we want to include in a report, calculating descriptive statistics is helpful for the reader to show the context behind the results you present. Here, we can report the mean and standard deviation of our two variables.\n\ndawtry_clean %&gt;% \n  # pivot longer to avoid repeating yourself\n  pivot_longer(cols = fairness_satisfaction:redistribution,\n               names_to = \"Variable\", \n               values_to = \"Value\") %&gt;% \n  # group by Variable to get one value per variable\n  group_by(Variable) %&gt;% \n  # mean and SD, rounded to 2 decimals\n  summarise(mean_variable = round(mean(Value), 2),\n            sd_variable = round(sd(Value), 2))\n\n\n\n\nVariable\nmean_variable\nsd_variable\n\n\n\nfairness_satisfaction\n3.54\n2.02\n\n\nredistribution\n3.91\n1.15\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want some reinforcement of how these skills apply to published research, look at Table 1 in Dawtry et al. (2015). The means and standard deviations here (and the correlation in Activity 4) exactly reproduce the values they report.\n\n\nIf other types of descriptive statistic would be more suitable to your data, then you can just replace the functions you use within summarise().\n\n1.3.2 Activity 6 - Using the lm() function\nFor our research question of “is there a relationship between support for wealth redistribution and fairness and satisfaction”, we can address it with simple linear regression.\nInstead of a standardised correlation coefficient, we can frame it as whether knowing fairness and satisfaction can predict values of support for wealth redistribution. The design is still correlational, so it does not tell us anything about a causal relationship in isolation. We use the word predict in the statistical sense, where we can ask whether knowing values of one variable help us predict values of another variable with a degree of error.\nThe first step is to create an object (lm_redistribution) for the linear model.\n\nlm_redistribution &lt;- lm(redistribution ~ fairness_satisfaction,\n                        data = dawtry_clean)\n\nThe function lm() is built into R and is incredibly flexible for creating linear regression models.\n\nThe first argument is to specify a formula which defines our model. The first component (redistribution) is our outcome variable for what we are interested in modelling.\nThe tilde (~) separates the equation, where everything on the right is your predictor variable(s). In simple linear regression, we just have one predictor, which is fairness_satisfaction in our model here. This is saying we want to predict redistribution as our outcome from fairness_satisfaction as our predictor.\nWe then specify the data frame we want to use.\n\n\n\n\n\n\n\nNote\n\n\n\nIn some resources, you might see people enter the same model as redistribution ~ 1 + fairness_satisfaction. The 1 + component explicitly tells R to fit an intercept, plus a slope from fairness_satisfaction. R includes an intercept by default, so you do not need to add it, but some people like to include it for clarity.\n\n\nWhen you create this object, it stores a bunch of information, but does not really tell us all the statistics we expect. If you simply print the object in the console, it will tell you the intercept and coefficient(s), but none of the model fitting nor hypothesis testing values. If you look at the object in the R environment, you will see it is a list containing several elements. It stores things like the model, the residuals, and other information you can use.\n\nlm_redistribution\n\n\nCall:\nlm(formula = redistribution ~ fairness_satisfaction, data = dawtry_clean)\n\nCoefficients:\n          (Intercept)  fairness_satisfaction  \n               5.3169                -0.3975  \n\n\nTo get that extra information, we need to call the summary() function around the linear model object to explore it’s properties like estimates and model fit.\n\nsummary(lm_redistribution)\n\n\nCall:\nlm(formula = redistribution ~ fairness_satisfaction, data = dawtry_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9193 -0.5279  0.0233  0.4782  3.3634 \n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            5.31686    0.09488   56.04   &lt;2e-16 ***\nfairness_satisfaction -0.39754    0.02328  -17.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8218 on 303 degrees of freedom\nMultiple R-squared:  0.4905,    Adjusted R-squared:  0.4888 \nF-statistic: 291.7 on 1 and 303 DF,  p-value: &lt; 2.2e-16\n\n\nTo walk through the output, Call: summarises the model you specified. Residuals: provides a summary of the model residuals which we will come back to later. Coefficients: provides our model output, this time with inferential statistics. The two key lines are:\n\n(Intercept) - This is the value of the outcome when our predictor is set to 0. For a fairness and satisfaction value of 0, we would expect a value of 5.32 for redistribution. You get a p-value for this, but in isolation it is not too useful. It just compares the intercept estimate to 0 which typically you are not interested in.\nfairness_satisfaction - This is the regression slope or coefficient. This is the change in the outcome for every 1 unit change in the predictor. So, for every 1 unit increase in fairness and satisfaction, we expect support for wealth redistribution to decrease (as we have a negative value) by 0.40 units. This is consistent with the correlation as we have a negative relationship between the two variables. Looking at the p-value, this is statistically significant (p &lt; .001), suggesting we can reject the null hypothesis and conclude there is an effect here.\n\n\n\n\n\n\n\nDoes it matter if the slope is positive or negative?\n\n\n\nWhen you have a continuous predictor, the sign is important to keep in mind. A positive slope would mean an increase in the predictor is associated with increased values of your outcome. A negative slope would mean an increase in the predictor is associated with decreased values of your outcome. This is crucial for interpreting the coefficient.\n\n\nAt the bottom of the model output, you then get the fit statistics. Multiple \\(R^2\\) tells you how much variance in your outcome your predictor(s) explain. Adjusted \\(R^2\\) tends to be more conservative as it adjusts for the number of predictors in the model (something we will not cover until Chapter 3 on multiple regression), but they will be very similar when you have one predictor. Adjusted \\(R^2\\) is .49, suggesting fairness and satisfaction explains 49% of the variance in support for wealth redistribution.\nFinally, we have the model fit statistics to tell us whether the model explains a significant amount of variance in the outcome. With one predictor, the p-value next to the coefficient and next to the model fit will be identical, as one predictor is the whole model. The F-statistic is 291.7, the model degrees of freedom is 1, the residual degrees of freedom is 303, and the p-value is p &lt; .001.\n\n\n\n\n\n\nWhat does 2e-16 mean?\n\n\n\nFor the p-value here, the output looks a little weird. R reports very small or very large numbers using scientific notation to save space. We normally report p-values to three decimals, so we report anything smaller as p &lt; .001 to say it is smaller than this.\nIf you want to see the real number, you can use the following function which shows just how small the p-value is:\n\nformat(2e-16, scientific = FALSE)\n\n[1] \"0.0000000000000002\"\n\n\n\n\n\n\n\n\n\n\nHow are correlation and regression the same?\n\n\n\n\n\nIf you are interested in the relationship between the two concepts, we said correlation was a specific application of the general linear model. It describes the - standardised - covariation between two variables compared to their total variability. For values of -1 and 1, knowing the value of one variable perfectly correlates to the value of your other variable. As you approach 0, the relationship between the variables is less perfect, meaning there is more variability left over compared to the covariance.\nIn regression, we frame it as how much variance you explain compared to the total amount of variance. The more variance your predictor explains, the less unexplained variance there is left over. We no longer calculate r, we calculate \\(R^2\\) as the proportion of variance in your outcome explained by your model. A value of 0 would be you explain no variance and a value of 1 means you explain all the variance.\nYou can see the connection between the two by comparing the value of Pearson’s r from Activity 4 (-.70) to the value of \\(R^2\\) = .4905. If you take the square root to get r (sqrt(.4905)), you get .70, which is exactly the same absolute value since \\(R^2\\) can only be positive.\nSo, when you have a single continuous predictor, it is the exact same process as correlation, just expressed slightly different.\n\n\n\n\n1.3.3 Activity 7 - Calculating confidence intervals\nIn the standard lm() and summary() output, we get most of the key values we need for our inferential statistics, but the one thing missing is confidence intervals around our estimates. Fortunately, R has a built-in function called confint() for calculating confidence intervals using your linear model object.\n\nconfint(lm_redistribution)\n\n                           2.5 %     97.5 %\n(Intercept)            5.1301581  5.5035664\nfairness_satisfaction -0.4433442 -0.3517332\n\n\nNormally, you focus on the confidence interval around your slope estimate as the intercept is not usually super useful for interpreting your findings when you have a continuous predictor. Now, we can summarise the three key concepts of inferential statistics as:\n\nHypothesis testing: p &lt; .001, suggesting we can reject the null hypothesis assuming \\(\\alpha\\) = .05. Fairness and satisfaction is a significant predictor of support for wealth redistribution.\nEffect size: \\(b_1\\) = -0.40, suggesting fairness and satisfaction is a negative predictor.\nConfidence interval: [-0.44, -0.35], showing the precision around the slope estimate.\n\n\n\n\n\n\n\nTry this\n\n\n\nGreat work following along so far, but now it is time to test your understanding on a new set of variables. This time, use redistribution as your outcome, age as your predictor, and use dawtry_clean as your data. We can ask the same question as before: “What is the relationship between age and attitudes on wealth redistribution?”.\nApply simple linear regression to get your inferential statistics and answer the following questions:\n\nHypothesis testing: Assuming \\(\\alpha\\) = .05, age is a \nstatistically significant\nnon-significant predictor of support for wealth redistribution.\nEffect size: Rounded to 2 decimals, the age coefficient is \n4.01\n-0.003\n0.005\n0.22.\nConfidence interval: Rounded to 2 decimals, the lower bound of the age coefficient is \n3.59\n-0.01\n4.44\n0.01 and the upper bound is \n3.59\n-0.01\n4.44\n0.01.\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe conclusions are the same as when we calculated the correlation where age is not a statistically significant predictor of support for wealth redistribution. As a regression model, we get the same conclusions expressed in a slightly different way. Age is negative, but the size of the slope is very small (-0.003) and non-significant (p = .625). We explain pretty much no variance in support for wealth redistribution (\\(R^2\\) = .0008), so age is not very informative as a predictor.\n\n# Create lm object for age as a predictor\nlm_age &lt;- lm(redistribution ~ age,\n             data = dawtry_clean)\n\n# summary of the model object\nsummary(lm_age)\n\n# confidence intervals around estimates\nconfint(lm_age)\n\n\nCall:\nlm(formula = redistribution ~ age, data = dawtry_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.93382 -0.69527  0.08099  0.84379  2.13621 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.011931   0.216042   18.57   &lt;2e-16 ***\nage         -0.002693   0.005499   -0.49    0.625    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.153 on 302 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.0007938, Adjusted R-squared:  -0.002515 \nF-statistic: 0.2399 on 1 and 302 DF,  p-value: 0.6246\n\n                  2.5 %     97.5 %\n(Intercept)  3.58679351 4.43706821\nage         -0.01351424 0.00812738\n\n\n\n\n\n\n1.3.4 Activity 8 - Centering and standardising predictors\nSo far, we have covered specifying your outcome and predictor variables as their raw values in the data. However, there are two variations that are useful to understand: centering and standardising predictors. These do not change the model fitting or p-values, but change how you interpret the intercept and/or slope.\n\n1.3.4.1 Centering predictors\nCenteringUsually, centering a predictor means subtracting the mean from each value, so the mean is 0. You can then interpret the intercept as the value of your outcome for the mean value of your predictor(s). predictors is where you change the values of your predictor, but not their scale. Typically, this means substracting the mean of your predictor from each observation. This changes how you interpret the intercept of your regression model.\nRemember the interpretation of the intercept is the predicted value of your outcome when your predictor is set to 0. If 0 is not present in your data or a value of 0 would be uninformative, the intercept can be difficult to interpret. When you center your predictor, 0 becomes the mean value of your predictor. So, the intercept is now the predicted value of your outcome for the mean value of your predictor, but the slope itself does not change. You can see the impact of this by plotting the data side by side in Figure 1.1.\n\ndawtry_clean &lt;- dawtry_clean %&gt;% \n  # subtract fairness values from mean of fairness\n  mutate(fairness_center = fairness_satisfaction - mean(fairness_satisfaction))\n\n\n\n\n\n\n\n\nFigure 1.1: Top: The relationship between wealth redistribution and perceived fairness and satisfaction using raw values. Bottom: The relationship after centering perceived fairness and satisfaction values.\n\n\n\n\nThe relationship between the two variables is exactly the same, but the values of fairness and satisfaction shifted so the mean is 0. If you create a new linear model object, you can see the difference this makes to the output.\n\nlm_center &lt;- lm(redistribution ~ fairness_center, \n                data = dawtry_clean)\n\nsummary(lm_center)\n\n\nCall:\nlm(formula = redistribution ~ fairness_center, data = dawtry_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9193 -0.5279  0.0233  0.4782  3.3634 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      3.90984    0.04706   83.09   &lt;2e-16 ***\nfairness_center -0.39754    0.02328  -17.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8218 on 303 degrees of freedom\nMultiple R-squared:  0.4905,    Adjusted R-squared:  0.4888 \nF-statistic: 291.7 on 1 and 303 DF,  p-value: &lt; 2.2e-16\n\n\nEvery single one of the values remains the same apart from the intercept. Now, we can interpret it as the predicted value of redistribution when fairness and satisfaction is set to 0, i.e., the mean value. So, for the mean value of fairness and satisfaction, we would predict a value of 3.91 for redistribution.\n\n1.3.4.2 Standardising predictors\nStandardisingStandardising involves substracting the variable mean from each value and dividing it by the variable standard deviation. It then has the property of a mean of 0 and standard deviation of 1, so you interpret the units as standard deviations. predictors is where you first convert your values to z-scores. This means you interpret the values as standard deviations rather than your original units. This is more useful in multiple regression (Chapter 3) to compare the magnitude of predictors, but it is useful to get used to now when you only have one predictor to focus on.\nThe first step is to standardise all your variables, not just the predictor this time. This involves subtracting the mean of your variable from each value, and dividing by the standard deviation of the variable. They now have a mean of 0 and a standard deviation of 1.\n\n# Be careful with the bracket placement to subtract the mean first\ndawtry_clean &lt;- dawtry_clean %&gt;% \n  mutate(redistribution_std = (redistribution - mean(redistribution)) / sd(redistribution),\n         fairness_std = (fairness_satisfaction - mean(fairness_satisfaction)) / sd(fairness_satisfaction))\n\nOnce we enter them into the model, we no longer have values in the original units of measurement, we now have them expressed as standard deviations.\n\nlm_standardised &lt;- lm(redistribution_std ~ fairness_std, \n                      data = dawtry_clean)\n\nsummary(lm_standardised)\n\n\nCall:\nlm(formula = redistribution_std ~ fairness_std, data = dawtry_clean)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.53979 -0.45930  0.02026  0.41604  2.92617 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   7.387e-16  4.094e-02    0.00        1    \nfairness_std -7.003e-01  4.101e-02  -17.08   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.715 on 303 degrees of freedom\nMultiple R-squared:  0.4905,    Adjusted R-squared:  0.4888 \nF-statistic: 291.7 on 1 and 303 DF,  p-value: &lt; 2.2e-16\n\n\nLike centering, the model fit and p-values do not change again, apart from the intercept. The relationship between the variables is exactly the same, but we changed their units. The intercept is tiny and close enough to zero that the p-value is 1.\nMore importantly, the slope is now expressed in standard deviations. Annoyingly, R prints the values in scientific notation, so this can be awkward to read (remember the format() function). Now, for every 1 standard deviation increase in our predictor, we predict the outcome to decrease by 0.70 standard deviations.\n\n\n\n\n\n\nTip\n\n\n\nIt is important to demonstrate the underlying concepts first but if you want a shortcut without needing to standardise all your variables, the effectsize package has a handy function called standardize_parameters() which you can apply to your initial lm() object.\n\nstandardize_parameters(lm_redistribution)\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n(Intercept)\n0.00000\n0.95\n-0.0805627\n0.0805627\n\n\nfairness_satisfaction\n-0.70034\n0.95\n-0.7810351\n-0.6196449",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#checking-assumptions",
    "href": "01-lm-continuous.html#checking-assumptions",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.4 Checking assumptions",
    "text": "1.4 Checking assumptions\nFor the inferential statistics to work as intended, the model makes certain assumptions about the data you are putting into it and the accuracy of the inferences depends on how sensible these assumption are. Remember these functions will always work even if the numbers you enter are nonsense, so it’s important for you as the researcher to recognise when it’s appropriate to use these techniques and when it is not.\n\n1.4.1 Activity 9 - Diagnostic plots for linear regression\nAs a reminder, the assumptions for simple linear regression are:\n\nThe outcome is interval/ratio level data.\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\nThe predictors have non-zero variance.\nThe relationship between the outcome and predictor is linear.\nThe residuals should be normally distributed.\nThere should be homoscedasticity.\n\nAssumptions 1-4 are pretty straight forward as they relate to your understanding of the design or a simple check on the data for non-zero variance (the responses are not all the exact same value).\nAssumptions 5-7 require diagnostic checks on the residualsThe difference between the observed value in the data and the predicted value from the model given its assumptions. from the model. The residuals are the difference between your observed values in the data and the values your model predicts given it’s assumptions. If you remember back, we highlighted the model output mentioned Residuals: and they are saved within the model object.\n\n# head shows the first 6 values \nhead(lm_redistribution$residuals)\n\n         1          2          3          4          5          6 \n 0.5806764 -0.6754769  0.4208311  0.2159084 -0.5279383 -0.5730156 \n\n\n\n\n\n\n\n\nWhat does the $ symbol mean?\n\n\n\nWe have not used this symbol in the book yet, but it is a base R operator for extracting information. You use it to access specific components within a data frame or object.\nTry the following in the console to see what it does:\n\ndawtry_clean$age\nlm_redistribution$coefficients\n\nIf you type an object name into the console and add the $, you will see all the components appear to auto complete.\n\n\nIn your reading, you might see individual statistical tests to check these assumptions, but they have more limitations than benefits. The best way to check the assumptions is through diagnostic plots which express the model residuals in different ways. We want to walk through this longer way of checking assumptions to develop a solid understanding before showing you a shortcut to see them all below.\nIn the code below, we use a format you will be less familiar with as all these functions come from base R. If you just run the code for the diagnostic plots plot(lm_redistribution), each one gets individually printed to your Plots window. Here, we create a 2x2 panel to show them all together.\n\n# Change the panel layout to 2 x 2\npar(mfrow=c(2,2))\n\n# plot the diagnostic plots \nplot(lm_redistribution)\n\n\n\n\n\n\n\nFor more information on each of these plots, see this great resource by Kim (2015) via the University of Virginia, but we will break the key ones down below.\n\n1.4.2 Checking linearity\nTo isolate each plot, we can use the which argument. Plot 1 is a residuals vs fitted plot and helps us check linearity by showing the residuals on the y-axis and the fitted (predicted) values on the x-axis.\n\nplot(lm_redistribution, \n     which = 1)\n\n\n\n\n\n\n\nHere, you are looking out for a roughly flat horizontal red line. Common patterns to look out for if there is a problem are when the line has an obvious curve to look like a hump or several bends to look like an S.\nThe only downside to using diagnostic plots is it takes experience to recognise when there is nothing wrong with a regression model compared to when it violates the assumptions. It is easy to see a little deviation and think there is some drastically wrong. Our advice is if you squint and it looks fine, it is probably fine. You are looking for clear and obvious deviations from what you expect and all the models we use in Chapters 1 to 3 are intentionally fine to develop foundational skills. In Chapter 4 and 5, we then introduce you to more problematic cases and what your options are, and introducing more flexible models.\n\n\n\n\n\n\nError mode\n\n\n\nIf you want to save these plots to add into a report, you might try using ggsave(). However, it will not work as these plots have not been created by ggplot2.\nTo save these plots, you can either right click and choose save as to save on your computer. Alternatively, if they open in the Plots window, you can click on Export and save them as an image or PDF to insert into your documents.\n\n\n\n1.4.3 Checking normality\nPlot 2 is a qq-plot (quantile-quantile plot) and helps us check the normality of the model residuals by showing the standardised residuals on the y-axis and the theoretical quantiles on the x-axis. A common misconception is your variables should all be normally distributed, but it is actually the model residuals which should be normal.\n\nplot(lm_redistribution, \n     which = 2)\n\n\n\n\n\n\n\nIn this plot, you are looking for the data points to roughly follow the dashed line. The idea is there should be a linear relationship between the residuals and the values we would expect under a normal distribution.\nLike the other diagnostic plots, it is tempting to think there are problems where there are none. The vast majority of the points here follow the line nicely, but tail off a little at the extremes. It flags the points with the largest deviations but there do not appear to be any obvious problems.\nWhen there are problems with normality, you are looking for obvious deviations, like the points curving around in a banana shape, or snaking around like an S.\n\n1.4.4 Checking homoscedasticity\nPlot 3 is a scale-location plot and helps us check homoscedasticity by showing the square root of the standardised residuals on the y-axis and the fitted values on the x-axis. Homoscedasticity is where the variance of the residuals is approximately equal across the range of your predictor(s).\n\nplot(lm_redistribution, \n     which = 3)\n\n\n\n\n\n\n\nIn this plot, you are looking out for a roughly random spread of points as you move from one end of the x-axis to the other. The red line should be roughly flat and horizontal.\nWhen there is heteroscedasticity, the characteristic patterns to look out for are a kind of arrow shape where there is a wide spread of points at one end and decreases to a denser range at the other end, or a bow tie where there is a wide spread of points at each end and dense in the middle.\n\n1.4.5 Checking influential cases\nFinally, there are two main plots to help us identify influential cases. You might have heard of the term outlier before and this is one way of classifying data points that are different enough to the rest of the data in a regression model. It is not strictly an assumption of linear regression, but it can affect the other assumptions. Identifying outliers is a complex decision and we will explore your options in the course materials and Chapter 14 on data screening.\nIf you only run plot(lm_redistribution) and cycle through the plots, you do not see this version. This plot shows values of Cook’s distance for each observation in your data along the x-axis. Cook’s distance measures the influence of deleting a given observation, where higher values mean deleting that observation results in a larger change to the model estimates. There are different thresholds in the literature, but estimates range from 1, 0.5, to 4/n. We explore the decision making around this and your options in Chapter 4.\n\nplot(lm_redistribution, \n     which = 4)\n\n\n\n\n\n\n\nFinally, we get a residuals vs leverage plot to show influential cases in a slightly different way. Instead of just the Cook’s distance value of each observation, it plots the standardised residuals against leverage values.\n\nplot(lm_redistribution, \n     which = 5)\n\n\n\n\n\n\n\nInfluential points and potential outliers would have high leverage values and the plot will show a threshold of Cook’s distance as red dashed lines. In this plot, they are not visible as there is no value with a big enough leverage value, but you would be looking for data points outside this threshold to identify influential values.\n\n1.4.6 Checking all the assumptions\nNow we have covered the individual diagnostic plots, there is a handy function called check_model() from the performance package. This function reports all the diagnostic checks from plot(), but tidies up the presentation and has some useful reminders of what you are looking for.\n\ncheck_model(lm_redistribution)\n\n\n\n\n\n\n\nThe key difference is you get a posterior predictive check (essentially comparing values you observe compared to what your model predicts). This will be an idea we return to in a much more important role when we cover Bayesian statistics later in the course, but it’s useful for these models too. The qq-plot for normality of residuals also looks a little different. Instead of a kind of angled line, the residuals are expressed as deviations instead, so the points should be close to a flat horizontal line. This version can make smaller deviations look worse, so keep in mind again you are looking for clear deviations in the overall pattern.\n\n\n\n\n\n\nNote\n\n\n\nThe performance version of the diagnostic plots are actually created using ggplot2, so the function ggsave() would work here if you need to save the plot to add into your report.\n\n\n\n\n\n\n\n\nTry this\n\n\n\nIn activity 7, you should have calculated the relationship between age and support for redistribution for your independent task. Using the model object lm_age, work through assumptions for simple linear regression and make a note of whether you think it meets the assumptions, or there might be any problems. Some of the assumptions you consider what you know about the design, while others you need the diagnostic plots.\n\nThe outcome is interval/ratio level data.\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\nThe predictors have non-zero variance.\nThe relationship between the outcome and predictor is linear.\nThe residuals should be normally distributed.\nThere should be homoscedasticity.\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nThe outcome is interval/ratio level data.\n\nThere is a debate here we cover in the course materials, but there is an argument you can treat the average of multiple Likert items as interval, but you need to be careful.\n\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\n\nOur predictor age is nicely ratio.\n\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\n\nYou need to know this from the design / data collection, but it appears to be the case in this study.\n\nThe predictors have non-zero variance.\n\nThere are a range of ages in the data.\n\nThe relationship between the outcome and predictor is linear.\n\nLooking at the first plot, the red line is pretty flat and horizontal, so we are happy with this.\n\nplot(lm_age, which = 1)\n\n\n\n\n\n\n\n\nThe residuals should be normally distributed.\n\nThe qq-plot is fine for the vast majority of the range. We just have a few deviations in the extreme ends of the x-axis, but this is a byproduct of using a scale score as our outcome as responses cannot go beyond 1-6.\n\nplot(lm_age, which = 2)\n\n\n\n\n\n\n\n\nThere should be homoscedasticity.\n\nThe red line is pretty flat and it looks like there is a fairly even range of values across the x-axis range.\n\nplot(lm_age, which = 3)\n\n\n\n\n\n\n\nAll in all, there do not appear to be any issues with the assumptions here.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#reporting-your-results",
    "href": "01-lm-continuous.html#reporting-your-results",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.5 Reporting your results",
    "text": "1.5 Reporting your results\nNow we have some results to go with, there are a few recommendations on how to communicate that information. In psychology (and other disciplines), we tend to follow the American Psychological Association (APA) formatting guidelines as they provide a comprehensive standardised style to make sure information is being reported as easily digestible and consistent as possible. You can see this PDF online for a little cheat sheet for numbers and statistics, but we will outline some key principles to ensure you provide your reader with enough information.\n\nExplain to the reader what your linear regression model was. For example, what was your outcome and predictor variable?\nReport descriptive statistics to help contextualise your findings. For example, the mean/standard deviation for your outcome and continuous predictor.\nProvide an appropriate data visualisation to help communciate key patterns to the reader. For example, a scatterplot for the relationship between your outcome and predictor.\nReport all three key inferential statistics concepts for the coefficient: the slope, the confidence interval around your slope, and the p-value for hypothesis testing. When you have one predictor in simple linear regression, you typically focus on the slope as your key effect size that helps address your research question and hypothesis. APA style rounds numbers to 2 decimal places when numbers can be bigger than 1, and 3 decimals with no leading zero when it cannot be bigger than 1. When you report the unstandardised slope, you use the symbol \\(b_1\\) but for the standardised slope, you use Beta instead \\(\\beta_1\\).\nProvide an overview of the model fit statistics for whether your model explained a significant amount of variance in your outcome. Remember: the p-value for your model will be the same as for the slope in simple linear regression.\n\nFor our main example, we could summarise the findings as:\n“Our research question was: is there a relationship between support for wealth redistribution and fairness and satisfaction with the current system? To test this, we applied simple linear regression using fairness and satisfaction as a predictor and support for wealth redistribution as our outcome. Figure 1 shows a scatterplot of the relationship.\n\n\n\n\n\n\n\n\nOur model explained a statistically significant amount of variance in our outcome (adjusted \\(R^2\\) = .489, F(1, 303) = 291.70, p &lt; .001). Fairness and satisfaction was a negative predictor, where for every 1-unit increase we expect support for redistribution to decrease by 0.40 (\\(b_1 = -0.40\\), 95% CI = [-0.44, -0.35], p &lt; .001).”\nNote: we have not included an APA formatted Figure title here as it is not easy to format in our book, so refer to the course materials for guidance.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#test-yourself",
    "href": "01-lm-continuous.html#test-yourself",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.6 Test Yourself",
    "text": "1.6 Test Yourself\nTo end the chapter, we have some knowledge check questions to test your understanding of the concepts we covered in the chapter.\nFor this chapter’s knowledge check section, we have another example of linear regression from Dawtry et al. (2015). Feel free to create this model yourself, but we will show you some output and ask you questions based on it.\nFor this model, we focus on the two variables estimated population inequality index (Population_Inequality_Gini_Index) and support for wealth redistribution (redistribution). Check back to the code book in Activity 2 if you need a reminder of what the variables mean.\nQuestion 1. In the scatterplot of the relationship below, this shows a negative relationship between the inequality index and support for redistribution: \nTRUE\nFALSE.\n\n\n\n\n\n\n\n\nQuestion 2 For the next few questions, we have the output from a linear regression model and we would like you to interpret it.\n\n\n\nCall:\nlm(formula = redistribution ~ Population_Inequality_Gini_Index, \n    data = dawtry_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.9478 -0.6384  0.1389  0.8511  2.3854 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      3.097452   0.316914   9.774  &lt; 2e-16 ***\nPopulation_Inequality_Gini_Index 0.022879   0.008734   2.619  0.00925 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.139 on 303 degrees of freedom\nMultiple R-squared:  0.02214,   Adjusted R-squared:  0.01892 \nF-statistic: 6.861 on 1 and 303 DF,  p-value: 0.009251\n\n\nThe outcome variable in this model is \nAttitudes on Wealth Redistribution\nPopulation Inequality Index and the predictor variable is \nAttitudes on Wealth Redistribution\nPopulation Inequality Index.\nQuestion 3 Rounded to 2 decimals, when the predictor is 0, we predict a value of  for our outcome variable.\nQuestion 4 The predictor is \nsignificant\nnon-significant with a p-value of .\nQuestion 5 The predictor is \npositive\nnegative, where we expect for every 1-unit increase in the predictor a  -unit \nincrease\ndecrease in the outcome.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#words-from-this-chapter",
    "href": "01-lm-continuous.html#words-from-this-chapter",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.7 Words from this Chapter",
    "text": "1.7 Words from this Chapter\nBelow you will find a list of words that were used in this chapter that might be new to you in case it helps to have somewhere to refer back to what they mean. The links in this table take you to the entry for the words in the PsyTeachR Glossary. Note that the Glossary is written by numerous members of the team and as such may use slightly different terminology from that shown in the chapter.\n\n\n\nterm\ndefinition\n\n\n\nCentered predictors\nUsually, centering a predictor means subtracting the mean from each value, so the mean is 0. You can then interpret the intercept as the value of your outcome for the mean value of your predictor(s).\n\n\nlist\n\n\n\noutcome\nThe outcome (also known as the dependent variable) is the variable you are interested in seeing a potential change in.\n\n\nPearson\nA standardised measure of the linear relationship between two variables that makes stringent assumptions about the population.\n\n\npredictor\nThe predictor (also known as an independent variable) is the variable you measure or manipulate to see how it is associated with changes in the outcome variable.\n\n\nresiduals\nThe difference between the observed value in the data and the predicted value from the model given its assumptions.\n\n\nSpearman\nA standardised measure of the relationship between two variables that assumes a monotonic - but not necessarily a linear - relationship and makes less stringent assumptions about the population.\n\n\nStandardised predictors\nStandardising involves substracting the variable mean from each value and dividing it by the variable standard deviation. It then has the property of a mean of 0 and standard deviation of 1, so you interpret the units as standard deviations.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "01-lm-continuous.html#end-of-chapter",
    "href": "01-lm-continuous.html#end-of-chapter",
    "title": "\n1  Regression with one continuous predictor\n",
    "section": "\n1.7 End of chapter",
    "text": "1.7 End of chapter\nGreat work, that was your first chapter working on inferential statistics!\nIn the next chapter, we reinforce most of the content by applying simple linear regression to a categorical predictor. This is when you want to test for differences between two groups on your outcome instead of testing the relationship between two continuous variables.\n\n\n\n\nDawtry, R. J., Sutton, R. M., & Sibley, C. G. (2015). Why Wealthier People Think People Are Wealthier, and Why It Matters: From Social Sampling to Attitudes to Redistribution. Psychological Science, 26(9), 1389–1400. https://doi.org/10.1177/0956797615586560",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Regression with one continuous predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html",
    "href": "02-lm-categorical.html",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "",
    "text": "2.1 Chapter preparation",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#chapter-preparation",
    "href": "02-lm-categorical.html#chapter-preparation",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "",
    "text": "2.1.1 Introduction to the data set\nFor most of this chapter, we are using open data from Lopez et al. (2023). The abstract of their article is:\n\nImagine a bowl of soup that never emptied, no matter how many spoonfuls you ate—when and how would you know to stop eating? Satiation can play a role in regulating eating behavior, but research suggests visual cues may be just as important. In a seminal study by Wansink et al. (2005), researchers used self-refilling bowls to assess how visual cues of portion size would influence intake. The study found that participants who unknowingly ate from self-refilling bowls ate more soup than did participants eating from normal (not self-refilling) bowls. Despite consuming 73% more soup, however, participants in the self-refilling condition did not believe they had consumed more soup, nor did they perceive themselves as more satiated than did participants eating from normal bowls. Given recent concerns regarding the validity of research from the Wansink lab, we conducted a preregistered direct replication study of Wansink et al. (2005) with a more highly powered sample (N = 464 vs. 54 in the original study). We found that most results replicated, albeit with half the effect size (d = 0.45 instead of 0.84), with participants in the self-refilling bowl condition eating significantly more soup than those in the control condition. Like the original study, participants in the selfrefilling condition did not believe they had consumed any more soup than participants in the control condition. These results suggest that eating can be strongly controlled by visual cues, which can even override satiation.\n\nIn summary, they replicated an (in)famous experiment that won the Ig-Nobel prize. Participants engaged in a intricate setting (seriously, go and look at the diagrams in the article) where they ate soup from bowls on a table. In the control group, participants could eat as much soup as they wanted and could ask for a top-up from the researchers. In the experimental group, the soup bowls automatically topped up through a series of hidden tubes under the table. The idea behind the control group is they get an accurate visual cue by the soup bowl reducing, and the experimental group get an inaccurate visual cue by the soup bowl seemingly never reducing. So, the inaccurate visual cue would interfere with natural signs of getting full and lead to people eating more.\nIn the original article, participants in the experimental group ate more soup than participants in the control group, but the main author was involved in a series of research misconduct cases. Lopez et al. (2023) wanted to see if the result would replicate in an independent study, so they predicted they would find the same results. In this chapter, we will explore the difference between the control and experimental groups on several variables in their data set.\n\n2.1.2 Organising your files and project for the chapter\nBefore we can get started, you need to organise your files and project for the chapter, so your working directory is in order.\n\nIn your folder for statistics and research design Stats_Research_Design, create a new folder called 02_regression_categorical. Within 02_regression_categorical, create two new folders called data and figures.\nCreate an R Project for 02_regression_categorical as an existing directory for your chapter folder. This should now be your working directory.\nCreate a new R Markdown document and give it a sensible title describing the chapter, such as 02 t-tests and Regression. Save the file in your 02_regression_categorical folder.\nWe are working with a new data set, so please save the following data file: Lopez_2023.csv. Right click the link and select “save link as”, or clicking the link will save the files to your Downloads. Make sure that you save the file as “.csv”. Save or copy the file to your data/ folder within 02_regression_categorical.\n\nYou are now ready to start working on the chapter!\n\n2.1.3 Activity 1 - Read and wrangle the data\nAs the first activity, try and test yourself by completing the following task list to practice your data wrangling skills. In this example, there is not loads to do, you just need to tidy up some variables. Create a final object called lopez_clean to be consistent with the tasks below. If you want to focus on t-tests and regression, then you can just type the code in the solution.\n\n\n\n\n\n\nTry this\n\n\n\nTo wrangle the data, complete the following tasks:\n\n\nLoad the following packages:\n\ntidyverse\neffectsize\nperformance\n\n\nRead the data file data/Lopez_2023.csv to the object name lopez_data.\n\nCreate a new object called lopez_clean based on lopez_data:\n\nModify the variable Condition to turn it into a factor.\nCreate a new variable called Condition_label by recoding Condition. “0” is the “Control” group and “1” is the “Experimental” group.\n\n\n\nYour data should look like this to be ready to analyse:\n\n\nRows: 464\nColumns: 10\n$ ParticipantID      &lt;dbl&gt; 1002, 1004, 1007, 1016, 1018, 1021, 1022, 1024, 102…\n$ Sex                &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, …\n$ Age                &lt;dbl&gt; 18, 19, 19, 21, 20, 20, 21, 21, 19, 20, 21, 20, 21,…\n$ Ethnicity          &lt;dbl&gt; 7, 3, 3, 4, 1, 3, 1, 6, 4, 7, 1, 3, 3, 4, 7, 2, 3, …\n$ OzEstimate         &lt;dbl&gt; 3.0, 2.0, 1.0, 3.0, 5.0, 1.0, 1.0, 3.0, 4.0, 1.0, 4…\n$ CalEstimate        &lt;dbl&gt; 65, 10, 20, 25, 50, 5, 20, 180, 470, 50, 130, 100, …\n$ M_postsoup         &lt;dbl&gt; 3.3, 3.1, 43.4, 5.5, 6.0, 0.8, 3.8, 4.5, 7.9, 8.1, …\n$ F_CaloriesConsumed &lt;dbl&gt; 73.19441, 68.75839, 962.61743, 121.99069, 133.08075…\n$ Condition          &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Condition_label    &lt;chr&gt; \"Control\", \"Control\", \"Control\", \"Control\", \"Contro…\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nYou should have the following in a code chunk:\n\n# load the relevant packages\nlibrary(effectsize)\nlibrary(performance)\nlibrary(tidyverse)\n\n# Read the Lopez_2023.csv file \nlopez_data &lt;- read_csv(\"data/Lopez_2023.csv\")\n\n# turn condition into a factor and recode\nlopez_clean &lt;- lopez_data %&gt;% \n  mutate(Condition = as.factor(Condition),\n         Condition_label = case_match(Condition,\n                                      \"0\" ~ \"Control\",\n                                      \"1\" ~ \"Experimental\"))\n\n\n\n\n\n2.1.4 Activity 2 - Explore the data\n\n\n\n\n\n\nTry this\n\n\n\nAfter the wrangling steps, try and explore lopez_clean to see what variables you are working with. For example, opening the data object as a tab to scroll around, explore with glimpse(), or try plotting some of the individual variables.\n\n\nIn lopez_clean, we have the following variables:\n\n\n\n\n\n\n\nVariable\nType\nDescription\n\n\n\nParticipantID\ndouble\nParticipant ID number.\n\n\nSex\ndouble\nParticipant sex.\n\n\nAge\ndouble\nParticipant age in years.\n\n\nEthnicity\ndouble\nParticipant ethnicity.\n\n\nOzEstimate\ndouble\nEstimated soup consumption in ounces (Oz).\n\n\nCalEstimate\ndouble\nEstimated soup consumption in calories (kcals).\n\n\nM_postsoup\ndouble\nActual soup consumption in ounces (Oz).\n\n\nF_CaloriesConsumed\ndouble\nActual soup consumption in calories (kcals).\n\n\nCondition\ninteger\nCondition labelled numerically as 0 (Control) and 1 (Experimental).\n\n\nCondition_label\ncharacter\nCondition as a direct label: Control and Experimental.\n\n\n\nWe will use this data set to demonstrate t-tests and regression when you have one categorical predictor.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#comparing-differences-using-the-t-test",
    "href": "02-lm-categorical.html#comparing-differences-using-the-t-test",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.2 Comparing differences using the t-test",
    "text": "2.2 Comparing differences using the t-test\nLike correlations are a specific application of the general linear model for the relationship between two continuous variables, t-tests are a specific application for the difference between two groups. Before we demonstrate how you can express this kind of design as a regression model, we cover t-tests so you know how to calculate and interpret them when you come across them in your research.\n\n2.2.1 Activity 3 - Visualising the difference\nTo visualise the difference between two groups, it is useful to create something like a boxplot early for yourself, then provide a more professional looking violin-boxplot to help communicate your results. For most of the demonstrations in this chapter, we will try and answer the research question: “Is there a difference in actual calories consumed between the control and experimental groups?”\n\n\n\n\n\n\nTry this\n\n\n\nUsing your data visualisation skills from Repro Res, recreate the violin-boxplot below using the variables F_CaloriesConsumed and Condition_label from lopez_clean.\n\n\n\n\n\n\n\n\nLooking at the graph, the \nControl\nExperimental group consumed more calories on average.\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe violin-boxplot shows the experimental group who had the biased visual cues consumed more soup in calories than the control group who had the accurate visual cues.\nYou should have the following in a code chunk:\n\nlopez_clean %&gt;% \n  ggplot(aes(y = F_CaloriesConsumed, x = Condition_label, fill = Condition_label)) +\n  geom_violin(alpha = 0.5) + \n  geom_boxplot(width = 0.2, \n               fatten = NULL) + \n  stat_summary(fun = \"mean\", \n               geom = \"point\") +\n  stat_summary(fun.data = \"mean_cl_boot\", # confidence interval\n               geom = \"errorbar\", \n               width = 0.1) +\n  scale_fill_viridis_d(option = \"E\") + \n  scale_y_continuous(name = \"Actual Calories Consumed (kcals)\") +\n  scale_x_discrete(name = \"Study Condition\") + \n  guides(fill = FALSE) + \n  theme_classic()\n\n\n\n\n\n2.2.2 Activity 4 - Using the t.test() function\nA t-test is a specific application of the general linear model. In this test, we express the difference in an outcome between two groups as a kind of standardised mean difference. If you are interested, see the Handy Workbook (McAleer, 2023) for the calculations behind the Student and Welch t-test. Conceptually, a t-test is the difference between two groups divided by the standard error of the difference. There are two main versions of a t-test:\n\nStudent t-testCalculating a t-value based on the mean difference divided by the pooled standard deviation. In the Student t-test, we times the pooled standard deviation by a term containing the sample sizes of each group.\nWelch t-testCalculating a t-value based on the mean difference divided by a term containing the variance of each group. We also correct the degrees of freedom for the difference in variances.\n\nThere is a function built into R to calculate the t-test: t.test(). The function requires:\n\nA formula like lm() where you specify the outcome/dependent variable and the predictor/independent variable in the form outcome ~ predictor.\nThe data set you want to use.\n\nFor our lopez_clean data, we would run the following code for a two-tailed Welch t-test:\n\nt.test(formula = F_CaloriesConsumed ~ Condition_label, \n       data = lopez_clean)\n\n\n    Welch Two Sample t-test\n\ndata:  F_CaloriesConsumed by Condition_label\nt = -4.8578, df = 453.45, p-value = 1.638e-06\nalternative hypothesis: true difference in means between group Control and group Experimental is not equal to 0\n95 percent confidence interval:\n -88.55610 -37.54289\nsample estimates:\n     mean in group Control mean in group Experimental \n                  196.6818                   259.7313 \n\n\nFor the three key concepts of inferential statistics, we get\n\n\nHypothesis testing: p &lt; .001, suggesting we can reject the null hypothesis assuming \\(\\alpha\\) = .05.\n\n\n\n\n\n\n\nWhat does 1.638e-06 mean?\n\n\n\nRemember: R reports very small or very large numbers using scientific notation to save space. We normally report p-values to three decimals, so we report anything smaller as p &lt; .001 to say it is smaller than this.\nIf you want to see the real number, you can use the following function which shows just how small the p-value is:\n\nformat(1.638e-06, scientific = FALSE)\n\n[1] \"0.000001638\"\n\n\n\n\n\n\nEffect size: Somewhat annoyingly, we do not directly get the mean difference between groups as a raw/unstandardised mean difference. We must manually calculate it by subtracting the means of each group (196.6818 - 259.7313 = -63.05). So, those in the experimental group ate on average 63 more calories of soup than the control group.\n\n\n\n\n\n\n\nDoes it matter whether the difference is positive or negative?\n\n\n\nFor effect sizes describing the difference between two groups, it is the absolute difference which is important, providing it is consistent with your predictions (if applicable). If you entered the groups the other way around, the mean difference would become 259.7313 - 196.6818 = 63.05. The same applies when we calculate a standardised mean difference like Cohen’s d later.\n\n\n\n\nConfidence interval: [-88.56, -37.54], although we do not get the mean difference, we get the confidence interval around the mean difference.\n\nTo summarise: A Welch t-test showed participants in the experimental group ate significantly more calories of soup than participants in the control group, t (453.45) = -4.86, p &lt; .001. On average, those in the experimental group ate 63.05 (95% CI = [37.54, 88.56]) more calories than those in the control group.\nWhen you have statistics software like R to do the heavy lifting for you, there is not really a scenario where you would use the Student t-test anymore, but if you did, you can use the var.equal argument to say you assume there are equal variances in each group:\n\nt.test(formula = F_CaloriesConsumed ~ Condition_label, \n       data = lopez_clean, \n       var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  F_CaloriesConsumed by Condition_label\nt = -4.8625, df = 462, p-value = 1.591e-06\nalternative hypothesis: true difference in means between group Control and group Experimental is not equal to 0\n95 percent confidence interval:\n -88.52983 -37.56915\nsample estimates:\n     mean in group Control mean in group Experimental \n                  196.6818                   259.7313 \n\n\nYou can see the main difference between the two versions is the Welch t-test corrects the degrees of freedom, so they are a decimal. While the Student t-test does not correct the degrees of freedom, so they are predictably N - 2.\nTo summarise: A Student t-test showed participants in the experimental group ate significantly more calories of soup than participants in the control group, t (462) = -4.86, p &lt; .001. On average, those in the experimental group ate 63.05 (95% CI = [37.57, 88.53]) more calories than those in the control group.\nOne further useful argument is specifying a one-tailed test if you have a specific prediction. The only downside to using linear models later is there is not a simple argument to apply a one-tailed test.\n\nt.test(formula = F_CaloriesConsumed ~ Condition_label, \n       data = lopez_clean, \n       alternative = \"less\")\n\n\n    Welch Two Sample t-test\n\ndata:  F_CaloriesConsumed by Condition_label\nt = -4.8578, df = 453.45, p-value = 8.188e-07\nalternative hypothesis: true difference in means between group Control and group Experimental is less than 0\n95 percent confidence interval:\n     -Inf -41.6571\nsample estimates:\n     mean in group Control mean in group Experimental \n                  196.6818                   259.7313 \n\n\nThe difference here is specifying the alternative argument. You can use “less” or “greater” depending if you predict a negative (group A &lt; group B) or positive difference (group A &gt; group B).\n\n2.2.3 Activity 5 - Calculating Cohen’s d\nRaw/unstandardised effect sizes are great for putting results in context, particularly when the units are comparable across studies. For our outcome in this study, differences in calories are easy to put in context.\nAlternatively, it can be useful to calculate standardised effect sizes. This helps for power analyses and when you want to compare across comparable studies with slightly different measurement scales.\nThere are different formulas for calculating Cohen’s d, but if you know the t-value and degrees of freedom, you can calculate Cohen’s d through:\n\\(d = \\frac{2t}{\\sqrt{df}} = \\frac{-9.725}{21.49} = -0.45\\)\nNote the different formulas make different assumption and have various rounding errors from the statistics available, so this value will be slightly different to what we calculate shortly.\nIt is important to know the concepts before you use shortcuts, but there is the cohens_d() function from the effectsize package which uses the same format as t.test().\n\ncohens_d(F_CaloriesConsumed ~ Condition_label, \n         data = lopez_clean)\n\n\n\n\nCohens_d\nCI\nCI_low\nCI_high\n\n\n-0.4523004\n0.95\n-0.6366884\n-0.2674345\n\n\n\n\n\n\n\n\n\n\n\nTry this\n\n\n\nGreat work following along so far, but now it is time to test your understanding on a new set of variables. Use the variables CalEstimate and Condition_label from lopez_clean. We can ask the question: “What is the difference in estimated calories consumed between the experimental and control groups?”\n\nCreate a violin-boxplot to visualise the difference between CalEstimate and Condition_label from lopez_clean.\n\nApply the Welch t-test to get your inferential statistics and answer the following questions:\n\nHypothesis testing: Assuming \\(\\alpha\\) = .05, the difference between the experimental and control groups on estimated calories consumed was \nstatistically significant\nnot statistically significant.\nEffect size: Rounded to 2 decimals, the raw effect size was an average difference of  estimates calories between the two groups. Expressed as a standardised effect size, this difference equates to Cohen’s d = .\nConfidence interval: Rounded to 2 decimals, the 95% confidence interval for the mean difference is \n-4.08\n-0.03\n39.85\n0.33 to \n-4.08\n-0.03\n39.85\n0.33. The 95% confidence interval for Cohen’s d is \n-4.08\n-0.03\n39.85\n0.33 to \n-4.08\n-0.03\n39.85\n0.33.\n\n\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe violin-boxplot shows little difference between the two groups on estimated calories consumed.\n\nlopez_clean %&gt;% \n  ggplot(aes(y = CalEstimate, x = Condition_label, fill = Condition_label)) +\n  geom_violin(alpha = 0.5) + \n  geom_boxplot(width = 0.2, \n               fatten = NULL) + \n  stat_summary(fun = \"mean\", \n               geom = \"point\") +\n  stat_summary(fun.data = \"mean_cl_boot\", # confidence interval\n               geom = \"errorbar\", \n               width = 0.1) +\n  scale_fill_viridis_d(option = \"E\") + \n  scale_y_continuous(name = \"Estimated Calories Consumed (kcals)\") +\n  scale_x_discrete(name = \"Study Condition\") + \n  guides(fill = FALSE) + \n  theme_classic()\n\n\n\n\n\n\n\nFor our inferential statistics, a Welch t-test showed the difference is not statistically significant, t (455.06) = 1.60, p = .110.\n\nt.test(formula = CalEstimate ~ Condition_label, \n       data = lopez_clean)\n\n\n    Welch Two Sample t-test\n\ndata:  CalEstimate by Condition_label\nt = 1.6001, df = 455.06, p-value = 0.1103\nalternative hypothesis: true difference in means between group Control and group Experimental is not equal to 0\n95 percent confidence interval:\n -4.080399 39.846433\nsample estimates:\n     mean in group Control mean in group Experimental \n                  133.0328                   115.1498 \n\n\nThe control group estimated they consumed 17.88 (95% CI = [-4.08, 39.85]) more calories than the experimental group, but the difference was not significant. Expressed as a standardised effect size, this equates to Cohen’s d = 0.15 (95% CI = [-0.03, 0.33]).\n\ncohens_d(CalEstimate ~ Condition_label, \n         data = lopez_clean)\n\n\n\n\nCohens_d\nCI\nCI_low\nCI_high\n\n\n0.1490887\n0.95\n-0.0341294\n0.3321449",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#09-categorical",
    "href": "02-lm-categorical.html#09-categorical",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.3 Linear regression with one categorical predictor",
    "text": "2.3 Linear regression with one categorical predictor\nNow you know how to calculate a t-test in R, we can turn to simple linear regression as a more flexible tool for modelling the difference between two groups. As a reminder, there is a chapter in the Handy Workbook (McAleer, 2023) dedicated to manually calculating simple linear regression if you want to work through what the functions are doing behind the scenes.\n\n2.3.1 Activity 6 - Descriptive statistics\nFor all the information we want to include in a report, calculating descriptive statistics is helpful for the reader to show the context behind the results you present. Here, we can report the mean and standard deviation of our outcome per group.\n\nlopez_clean %&gt;% \n  group_by(Condition_label) %&gt;% \n  summarise(mean_cals = round(mean(F_CaloriesConsumed), 2), \n            sd_cals = round(mean(F_CaloriesConsumed), 2))\n\n\n\n\nCondition_label\nmean_cals\nsd_cals\n\n\n\nControl\n196.68\n196.68\n\n\nExperimental\n259.73\n259.73\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want some reinforcement of how these skills apply to published research, look at Table 1 in Lopez et al. (2023). The means and standard deviations here (and Cohen’s d from Activity 5) exactly reproduce the values they report, apart from the SD for the control group (maybe there is a typo in their article).\n\n\n\n2.3.2 Activity 7 - Using the lm() function\nFor our research question of “is there a difference in actual calories consumed between the control and experimental group?”, we can address it with simple linear regression. In this study, we can make causal conclusions as it was an experiment to randomly allocate people into one of two groups, but you can also use regression to compare two self-selecting groups when you cannot make a causal conclusion in isolation. Think carefully about what you can conclude from your design.\nLike Chapter 1, we start by defining our regression model with a formula in the pattern outcome ~ predictor and specify the data frame you want to use. We must then use the summary() function around your model object to get all the statistics you need.\nThere are two ways you can use a categorical predictor. First, we can code groups numerically which people called dummy codingEntering a categorical predictor using two values such as 0 and 1.. You code your first group 0 and you code your second group as 1, which maps on directly to how the regression model works. Let’s look at the output.\n\n# Condition as a factor containing 0 and 1\nlm_cals_numbers &lt;- lm(formula = F_CaloriesConsumed ~ Condition, \n                      data = lopez_clean)\n\nsummary(lm_cals_numbers)\n\n\nCall:\nlm(formula = F_CaloriesConsumed ~ Condition, data = lopez_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-230.90  -99.09  -24.15   62.83  828.04 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  196.682      8.888  22.130  &lt; 2e-16 ***\nCondition1    63.049     12.966   4.863 1.59e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 139.4 on 462 degrees of freedom\nMultiple R-squared:  0.04869,   Adjusted R-squared:  0.04663 \nF-statistic: 23.64 on 1 and 462 DF,  p-value: 1.591e-06\n\n\nCompared to when we had a continuous predictor in Chapter 1, the output is identical. We just need to remember what the key numbers represent. The intercept is the predicted value of your outcome when your predictor is set to 0. When we have two groups coded as 0 and 1, this means the intercept is essentially the mean value of group 0 (here, the control group). We call this the reference groupIn a dummy-coded variable, the first level of your variable, typically 0.. You can confirm this by comparing the intercept estimate 196.68 to the mean value of the control group we calculated in Activity 6.\nThe slope estimate then represents how we predict the outcome to change for every 1-unit increase in the predictor. Since we coded the predictor 0 and 1, this just represents the shift from group 1 to group 2. We call the group we code as 1 the target groupIn a dummy-coded variable, the second level of your variable, typically 1.. You see the target group appended to the variable name, which is Condition1 here. So, for a categorical predictor, the slope represents the mean difference between the reference group (0) and the target group (1): 63.05. In contrast to the t-test, this is our raw/unstandardised effect size for the mean difference we do not need to manually calculate.\n\n\n\n\n\n\nDoes it matter if the slope is positive or negative?\n\n\n\nWhen you have a categorical predictor, the sign is only important for interpreting which group is bigger or smaller. The absolute size is relevant for the effect size where a larger absolute value indicates a larger effect. Whether the slope is positive or negative depends on the order of the groups and which has a larger mean. If the reference is larger than the target, you will get a negative slope. If the target is larger than the reference, you will get a positive slope.\n\n\nLike the continuous predictor, we get values for \\(R^2\\) and adjusted \\(R^2\\), showing we explain .046 (in other words, 4.6%) variance in the outcome through our condition manipulation. We then get the model fit statistics, but with a single predictor, the p-value is identifical to the slope.\nAlternatively, you can use character labels for your categorical predictor and it will still work. This time, we use Condition_label. By default, it will set the order of the reference and target groups alphabetically, but you can manually specify the order by setting factor levels.\n\n# Condition_label as characters\nlm_cals_labels &lt;- lm(formula = F_CaloriesConsumed ~ Condition_label, \n                     data = lopez_clean)\n\nsummary(lm_cals_labels)\n\n\nCall:\nlm(formula = F_CaloriesConsumed ~ Condition_label, data = lopez_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-230.90  -99.09  -24.15   62.83  828.04 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  196.682      8.888  22.130  &lt; 2e-16 ***\nCondition_labelExperimental   63.049     12.966   4.863 1.59e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 139.4 on 462 degrees of freedom\nMultiple R-squared:  0.04869,   Adjusted R-squared:  0.04663 \nF-statistic: 23.64 on 1 and 462 DF,  p-value: 1.591e-06\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you want to test specifying the factor order to see how it affects the output, try running this code prior to the regression model:\n\n# Specify group order of Experimental then Control\nlopez_clean &lt;- lopez_clean %&gt;% \n  mutate(Condition_label = factor(Condition_label, \n                                  levels = c(\"Experimental\", \"Control\")))\n\n\n\n\n\n\n\n\n\nHow are t-tests and regression the same?\n\n\n\n\n\nIf you are interested in the relationship between the two concepts, we said a t-test was a specific application of the general linear model. In the t-test calculations, it expresses the mean difference between groups by the standard error of the difference. In essence, it describes the difference in standard errors, which we can describe with a t-distribution to calculate p-values.\nIn regression, we frame the model as how much variance you explain compared to the total amount of variance. The more variance your predictor explains, the less unexplained variance there is left over. For the slope estimate though, this is identical to the t-test as we estimate the mean difference between groups plus the standard error around the mean difference. We calculate a p-value for the slope from a t-distribution, so you get a t-value in the output.\nYou can see the process is identical by comparing the key values from the regression output to the Student t-test. We can recreate the mean difference to compare to the slope, the t-value is the same, the p-value is the same, the degrees of freedom are the same, and the 95% confidence intervals below are the same.\nSo, when you have a single categorical predictor, it is the exact same process as the Student t-test, just expressed slightly different. The only downside to this procedure is it is much more difficult to recreate the Welch t-test.\n\n\n\n\n2.3.3 Activity 8 - Calculating confidence intervals\nThe only thing we are missing is our confidence intervals around the estimates which we can calculate through the confint() function.\n\nconfint(lm_cals_numbers)\n\n                2.5 %    97.5 %\n(Intercept) 179.21657 214.14704\nCondition1   37.56915  88.52983\n\n\nNow, we can summarise the three key concepts of inferential statistics as:\n\nHypothesis testing: p &lt; .001, suggesting we can reject the null hypothesis assuming \\(\\alpha\\) = .05. The experimental group ate significantly more calories of soup than the control group.\nEffect size: \\(b_1\\) = 63.05, suggesting the experimental group ate on average 63 more calories than the control group.\nConfidence interval: [37.57, 88.53], showing the precision around the slope estimate.\n\n\n\n\n\n\n\nTry this\n\n\n\nNow it is time to test your understanding on a new set of variables. This time, use CalEstimate as your outcome, Condition_label as your predictor, and use lopez_clean as your data. We can ask the same question as Activity 5: “What is the difference in estimated calories consumed between the experimental and control groups?”.\nApply simple linear regression to get your inferential statistics and answer the following questions:\n\nHypothesis testing: Assuming \\(\\alpha\\) = .05, Condition is a \nstatistically significant\nnon-significant predictor of estimates calories consumed.\nEffect size: Rounded to 2 decimals, the Condition slope coefficient means there was a mean difference of \n133.03\n7.68\n-17.88\n11.19.\nConfidence interval: Rounded to 2 decimals, the lower bound of the slope is \n117.94\n-39.88\n148.12\n4.11 and the upper bound is \n117.94\n-39.88\n148.12\n4.11.\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\nThe conclusions are the same as when we calculated the t-test, where condition is not a statistically significant predictor of estimated calories consumed. As a regression model, we get the same conclusions expressed in a slightly different way. Condition is a negative but non-significant predictor (p = .111). The control group ate 17.88 (\\(b_1\\) = -17.88, 95% CI = [-39.88, 4.11]) more calories than the experimental group. We explain very little variance in estimated calories consumed (adjusted \\(R^2\\) = .003), so the condition manipulation had little effect.\n\n# Create lm object for condiiton label as a predictor\nlm_cal_est &lt;- lm(CalEstimate ~ Condition_label, \n                 data = lopez_clean)\n\n# summary of the model object\nsummary(lm_cal_est)\n\n# confidence intervals around estimates\nconfint(lm_cal_est)\n\n\nCall:\nlm(formula = CalEstimate ~ Condition_label, data = lopez_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-130.03  -83.03  -33.03   44.85  666.97 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                  133.033      7.679  17.324   &lt;2e-16 ***\nCondition_labelExperimental  -17.883     11.192  -1.598    0.111    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 119.9 on 459 degrees of freedom\n  (3 observations deleted due to missingness)\nMultiple R-squared:  0.005531,  Adjusted R-squared:  0.003365 \nF-statistic: 2.553 on 1 and 459 DF,  p-value: 0.1108\n\n                                2.5 %     97.5 %\n(Intercept)                 117.94256 148.123015\nCondition_labelExperimental -39.87763   4.111599\n\n\n\n\n\n\n2.3.4 Activity 9 - Standardising predictors\nFor simple linear regression with two levels of a categorical predictor, centering the variable does not help, but we can standardise our outcome to express the estimate in standard deviations rather than the raw units. This is analogous to calculating Cohen’s d as we express the standardised mean difference. In contrast to continuous predictors, we only need to standardise the outcome, rather than both the outcome and predictor(s). We then use the standardised variable as our outcome.\n\n# Be careful with the bracket placement to subtract the mean first\nlopez_clean &lt;-lopez_clean %&gt;% \n  mutate(actual_calories_std = (F_CaloriesConsumed - mean(F_CaloriesConsumed)) / sd(F_CaloriesConsumed))\n\n# Condition as a factor containing 0 and 1\nlm_cals_std &lt;- lm(formula = actual_calories_std ~ Condition, \n                      data = lopez_clean)\n\nsummary(lm_cals_std)\n\n\nCall:\nlm(formula = actual_calories_std ~ Condition, data = lopez_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6173 -0.6941 -0.1692  0.4401  5.8000 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.20749    0.06225  -3.333 0.000928 ***\nCondition1   0.44163    0.09082   4.863 1.59e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9764 on 462 degrees of freedom\nMultiple R-squared:  0.04869,   Adjusted R-squared:  0.04663 \nF-statistic: 23.64 on 1 and 462 DF,  p-value: 1.591e-06\n\n\nNote, the estimate may be slightly different to directly calculating Cohen’s d as there are a few formulas. If you compare to Activity 5, we got d = 0.45 there and 0.44 here. Between the estimate and 95% confidence intervals, they are off by .02, so it does not have a material impact on the results.\n\n\n\n\n\n\nTip\n\n\n\nAs before, once you know how it works conceptually, there is a shortcut where you do not need to standardise all your variables first. The effectsize package has a handy function called standardize_parameters() which you can apply to your initial lm() object.\n\nstandardize_parameters(lm_cals_numbers)\n\n\n\n\nParameter\nStd_Coefficient\nCI\nCI_low\nCI_high\n\n\n\n(Intercept)\n-0.2074898\n0.95\n-0.3298249\n-0.0851547\n\n\nCondition1\n0.4416297\n0.95\n0.2631529\n0.6201065",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#checking-assumptions",
    "href": "02-lm-categorical.html#checking-assumptions",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.4 Checking assumptions",
    "text": "2.4 Checking assumptions\nFor the inferential statistics to work as intended, the model makes certain assumptions about the data you are putting into it and the accuracy of the inferences depends on how sensible these assumption are.\n\n2.4.1 Activity 10 - Diagnostic plots for linear regression\nWe have the same assumptions for simple linear regression now we have a categorical predictor:\n\nThe outcome is interval/ratio level data.\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\nThe predictors have non-zero variance.\nThe relationship between the outcome and predictor is linear.\nThe residuals should be normally distributed.\nThere should be homoscedasticity.\n\nAssumptions 1-4 are pretty straight forward as they relate to your understanding of the design or a simple check on the data for non-zero variance (the responses are not all the exact same value).\nAssumptions 5-7 require diagnostic checks on the residuals from the model. In contrast to continuous predictors, they are a little harder to identify patterns in. As we only have two values on the x-axis (0 and 1), all the residuals will be organised into vertical lines and the trend lines to help spot patterns do not look quite right.\n\n2.4.2 Checking linearity\nWhen you have a categorical predictor with two levels, you meet linearity by default, so you do not need to check this assumption directly. When you have two levels, you can only fit a straight line between the values.\n\nplot(lm_cals_numbers, \n     which = 1)\n\n\n\n\n\n\n\n\n2.4.3 Checking normality\nThe qq-plot is still the same to interpret. Now, this example presents a useful case of decision making in data analysis we explore more in Chapter 4. The plot here is approaching signs of violating normality as there is a clear curve to the data points with 3 and 118 the largest deviations (you can see these on the violin-boxplot as the two highest values in the control group). For this chapter, we are sticking with it and it would be consistent with how the original authors analysed the data, but note this would be a key decision to make and justify when reporting the analysis.\n\nplot(lm_cals_numbers, \n     which = 2)\n\n\n\n\n\n\n\n\n2.4.4 Checking homoscedasticity\nThe scale-location plot is harder to interpret when you have a categorical predictor. Like linearity, the points are all arranged in two vertical lines as we only have two levels. You are looking out for the spread of the two lines to be roughly similar. They look fine here, just points 118 and 3 separated a little from the other points.\n\nplot(lm_cals_numbers, \n     which = 3)\n\n\n\n\n\n\n\n\n2.4.5 Checking influential cases\nFinally, we have our plots for identifying influential cases. First, we get Cook’s distance for all the observations in your data. We see points 3 and 118 come up yet again, but although they are the largest deviations, they do not necessarily have worrying Cook’s distance values. There are different thresholds in the literature, but estimates range from 1, 0.5, to 4/n. It would only be under this final most conservative estimate (0.009) we would highlight several observations for further inspection.\n\nplot(lm_cals_numbers, \n     which = 4)\n\n\n\n\n\n\n\nFinally, the fifth plot shows residual values against leverage. Like Chapter 1, we cannot see the Cook’s distance threshold it uses in the plot as none of the points are a large enough deviation, despite 3 and 188 sticking out again.\n\nplot(lm_cals_numbers, \n     which = 5)\n\n\n\n\n\n\n\n\n2.4.6 Checking all the assumptions\nNow we have covered the individual diagnostic plots, there is a handy function called check_model() from the performance package. Like the plot() function, the output for linearity, homoscedasticity, and influential observations does not plot right as we only have two values for the predictor and the plot lines do not really work. Do not worry, you have not done anything wrong.\n\ncheck_model(lm_cals_numbers)\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat might explain the funky normality?\n\n\n\nThe posterior predictive check here provides an insight into why we get potential problems with normality. The outcome is ratio but cannot be smaller than 0 as we cannot have negative calories. So, in the green line for the observed data, the data are a little skewed as it drops off prior to 0. However, the model does not know that and it happily predicts normally distributed values which go below 0, creating a mismatch between what the model predicts and the values we enter into it.\nRemember the models will work regardless of the data you put into them, it is important to keep your role in mind to recognise when you need to be cautious about what you can learn from the model. This is what we will explore in Chapters 4 and 5 as we consider data screening and alternative model distributions.\n\n\n\n\n\n\n\n\nTry this\n\n\n\nIn activity 8, you should have calculated the effect of condition (Condition_label) on estimated calories consumed (CalEstimate) for your independent task. Using the model object lm_cal_est, work through assumptions for simple linear regression and make a note of whether you think it meets the assumptions, or there might be any problems. Some of the assumptions you consider what you know about the design, while others you need the diagnostic plots.\n\nThe outcome is interval/ratio level data.\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\nThe predictors have non-zero variance.\nThe relationship between the outcome and predictor is linear.\nThe residuals should be normally distributed.\nThere should be homoscedasticity.\n\n\n\n\n\n\n\n\n\nShow me the solution\n\n\n\n\n\n\nThe outcome is interval/ratio level data.\n\nThe outcome is nicely ratio as estimated calories have a logical 0 point (no calories) and the units are in equal measurements..\n\nThe predictor variable is interval/ratio or categorical (with two levels at a time).\n\nOur predictor is categorical with two levels.\n\nAll values of the outcome variable are independent (i.e., each score should come from a different participant/observation).\n\nThe short answer is this appears to be the case in this study. The longer answer is there might have been an issue with the participants apparently completing the study in groups of 3. It is not entirely clear in the data how they recorded this, but grouped data collection does present a potential problem with independence which you will not learn about until the final lecture on mixed effects models and the original authors did not seem to address it.\n\nThe predictors have non-zero variance.\n\nWe have observations from both levels of the predictor.\n\nThe relationship between the outcome and predictor is linear.\n\nWe meet linearity by default with two levels, so we do not need the plot here.\n\nThe residuals should be normally distributed.\n\nLike the actual calories consumed, this is firmly a clear deviation from what we expect and provides a good example of when it does not look right. If we were to analyse the data fully, we would explore the impact of this and alternative models, but for this chapter, we are going to note our concern and remember the authors analysed the data like this.\n\nplot(lm_cal_est, \n     which = 2)\n\n\n\n\n\n\n\n\nThere should be homoscedasticity.\n\nLooking at the spread of each group, it looks fine with a similar range until both groups are more sparsely distributed above 1.\n\nplot(lm_cal_est, \n     which = 3)\n\n\n\n\n\n\n\nIn summary, normality is a clear concern and something we will return to for your options in Chapter 4 and the course materials. For now, we will stick with the model but recognise we should be cautious.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#reporting-your-results",
    "href": "02-lm-categorical.html#reporting-your-results",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.5 Reporting your results",
    "text": "2.5 Reporting your results\nNow we have some results to go with, there are a few recommendations on how to communicate that information. If you need a reminder of APA style for formatting numbers, you can see this PDF online for a little cheat sheet for numbers and statistics.\n\nExplain to the reader what your linear regression model was. For example, what was your outcome and predictor variable?\nReport descriptive statistics to help contextualise your findings. For example, the mean/standard deviation for your outcome per group.\nProvide an appropriate data visualisation to help communciate key patterns to the reader. For example, a violin-boxplot for how each group responded on your outcome.\nReport all three key inferential statistics concepts for the coefficient: the slope, the confidence interval around your slope, and the p-value for hypothesis testing. When you have one predictor in simple linear regression, you typically focus on the slope as your key effect size that helps address your research question and hypothesis. APA style rounds numbers to 2 decimal places when numbers can be bigger than 1, and 3 decimals with no leading zero when it cannot be bigger than 1. When you report the unstandardised slope, you use the symbol \\(b_1\\) but for the standardised slope, you use Beta instead \\(\\beta_1\\).\nProvide an overview of the model fit statistics for whether your model explained a significant amount of variance in your outcome. Remember: the p-value for your model will be the same as for the slope in simple linear regression.\n\nFor our main example, we could summarise the findings as:\n“Our research question was: is there a difference in actual calories consumed between the control and experimental group? To test this, we applied simple linear regression using condition as a predictor with two levels (control and experimental) and actual calories consumed as our outcome. Figure 1 shows a violin-boxplot of the difference between the control and experimental groups.\n\n\n\n\n\n\n\n\nOur model explained a statistically significant amount of variance in our outcome (adjusted \\(R^2\\) = .047, F(1, 462) = 23.64, p &lt; .001). Condition was a positive predictor, where the experimental group consumed on average 63 more calories than the control group (\\(b_1\\) = 63.05, 95% CI = [37.57, 88.53], p &lt; .001). Expressed as a standardised effect size, the experimental group consumed 0.44 (95% CI = [0.26, 0.62]) more standard deviations than the control group.”\nNote: we have not included an APA formatted Figure title here as it is not easy to format in our book, so refer to the course materials for guidance.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#09-bonus",
    "href": "02-lm-categorical.html#09-bonus",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.6 One- and paired-samples tests",
    "text": "2.6 One- and paired-samples tests\nYou might find yourself in a situation where you want to test a continuous variable against a fixed value or compare conditions in the same participants. In experimental designs, you might have multiple conditions from the same participants which causes problems if you analyse the data as if they were groups as above. You will learn a more flexible approach in the final lecture on mixed effects models but for simple cases when you have two conditions, the following approach will work.\nFor this demonstration, we will use data from experiment 1 of Bem (2011), an (in)famous study that almost single-handedly started the replication crisis in psychology. Briefly, participants completed a computer task adapted from priming experiments where they could select one of two windows. They had to guess which window had an image hidden behind it and the images contained different stimuli like erotic or neutral/control images. Across many trials of the participants guessing the location, Bem calculated the proportion of successful trials which could range between 0 (never correct), 50 (50%, chance), and 100 (always correct). The headline finding was participants demonstrated precognition - or the ability to see into the future - to guess above chance levels, but what does the data look like?\nWe are working with a new data set, so please save the following data file: Bem_2011.csv. Right click the link and select “save link as”, or clicking the link will save the files to your Downloads. Make sure that you save the file as “.csv”. Save or copy the file to your data/ folder within 02_regression_categorical. Read in the data file to the object bem_data to be consistent with the tasks below.\n\n2.6.1 Activity 11 - One-sample comparing against a fixed value\nThere are scenarios where you want to compare a single continuous variable against a fixed value. For example, do your participants respond significantly above or below chance?\n\n2.6.1.1 Expressed as a t-test\nAs a t-test, we need to specify two arguments:\n\nx - This is the continuous variable you want to analyse and compare the mean value of. We must use the base R operator $ to specify the column from your data.\nmu - This is the fixed value you want to test your variable against.\n\nIn this scenario, we want to compare the hit rate to erotic images against a value of 50. This will tell us if the hit rate is significantly above or below chance.\n\nt.test(x = bem_data$Erotic.Hits.PC,\n       mu = 50)\n\n\n    One Sample t-test\n\ndata:  bem_data$Erotic.Hits.PC\nt = 2.5133, df = 99, p-value = 0.01358\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 50.66075 55.61703\nsample estimates:\nmean of x \n 53.13889 \n\n\nThe output is similar to the independent samples t-test. We get the p-value for hypothesis testing, the mean estimate of the variable, and it’s 95% confidence interval. To express it as an effect size, you can subtract 50 from each value. So, participants responded 3.14% above chance - statistically significant, but hardly convincing evidence for precognition.\n\n2.6.1.2 Expressed as a linear model\nWe can also express this as a linear model, but we must first add a small wrangling step. In the one-sample t-test, we can manually enter a fixed value to compare the mean against. In a linear model, we must compare against 0 by subtracting the fixed value from your variable. So, we subtract 50 from all the observations, so they become a kind of deviation from 50.\n\nbem_data &lt;- bem_data %&gt;% \n  mutate(erotic_deviation = Erotic.Hits.PC - 50)\n\nIn contrast to previous linear models, we only add a fixed intercept and do not add a predictor. This recreates the one-sample t-test by estimating the mean value of your outcome.\n\nlm_erotic &lt;- lm(erotic_deviation ~ 1, \n                data = bem_data)\n\nsummary(lm_erotic)\n\n\nCall:\nlm(formula = erotic_deviation ~ 1, data = bem_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-28.139  -8.694   2.417   7.972  30.194 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    3.139      1.249   2.513   0.0136 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 12.49 on 99 degrees of freedom\n\n\nThis process has the benefit of directly producing your effect size, as the intercept estimate is the deviation from your fixed value (here, 50). As we calculated manually before, the erotic hit rate is 3.14% above your fixed value. If you remember back to the linear model explanations, this is where the p-value for the intercept is finally useful as it tests against 0.\nIf you compare to the one-sample t-test, all of these values are identical. You also have the option of calculating confidence intervals around the estimate, calculating a standardised effect size, and checking assumptions by applying the previous linear regression activities.\n\n2.6.2 Activity 12 - Paired-samples comparing conditions\nAlternatively, you might want to compare two conditions from the same participants in paired samples/within-subjects design. For example, is the hit-rate significantly higher for erotic images compared to control images?\n\n2.6.2.1 Expressed as a t-test\nTo conduct a paired-samples t-test, we must specify three arguments:\n\nx - This is the first level of your condition as a column. You need your data in wide format, so the condition levels are spread across two columns per participant. We must use the base R operator $ to specify the column from your data.\ny - This is the second level of your condition as a column.\npaired - This instructs R you want a paired-samples t-test to compare conditions within participants.\n\nIn this scenario, we want to compare the hit rate for erotic images to control images.\n\nt.test(x = bem_data$Erotic.Hits.PC,\n       y = bem_data$Control.Hits.PC, \n       paired = TRUE)\n\n\n    Paired t-test\n\ndata:  bem_data$Erotic.Hits.PC and bem_data$Control.Hits.PC\nt = 1.8563, df = 99, p-value = 0.06638\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.2277431  6.8388542\nsample estimates:\nmean difference \n       3.305556 \n\n\nThe output is almost identical to the one-sample t-test, but this time the effect size is the mean difference between conditions, not just the mean per condition. Behind the scenes, a paired-samples t-test is actually a one-sample t-test in disguise as it uses the difference between conditions as the outcome.\nAs an aside, Bem (2011) reported a significant difference here, but only because he reported a one-tailed test (alternative = \"greater\"). This is an example where you ideally need a strong (ideally pre-registered) prediction as it makes a material impact on the inferences you would make.\n\n2.6.2.2 Expressed as a linear model\nFinally, we can express a paired-samples t-test as a linear model. We must apply a small data wrangling step to calculate a difference score between conditions. This is so the linear model compares the estimate against 0 of no difference. So, for this example, we create a variable for the difference between erotic and control images.\n\nbem_data &lt;- bem_data %&gt;% \n  mutate(erotic_control = Erotic.Hits.PC - Control.Hits.PC)\n\nLike the one-sample scenario, we only add a fixed intercept for the new difference variance and do not add a predictor. This recreates the paired-samples t-test by estimating the mean value of your difference score.\n\nlm_paired &lt;- lm(erotic_control ~ 1, \n                data = bem_data)\n\nsummary(lm_paired)\n\n\nCall:\nlm(formula = erotic_control ~ 1, data = bem_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-58.861  -9.556   2.250   9.194  35.583 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)    3.306      1.781   1.856   0.0664 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.81 on 99 degrees of freedom\n\n\nThis process directly produces your effect size again, as the intercept estimate is the deviation from 0 for your difference score. As we saw in the paired-samples t-test output, there was a 3.31% higher hit rate for erotic images compared to control (as we calculated erotic - control).\nIf you compare to the paired-samples t-test, all of these values are identical. You also have the option of calculating confidence intervals around the estimate, calculating a standardised effect size, and checking assumptions by applying the previous linear regression activities.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#09-test",
    "href": "02-lm-categorical.html#09-test",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.7 Test Yourself",
    "text": "2.7 Test Yourself\nFor this chapter’s knowledge check section, we have something a little different. Instead of purely conceptual questions about functions, we have another example of linear regression from Lopez et al. (2023). Feel free to create this model yourself, but we will show you some output and ask you questions based on it.\nFor this model, we focus on ounces of soup consumed (M_postsoup) rather than calories by each Condition group (Condition_label). You might have a good idea about the results based on the chapter, but you will still need to interpret the output accurately.\nQuestion 1. In the violin-boxplot below, the experimental group consumed more soup in ounces than the control group: \nTRUE\nFALSE.\n\n\n\n\n\n\n\n\nQuestion 2 For the next few questions, we have the output from a linear regression model and we would like you to interpret it.\n\n\n\nCall:\nlm(formula = M_postsoup ~ Condition_label, data = lopez_clean)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-10.410  -4.467  -1.089   2.833  37.333 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   8.8675     0.4007  22.130  &lt; 2e-16 ***\nCondition_labelExperimental   2.8426     0.5846   4.863 1.59e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.285 on 462 degrees of freedom\nMultiple R-squared:  0.04869,   Adjusted R-squared:  0.04663 \nF-statistic: 23.64 on 1 and 462 DF,  p-value: 1.591e-06\n\n\nThe outcome variable in this model is \nActual ounces of soup consumed\nExperimental condition and the predictor variable is \nActual ounces of soup consumed\nExperimental condition.\nQuestion 3 In this model, the reference group is \nControl\nExperimental and the target group is \nControl\nExperimental\nQuestion 4 Rounded to 2 decimals, we predict a value of  for our reference group.\nQuestion 5 The predictor is \nstatistically significant\nnon-significant and \npositive\nnegative.\nQuestion 6 The target group consumed on average  ounces \nless\nmore soup than the reference group.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#words-from-this-chapter",
    "href": "02-lm-categorical.html#words-from-this-chapter",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.8 Words from this Chapter",
    "text": "2.8 Words from this Chapter\nBelow you will find a list of words that were used in this chapter that might be new to you in case it helps to have somewhere to refer back to what they mean. The links in this table take you to the entry for the words in the PsyTeachR Glossary. Note that the Glossary is written by numerous members of the team and as such may use slightly different terminology from that shown in the chapter.\n\n\n\nterm\ndefinition\n\n\n\ndummy coding\nEntering a categorical predictor using two values such as 0 and 1.\n\n\nreference group\nIn a dummy-coded variable, the first level of your variable, typically 0.\n\n\nStudent t-test\nCalculating a t-value based on the mean difference divided by the pooled standard deviation. In the Student t-test, we times the pooled standard deviation by a term containing the sample sizes of each group.\n\n\ntarget group\nIn a dummy-coded variable, the second level of your variable, typically 1.\n\n\nWelch t-test\nCalculating a t-value based on the mean difference divided by a term containing the variance of each group. We also correct the degrees of freedom for the difference in variances.\n\n\n\n\n\n\n\n\nBem, D. J. (2011). Feeling the future: Experimental evidence for anomalous retroactive influences on cognition and affect. Journal of Personality and Social Psychology, 100(3), 407–425. https://doi.org/10.1037/a0021524\n\n\nLopez, A., Choi, A. K., Dellawar, N. C., Cullen, B. C., Avila Contreras, S., Rosenfeld, D. L., & Tomiyama, A. J. (2023). Visual cues and food intake: A preregistered replication of Wansink et al (2005). Journal of Experimental Psychology: General. https://doi.org/10.1037/xge0001503.supp",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  },
  {
    "objectID": "02-lm-categorical.html#end-of-chapter",
    "href": "02-lm-categorical.html#end-of-chapter",
    "title": "\n2  Regression with one categorical predictor\n",
    "section": "\n2.9 End of chapter",
    "text": "2.9 End of chapter\nWell done, you have now completed the first core set of chapters for inferential statistics!\nAt this point, you can now address a range of research questions by applying variations of the general linear model. As a researcher, the most important thing is starting with your research question (and where applicable, your hypothesis), designing a study to address that research question, and using an appropriate statistical model for your design and research question. But, before you can identify an appropriate statistical model, you need to know what they look like! This is everything we cover in Research Methods 1 to focus on a select range of foundational skills. You will then build on these modelling techniques in Chapters 12-14 for Research Methods 2.\nYou are now ready to complete the second data analysis journey chapter: Simple Linear Regression. This is where you can test your new skills in a slightly less structured way, from wrangling data, to answering a research question.\nIn the next core chapter, we turn to statistical power and work on how you can conduct a power analysis in R/RStudio.\n\n\n\n\nBem, D. J. (2011). Feeling the future: Experimental evidence for anomalous retroactive influences on cognition and affect. Journal of Personality and Social Psychology, 100(3), 407–425. https://doi.org/10.1037/a0021524\n\n\nLopez, A., Choi, A. K., Dellawar, N. C., Cullen, B. C., Avila Contreras, S., Rosenfeld, D. L., & Tomiyama, A. J. (2023). Visual cues and food intake: A preregistered replication of Wansink et al (2005). Journal of Experimental Psychology: General. https://doi.org/10.1037/xge0001503.supp",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Regression with one categorical predictor</span>"
    ]
  }
]