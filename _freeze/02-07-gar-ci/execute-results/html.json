{
  "hash": "23b272163223d4c59a910a512cb0c060",
  "result": {
    "engine": "knitr",
    "markdown": "# Confidence Intervals\n\n\n\n\n\n\n\n\n\n## Chapter preparation\n\nIn this chapter, we need a few different datasets and source files, so we will introduce them as we need them.\n\n### Organising your files and project for the chapter\n\nBefore we can get started, you need to organise your files and project for the chapter, so your working directory is in order.\n\n1. In your folder for statistics and research design `Stats_Research_Design`, create a new folder called `07_confidence_intervals`. \n\n2. We are working with a few data sets and source files in this chapter, so please save the following zip file containing all the files you need: [Chapter 07 Files](data/07_files.zip). Right click the link and select \"save link as\", or clicking the link will save the file to your Downloads. Extra the files and save the two folders (`data` and `code`) in your `07_confidence_intervals` folder. All the code in this chapter assumes these two folders with all their files inside are in the same folder as your Quarto document. \n\n3. Create an R Project for `07_confidence_intervals` as an existing directory for your chapter folder. This should now be your working directory.\n\n4. Create a new Quarto document and give it a sensible title describing the chapter, such as `07 Confidence Intervals`. Save the file in your `07_confidence_intervals` folder. \n\nYou are now ready to start working on the chapter! \n\n## Dependencies\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(tibble)\nsource(\"code/theme_gar.txt\")\nsource(\"code/Rallfun-v35-light.txt\")\nlibrary(beepr) # for little auditory rewards :)\n```\n:::\n\n\n\n\n## Rand Wilcox's code\n\nWhen I mention code from Rand Wilcox, it is available in a giant text file (see Wilcox's [website](https://dornsife.usc.edu/labs/rwilcox/software/) for the most recent version of the code). You access the functions by using the `source()` function:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"code/Rallfun-v35.txt\")\n```\n:::\n\n\n\n\nFor convenience, most of the functions necessary for this course are available in the smaller file `Rallfun-v35-light.txt`. In this version, I've removed the option `SEED=TRUE`, which allows users to set the random seed inside the function, which means you get the same results every time you use the function. With `SEED=FALSE`, different random bootstrap samples are returned each time the function is called. It is better practice to set the seed in your R notebook, for reproducibility and transparency. So be careful to use `SEED=FALSE` when using functions from `Rallfun-v35.txt`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsource(\"code/Rallfun-v35-light.txt\")\n```\n:::\n\n\n\n\nIn some of the notebooks, I've extracted one or a few functions so you can source a smaller file. This prevents your environment from being cluttered with functions you don't need. Creating a well documented R package is a lot of work, so some statisticians don't spend the time required. But that means their code is less accessible.\n\nSome of the functions are also available in the `WRS2` package available on [CRAN](https://cran.r-project.org/web/packages/WRS2/index.html). For correlation analyses, I have created an R package called [`bootcorci`](https://github.com/GRousselet/bootcorci). To quantify how distributions differ using quantile estimation, I have created the [`rogme`](https://github.com/GRousselet/rogme) R package.\n\n## P value sampling distribution\n\nConsider normality only.\n\n### No effect\n\nHow are p values distributed under the null?\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\n\nalpha.val <- 0.05\nnsim <- 10000\nn <- 20\np.sampdist.h0 <- vector(mode = \"numeric\", length = nsim)\np.sampdist.h1 <- vector(mode = \"numeric\", length = nsim)\nes <- 0.5 # effect size to add to each sample\n\n# get sampling distribution of t values\nfor(S in 1:nsim){\n  p.sampdist.h0[S] <- t.test(rnorm(n))$p.value\n  p.sampdist.h1[S] <- t.test(rnorm(n) + es)$p.value\n}\n# out <- density(p.sampdist.h0)\n# samp.dens <- tibble(x = out$x,\n#                     y = out$y)\n\n# ggplot(df, aes(x = x, y = y)) + theme_linedraw() +\ndf <- tibble(x = p.sampdist.h0)\nggplot(df, aes(x = x)) + theme_linedraw() +\n  geom_histogram(breaks = seq(0, 1, 0.05), fill=\"grey90\", colour = \"black\") +\n  theme(axis.text = element_text(size = 14),\n        axis.title = element_text(size = 16),\n        plot.title = element_text(size=20)) +\n  ggtitle(paste0(\"Distribution of p values under the null\")) +\n  labs(x = \"p values\", y = \"Frequency\") +\n  coord_cartesian(xlim = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](02-07-gar-ci_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n### Effect\n\nNow we look at the distribution of p values when there is a 0.5 effect. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- tibble(x = p.sampdist.h1)\nggplot(df, aes(x = x)) + theme_linedraw() +\n  geom_histogram(breaks = seq(0, 1, 0.05), fill=\"grey90\", colour = \"black\") +\n  theme(axis.text = element_text(size = 14),\n        axis.title = element_text(size = 16),\n        plot.title = element_text(size=20)) +\n  ggtitle(paste0(\"Distribution of p values when there is an effect\")) +\n  labs(x = \"p values\", y = \"Frequency\") +\n  coord_cartesian(xlim = c(0, 1))\n```\n\n::: {.cell-output-display}\n![](02-07-gar-ci_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n## Confidence intervals: sampling from normal distribution\n\nWe sample from a standard normal population (mean 0 and standard deviation 1).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npop.m <- 0 # population mean\npop.sd <- 1 # population sd\n\nx <- seq(-6, 6, 0.1)\ny <- dnorm(x, pop.m, pop.sd)\n\ndf <- tibble(x = x,\n             y = y)\n\nggplot(df, aes(x = x, y = y)) + theme_linedraw() +\n  geom_line() +\n  geom_vline(xintercept = pop.m, linetype = \"dashed\") +\n  theme(axis.text = element_text(size = 14),\n        axis.title = element_text(size = 18),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) +\n  xlab(\"Population values\") +\n  ylab(\"Density\") +\n  coord_cartesian(xlim = c(-5, 5))\n```\n\n::: {.cell-output-display}\n![](02-07-gar-ci_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# ggsave(filename = './figures/nb2_norm_pop.pdf', width = 7, height = 5)\n```\n:::\n\n\n\n\n### 20 experiments - 20 confidence intervals\n\nWe perform many experiments, and for each experiment we compute a parametric one-sample confidence interval, which we plot as horizontal lines.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(666) # set seed of random number generator\nalpha.val <- .1 # 90% confidence interval\n\nnexp <- 20 # number of experiments\nn <- 20 # sample size in each experiment\n\n#declare matrices to save results\nmean.all <- vector(mode = \"numeric\", length = nexp) # save sample means\nci.ttest <- matrix(NA, nrow = nexp, ncol = 2) # save confidence intervals\nci.ttest.in <- vector(mode = \"numeric\", length = nexp) # save coverage\nsamp.all <- matrix(NA, nrow = nexp, ncol = n) # save samples\n\nfor(E in 1:nexp){\n  # sample data\n  samp <- rnorm(n, pop.m, pop.sd) \n  mean.all[E] <- mean(samp) # save  mean of each sample\n  samp.all[E,] <- samp # save sample\n  ci.ttest[E,] <- t.test(samp, conf.level = 1-alpha.val)$conf.int\n  # check if the confidence interval includes the population value\n  if(ci.ttest[E,1] > pop.m || ci.ttest[E,2] < pop.m){\n    ci.ttest.in[E] <- 0  \n  } else{\n    ci.ttest.in[E] <- 1  \n  }\n  \n}\n```\n:::\n\n\n\n\n#### Illustrate results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# confidence intervals\ndf <- tibble(x = as.vector(ci.ttest),\n             y = rep(1:nexp,2),\n             gr = factor(rep(1:nexp,2)),\n             inout = factor(rep(ci.ttest.in,2))\n)\n\n# sample means\ndf2 <- tibble(x = mean.all,\n              y = 1:nexp,\n              inout = factor(ci.ttest.in))\n\n# samples\ndf3 <- tibble(x = as.vector(samp.all),\n              y = rep(1:nexp + 0.15, n)\n)\n\nggplot(df, aes(x = x, y = y)) + theme_linedraw() +\n  geom_line(aes(group = gr, colour = inout), linewidth = 0.8, lineend = \"round\", show.legend = FALSE) + # CI\n  geom_point(data = df3, size=0.3, alpha = 0.5) + # samples\n  geom_point(data = df2, aes(colour = inout), show.legend = FALSE) + # sample means\n  scale_colour_manual(values = c(\"darkgrey\", \"black\")) +\n  geom_vline(xintercept = pop.m, colour = \"black\", linetype = \"dashed\", linewidth = 0.2) +\n  labs(x = \"Values\", y = \"Experiments\") +\n  theme(axis.title = element_text(size = 16),\n        axis.text = element_text(size = 14)) +\n  scale_y_continuous(minor_breaks = 1:20)\n```\n\n::: {.cell-output-display}\n![](02-07-gar-ci_files/figure-html/unnamed-chunk-9-1.png){width=288}\n:::\n\n```{.r .cell-code}\n# ggsave(filename = './figures/nb2_20expt_ttci_norm.pdf', width = 5, height = 7)\n```\n:::\n\n\n\n\n### 20,000 experiments - 20,000 confidence intervals\n\nNow we do 20,000 experiments, and check the long term coverage of the confidence intervals. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21) # set seed of random number generator\nalpha.val <- .1 # 90% confidence interval\n\nnexp <- 20000 # number of experiments\nn <- 20 # sample size in each experiment\n\n#declare matrices to save results\nci.ttest <- matrix(NA, nrow = nexp, ncol = 2)\nci.ttest.in <- vector(mode = \"numeric\", length = nexp)\nmean.all <- vector(mode = \"numeric\", length = nexp)\n\nfor(E in 1:nexp){\n  # sample data \n  samp <- rnorm(n, pop.m, pop.sd)\n  # mean.all[E] <- mean(samp) # sample means\n  ci.ttest[E,] <- t.test(samp, conf.level = 1-alpha.val)$conf.int\n  # check if the confidence interval includes the population value\n  if(ci.ttest[E,1] > pop.m || ci.ttest[E,2] < pop.m){\n    ci.ttest.in[E] <- 0  \n  } else{\n    ci.ttest.in[E] <- 1  \n  }\n  \n}\n```\n:::\n\n\n\n\nConfidence interval coverage = 90.4%.\n\n#### Illustrate CIs excluding population\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci <- ci.ttest[ci.ttest.in==0, ]\ntodo <- sum(ci.ttest.in==0)\ndf <- tibble(x = as.vector(ci),\n             y = rep(1:todo, 2),\n             gr = factor(rep(1:todo,2))\n)\n\nggplot(df, aes(x = x, y = y)) + theme_linedraw() +\n  geom_line(aes(group = gr)) +\n  geom_vline(xintercept = pop.m, linetype = \"dashed\", linewidth = 0.2) +\n  labs(x = \"Bounds\", y = \"Confidence intervals\") + \n  theme(axis.title = element_text(size = 16),\n        axis.text = element_text(size = 14))\n```\n\n::: {.cell-output-display}\n![](02-07-gar-ci_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# ggsave(filename = './figures/nb2_ttci_norm_expop.pdf')\n```\n:::\n\n\n\n\nProportion of confidence intervals shifted to the **left** of the population value = 4.9%.\n\nProportion of confidence intervals shifted to the **right** of the population value = 4.7%.\n\n## Confidence intervals: sampling from lognormal distribution\n\nNow we sample from a lognormal population, which is an example of a skewed distribution.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogpop.m <- exp(pop.m + 0.5*pop.sd^2) # population mean\n# mean(rlnorm(20000)) # check that the population value is correct\n\nx <- seq(-2, 10, 0.01)\ny <- dlnorm(x, pop.m, pop.sd)\n\ndf <- tibble(x = x,\n             y = y)\n\nggplot(df, aes(x = x, y = y)) + theme_linedraw() +\n  geom_line() +\n  geom_vline(xintercept = logpop.m, linetype = \"dashed\") +\n  theme(axis.text = element_text(size = 14),\n        axis.title = element_text(size = 18),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) +\n  xlab(\"Population values\") +\n  ylab(\"Density\") +\n  coord_cartesian(xlim = c(-0.5, 8))\n```\n\n::: {.cell-output-display}\n![](02-07-gar-ci_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# ggsave(filename = './figures/nb2_lnorm_pop.pdf', width = 7, height = 5)\n```\n:::\n\n\n\n\n### 20 experiments - 20 confidence intervals\n\nWe perform many experiments, and for each experiment we compute a confidence interval, which we plot as horizontal lines.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21) # set seed of random number generator\nalpha.val <- .1 # 90% confidence interval\n\nnexp <- 20 # number of experiments\nn <- 20 # sample size in each experiment\n\n#declare matrices to save results\nmean.all <- vector(mode = \"numeric\", length = nexp)\nci.ttest <- matrix(NA, nrow = nexp, ncol = 2)\nci.ttest.in <- vector(mode = \"numeric\", length = nexp)\nsamp.all <- matrix(NA, nrow = nexp, ncol = n)\n\nfor(E in 1:nexp){\n  # sample data + bootstrap + compute mean of each bootstrap sample\n  samp <- rlnorm(n, pop.m, pop.sd)\n  samp.all[E,] <- samp\n  mean.all[E] <- mean(samp) # sample means\n  ci.ttest[E,] <- t.test(samp, conf.level = 1-alpha.val)$conf.int\n  # check if the confidence interval includes the population value\n  if(ci.ttest[E,1] > logpop.m || ci.ttest[E,2] < logpop.m){\n    ci.ttest.in[E] <- 0  \n  } else{\n    ci.ttest.in[E] <- 1  \n  }\n  \n}\n```\n:::\n\n\n\n\n#### Illustrate results\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# confidence intervals\ndf <- tibble(x = as.vector(ci.ttest),\n             y = rep(1:nexp,2),\n             gr = factor(rep(1:nexp,2)),\n             inout = factor(rep(ci.ttest.in,2))\n)\n\n# sample means\ndf2 <- tibble(x = mean.all,\n              y = 1:nexp,\n              inout = factor(ci.ttest.in))\n\n# samples\ndf3 <- tibble(x = as.vector(samp.all),\n              y = rep(1:nexp + 0.15, n)\n)\n\nggplot(df, aes(x = x, y = y)) + theme_linedraw() +\n  geom_line(aes(group = gr, colour = inout), linewidth = 0.8, lineend = \"round\", show.legend = FALSE) + # CI\n  geom_point(data = df3, size=0.3, alpha = 0.5) + # samples\n  geom_point(data = df2, aes(colour = inout), show.legend = FALSE) + # sample means\n  scale_colour_manual(values = c(\"darkgrey\", \"black\")) +\n  geom_vline(xintercept = logpop.m, colour = \"black\", linetype = \"dashed\", linewidth = 0.2) +\n  labs(x = \"Values\", y = \"Experiments\") +\n  theme(axis.title = element_text(size = 16),\n        axis.text = element_text(size = 14)) +\n  scale_y_continuous(minor_breaks = 1:20)\n```\n\n::: {.cell-output-display}\n![](02-07-gar-ci_files/figure-html/unnamed-chunk-14-1.png){width=288}\n:::\n\n```{.r .cell-code}\n# ggsave(filename = './figures/nb2_20expt_ttci_lnorm.pdf', width = 5, height = 7)\n```\n:::\n\n\n\n\n#### Illustrate results: bootstrap\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# re-use the previous chunk if you want to see how the results look like for the percentile bootstrap confidence intervals.\n```\n:::\n\n\n\n\n### 20,000 experiments - 20,000 confidence intervals\n\nNow we do 20,000 experiments, and check the long term coverage of the confidence intervals. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21) # set seed of random number generator\nalpha.val <- .1 # 90% confidence interval\n\nnexp <- 20000 # number of experiments\nn <- 20 # sample size in each experiment\n\n#declare matrices to save results\nci.ttest <- matrix(NA, nrow = nexp, ncol = 2)\nci.ttest.in <- vector(mode = \"numeric\", length = nexp)\nmean.all <- vector(mode = \"numeric\", length = nexp)\n\nfor(E in 1:nexp){\n  # sample data \n  samp <- rlnorm(n, pop.m, pop.sd)\n  # mean.all[E] <- mean(samp) # sample means\n  ci.ttest[E,] <- t.test(samp, conf.level = 1-alpha.val)$conf.int\n  # check if the confidence interval includes the population value\n  if(ci.ttest[E,1] > logpop.m || ci.ttest[E,2] < logpop.m){\n    ci.ttest.in[E] <- 0  \n  } else{\n    ci.ttest.in[E] <- 1  \n  }\n  \n}\n```\n:::\n\n\n\n\nConfidence interval coverage = 81.9%.\n\n#### Illustrate CIs excluding population\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nci <- ci.ttest[ci.ttest.in==0, ]\ntodo <- sum(ci.ttest.in==0)\ndf <- tibble(x = as.vector(ci),\n             y = rep(1:todo, 2),\n             gr = factor(rep(1:todo,2))\n)\n\nggplot(df, aes(x = x, y = y)) + theme_linedraw() +\n  geom_line(aes(group = gr)) +\n  geom_vline(xintercept = logpop.m, linetype = \"dashed\", linewidth = 0.2) +\n  labs(x = \"Bounds\", y = \"Confidence intervals\") + \n  theme(axis.title = element_text(size = 16),\n        axis.text = element_text(size = 14))\n```\n\n::: {.cell-output-display}\n![](02-07-gar-ci_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# ggsave(filename = './figures/nb2_ttci_norm_expop.pdf')\n```\n:::\n\n\n\n\nProportion of confidence intervals shifted to the **left** of the population value = 17.4%.\n\nProportion of confidence intervals shifted to the **right** of the population value = 0.7%.\n\n## Confidence interval coverage simulation: mean and 20% trimmed mean\n\nHere is another example of a simple simulation to check the probability coverage of a confidence interval method. The simulation has 10,000 iterations, which is often recommended or expected in the literature. For more complex applications, time might be a constraint.\n\nThe sample size is 30, which seems reasonably high for a psychology experiment. A more systematic simulation should include sample size as a parameter. \n\nThe populations are normal and lognormal and are generated outside the simulation loop. An alternative is to generate the random numbers directly inside the loop by using `samp <- rlnorm(nsamp)` and `samp <- rnorm(nsamp)`. The lognormal distribution is one of many skewed mathematical distributions. It serves to illustrate what can happen when sampling from skewed distributions in general. Other shapes could be used to, if some domain specific information is available. For instance, ex-Gaussian and shifted log-normal distributions do a good job at capturing the shape of reaction time distributions.\n\n### Illustrate populations\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- seq(-5, 15, 0.01)\nnx <- length(x)\ndn <- dnorm(x, 0, 1)\ndl <- dlnorm(x, 0, 1)\ndf <- tibble(x = rep(x, 2), \n             y = c(dl, dn), \n             distribution = factor(rep(c(\"Lognormal\", \"Normal\"), each = nx)))\n\nggplot(df, aes(x = x, y = y, colour = distribution)) + theme_gar +\n  geom_line(size = 1)  +\n  scale_color_manual(values=c(\"grey\", \"black\")) +\n  theme(legend.position = c(0.8, 0.8),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank()) +\n  labs(x = \"Values\", \n       y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](02-07-gar-ci_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# save figure\n# ggsave(filename = \"norm_lnorm_pop.pdf\", width = 15, height = 10, units = \"cm\")\n```\n:::\n\n\n\n\nThe population means and trimmed means differ and are estimated independently in the simulation: the sample mean is used to make inferences about the population mean, whereas the sample trimmed mean is used to make inferences about the population trimmed mean.\n\n### Trimmed means\n\n#### Illustrate 20% trimming - Normal distribution\n\nFor variety, the example below uses base R graphics to display a normal distribution with each blue area marking 20% of the distribution density (area under the curve). If your head hurts trying to understand the calls to the `polygon` function, don't worry, it took me many trials and errors until I could produce this figure.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntr <- .25\nxv <- seq(-4,4,0.01)\nyv <- dnorm(xv)\nplot(xv,yv,type=\"l\")\nzval <- qnorm(tr, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE)\npolygon(c(xv[xv<=zval],zval),c(yv[xv<=zval],yv[xv==-4]),col=5)\npolygon(c(xv[xv>=-zval],-zval),c(yv[xv>=-zval],yv[xv==4]),col=5)\n```\n\n::: {.cell-output-display}\n![](02-07-gar-ci_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n\n#### Illustrate 20% trimming - F distribution\n\nWhy an F distribution? No particular reason, it is one of many skewed distributions we could choose from.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntr <- .2\nxv <- seq(0.01,4,0.01)\nyv <- df(xv,6,18) #fx<-dlnorm(x)\nplot(xv,yv,type=\"l\")\nzval <- qf(tr,6,18)\npolygon(c(xv[xv<=zval],zval),c(yv[xv<=zval],yv[xv==0.01]),col=5)\nzval <- qf(1-tr,6,18)\npolygon(c(xv[xv>=zval],zval),c(yv[xv>=zval],yv[xv==4]),col=5)\n```\n\n::: {.cell-output-display}\n![](02-07-gar-ci_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n\n## Simulation\n\n**DO NOT RUN THE NEXT CHUNK UNLESS YOU'RE PLANNING A BREAK!**\n\nIf you want to try the code, and potentially look at other quantities and methods, you can speed things up by reducing `nsim` and `nboot`.\n\nTo make inferences about 20% trimmed means, we use a t-test in which the standard error was  adjusted to take into account the trimming, as implemented in the `trimci` function. We also use the more straightforward percentile bootstrap, which doesn't make parametric assumption and doesn't involve standard error estimation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(666) # reproducible results\n\n# Define parameters\nnsim <- 10000 # simulation iterations \nnsamp <- 30 # sample size\nnboot <- 1000 # could/should use more\ntp <- 0.2 # percentage of trimming\nalpha.val <- .05 # 1-alpha = 95% confidence interval\n\n# define populations\npop1 <- rnorm(1000000) \npop2 <- rlnorm(1000000) \n# population means\npop1.m <- mean(pop1) \npop2.m <- mean(pop2) \n# population 20% trimmed means\npop1.tm <- mean(pop1, trim = 0.2) \npop2.tm <- mean(pop2, trim = 0.2) \n\n# declare matrices of results\nci.cov.norm <- matrix(0, nrow = nsim, ncol = 3) # coverage for normal population\nci.cov.lnorm <- matrix(0, nrow = nsim, ncol = 3) # coverage for lognormal population\n\nfor(S in 1:nsim){ # simulation loop\n  \n  if(S == 1){ # iteration message to print in the console\n    print(paste(\"iteration\",S,\"/\",nsim))\n    beep(2)\n  }\n  if(S %% 1000 == 0){\n    print(paste(\"iteration\",S,\"/\",nsim))\n    beep(2)\n  }\n  \n  # Normal population =======================\n  samp <- sample(pop1, nsamp, replace = TRUE) # random sample from population\n  # Mean + t-test\n  ci <- t.test(samp, mu = pop1.m, conf.level = 1-alpha.val)$conf.int # standard t-test equation\n  ci.cov.norm[S,1] <- ci[1]<pop1.m && ci[2]>pop1.m # CI includes population value?\n  # 20% trimmed mean + adjusted t-test\n  ci <- trimci(samp, tr = tp, pr = FALSE, alpha = alpha.val)$ci # get adjusted t-test confidence interval\n  ci.cov.norm[S,2] <- ci[1]<pop1.tm && ci[2]>pop1.tm # CI includes population value?\n  # 20% trimmed mean + percentile bootstrap\n  ci <- onesampb(samp, est=mean, nboot=nboot, trim=tp, alpha=alpha.val)$ci # get bootstrap confidence interval\n  ci.cov.norm[S,3] <- ci[1]<pop1.tm && ci[2]>pop1.tm # CI includes population value?\n  \n  # Log-normal population =======================\n  samp <- sample(pop2, nsamp, replace = TRUE) # random sample from population\n  # Mean + t-test\n  ci <- t.test(samp, mu = pop2.m, conf.level = 1-alpha.val)$conf.int # standard t-test equation\n  ci.cov.lnorm[S,1] <- ci[1]<pop2.m && ci[2]>pop2.m # CI includes population value?\n  # 20% trimmed mean + adjusted t-test\n  ci <- trimci(samp, tr = tp, pr = FALSE, alpha = alpha.val)$ci # get adjusted t-test confidence interval\n  ci.cov.lnorm[S,2] <- ci[1]<pop2.tm && ci[2]>pop2.tm # CI includes population value?\n  # 20% trimmed mean + percentile bootstrap\n  ci <- onesampb(samp, est=mean, nboot=nboot, trim=tp, alpha=alpha.val)$ci # get bootstrap confidence interval\n  ci.cov.lnorm[S,3] <- ci[1]<pop2.tm && ci[2]>pop2.tm # CI includes population value?\n}\n\napply(ci.cov.norm, 2, mean) # average across simulations for each method\napply(ci.cov.lnorm, 2, mean) # average across simulations for each method\n\n# save simulation results to load in next chunk\nsave(ci.cov.norm, ci.cov.lnorm, file = \"data/ci.coverage.RData\")\n\nbeep(8)\n```\n:::\n\n\n\n\nHere are the results:\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"data/ci.coverage.RData\")\n\nres.norm <- apply(ci.cov.norm, 2, mean) # average across simulations\n\nres.lnorm <- apply(ci.cov.lnorm, 2, mean) # average across simulations\n```\n:::\n\n\n\n\nCoverage when sampling from the normal population:\n\n- t-test + mean = 95%  \n\n- adjusted t-test + 20% trimmed mean = 94.5%  \n\n- percentile bootstrap + 20% trimmed mean = 94.5%  \n\nCoverage when sampling from the lognormal population:\n\n- t-test + mean = 88.3%  \n\n- adjusted t-test + 20% trimmed mean = 93%  \n- percentile bootstrap + 20% trimmed mean = 94.5%  \n\nThese results demonstrate that when sampling from a skewed distribution such as the lognormal distribution, coverage can be very different from the expected one (here we expected 95% coverage).\n\nQuestion: What happens when we make inferences about the 20% trimmed mean?\n\n",
    "supporting": [
      "02-07-gar-ci_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}